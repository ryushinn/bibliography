@inproceedings{abadiTensorFlowSystemLargescale2016,
  title = {{{TensorFlow}}: A System for Large-Scale Machine Learning},
  shorttitle = {{{TensorFlow}}},
  booktitle = {Proceedings of the 12th {{USENIX}} Conference on {{Operating Systems Design}} and {{Implementation}}},
  author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2016-11-02},
  series = {{{OSDI}}'16},
  pages = {265--283},
  publisher = {{USENIX Association}},
  location = {{USA}},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  isbn = {978-1-931971-33-1}
}

@article{adlerSolvingIllposedInverse2017,
  title = {Solving Ill-Posed Inverse Problems Using Iterative Deep Neural Networks},
  author = {Adler, Jonas and Öktem, Ozan},
  date = {2017-12-01},
  journaltitle = {Inverse Problems},
  shortjournal = {Inverse Problems},
  volume = {33},
  number = {12},
  eprint = {1704.04058},
  eprinttype = {arxiv},
  pages = {124007},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/1361-6420/aa9581},
  url = {http://arxiv.org/abs/1704.04058},
  urldate = {2022-04-14},
  abstract = {We propose a partially learned approach for the solution of ill posed inverse problems with not necessarily linear forward operators. The method builds on ideas from classical regularization theory and recent advances in deep learning to perform learning while making use of prior information about the inverse problem encoded in the forward operator, noise model and a regularizing functional. The method results in a gradient-like iterative scheme, where the "gradient" component is learned using a convolutional network that includes the gradients of the data discrepancy and regularizer as input in each iteration. We present results of such a partially learned gradient scheme on a non-linear tomographic inversion problem with simulated data from both the Sheep-Logan phantom as well as a head CT. The outcome is compared against FBP and TV reconstruction and the proposed method provides a 5.4 dB PSNR improvement over the TV reconstruction while being significantly faster, giving reconstructions of 512 x 512 volumes in about 0.4 seconds using a single GPU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Functional Analysis,Mathematics - Numerical Analysis,Mathematics - Optimization and Control}
}

@inproceedings{al-shedivatDataEfficiencyMetalearning2021,
  title = {On {{Data Efficiency}} of {{Meta-learning}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Al-Shedivat, Maruan and Li, Liam and Xing, Eric and Talwalkar, Ameet},
  date = {2021-03-18},
  pages = {1369--1377},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v130/al-shedivat21a.html},
  urldate = {2022-05-24},
  abstract = {Meta-learning has enabled learning statistical models that can be quickly adapted to new prediction tasks. Motivated by use-cases in personalized federated learning, we study the often overlooked aspect of the modern meta-learning algorithms—their data efficiency. To shed more light on which methods are more efficient, we use techniques from algorithmic stability to derive bounds on the transfer risk that have important practical implications, indicating how much supervision is needed and how it must be allocated for each method to attain the desired level of generalization. Further, we introduce a new simple framework for evaluating meta-learning methods under a limit on the available supervision, conduct an empirical study of MAML, Reptile, andProtoNets, and demonstrate the differences in the behavior of these methods on few-shot and federated learning benchmarks. Finally, we propose active meta-learning, which incorporates active data selection into learning-to-learn, leading to better performance of all methods in the limited supervision regime.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@unpublished{alainVarianceReductionSGD2016,
  title = {Variance {{Reduction}} in {{SGD}} by {{Distributed Importance Sampling}}},
  author = {Alain, Guillaume and Lamb, Alex and Sankar, Chinnadhurai and Courville, Aaron and Bengio, Yoshua},
  date = {2016-04-16},
  eprint = {1511.06481},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1511.06481},
  url = {http://arxiv.org/abs/1511.06481},
  urldate = {2022-04-10},
  abstract = {Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{argelaguetRoleInteractionVirtual2016,
  title = {The Role of Interaction in Virtual Embodiment: {{Effects}} of the Virtual Hand Representation},
  shorttitle = {The Role of Interaction in Virtual Embodiment},
  booktitle = {2016 {{IEEE Virtual Reality}} ({{VR}})},
  author = {Argelaguet, Ferran and Hoyet, Ludovic and Trico, Michael and Lecuyer, Anatole},
  date = {2016-03},
  pages = {3--10},
  publisher = {{IEEE}},
  location = {{Greenville, SC, USA}},
  doi = {10.1109/vr.2016.7504682},
  url = {http://ieeexplore.ieee.org/document/7504682/},
  urldate = {2022-04-12},
  eventtitle = {2016 {{IEEE Virtual Reality}} ({{VR}})},
  isbn = {978-1-5090-0836-0}
}

@unpublished{arnoldLearn2learnLibraryMetaLearning2020,
  title = {Learn2learn: {{A Library}} for {{Meta-Learning Research}}},
  shorttitle = {Learn2learn},
  author = {Arnold, Sébastien M. R. and Mahajan, Praateek and Datta, Debajyoti and Bunner, Ian and Zarkias, Konstantinos Saitas},
  date = {2020-08-27},
  eprint = {2008.12284},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2008.12284},
  url = {http://arxiv.org/abs/2008.12284},
  urldate = {2022-05-10},
  abstract = {Meta-learning researchers face two fundamental issues in their empirical work: prototyping and reproducibility. Researchers are prone to make mistakes when prototyping new algorithms and tasks because modern meta-learning methods rely on unconventional functionalities of machine learning frameworks. In turn, reproducing existing results becomes a tedious endeavour -- a situation exacerbated by the lack of standardized implementations and benchmarks. As a result, researchers spend inordinate amounts of time on implementing software rather than understanding and developing new ideas. This manuscript introduces learn2learn, a library for meta-learning research focused on solving those prototyping and reproducibility issues. learn2learn provides low-level routines common across a wide-range of meta-learning techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning), and builds standardized interfaces to algorithms and benchmarks on top of them. In releasing learn2learn under a free and open source license, we hope to foster a community around standardized software for meta-learning research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning}
}

@inproceedings{arnoldWhenMAMLCan2021,
  title = {When {{MAML Can Adapt Fast}} and {{How}} to {{Assist When It Cannot}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Arnold, Sébastien and Iqbal, Shariq and Sha, Fei},
  date = {2021-03-18},
  pages = {244--252},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v130/arnold21a.html},
  urldate = {2022-09-16},
  abstract = {Model-Agnostic Meta-Learning (MAML) and its variants have achieved success in meta-learning tasks on many datasets and settings. Nonetheless, we have just started to understand and analyze how they are able to adapt fast to new tasks. In this work, we contribute by conducting a series of empirical and theoretical studies, and discover several interesting, previously unknown properties of the algorithm. First, we find MAML adapts better with a deep architecture even if the tasks need only a shallow one. Secondly, linear layers can be added to the output layers of a shallower model to increase the depth without altering the modelling capacity, leading to improved performance in adaptation. Alternatively, an external and separate neural network meta-optimizer can also be used to transform the gradient updates of a smaller model so as to obtain improved performances in adaptation. Drawing from these evidences, we theorize that for a deep neural network to meta-learn well, the upper layers must transform the gradients of the bottom layers as if the upper layers were an external meta-optimizer, operating on a smaller network that is composed of the bottom layers.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@unpublished{azinovicNeuralRGBDSurface2022,
  title = {Neural {{RGB-D Surface Reconstruction}}},
  author = {Azinović, Dejan and Martin-Brualla, Ricardo and Goldman, Dan B. and Nießner, Matthias and Thies, Justus},
  date = {2022-03-14},
  eprint = {2104.04532},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2104.04532},
  url = {http://arxiv.org/abs/2104.04532},
  urldate = {2022-05-09},
  abstract = {Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming applications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, virtual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radiance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not reconstruct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a surface is extracted using Marching Cubes, since during optimization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera refinement technique which improves the overall reconstruction quality. In contrast to concurrent work on integrating depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{baekPushingEnvelopeRGBBased2019,
  title = {Pushing the {{Envelope}} for {{RGB-Based Dense 3D Hand Pose Estimation}} via {{Neural Rendering}}},
  author = {Baek, Seungryul and Kim, Kwang In and Kim, Tae-Kyun},
  date = {2019},
  pages = {1067--1076},
  doi = {10.1109/CVPR.2019.00116},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Baek_Pushing_the_Envelope_for_RGB-Based_Dense_3D_Hand_Pose_Estimation_CVPR_2019_paper.html},
  urldate = {2022-04-11},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{bagherAccurateFittingMeasured2012,
  title = {Accurate Fitting of Measured Reflectances Using a {{Shifted Gamma}} Micro-Facet Distribution},
  author = {Bagher, M. M. and Soler, C. and Holzschuch, N.},
  date = {2012-06-01},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Comput. Graph. Forum},
  volume = {31},
  number = {4},
  pages = {1509--1518},
  issn = {0167-7055},
  doi = {10.1111/j.1467-8659.2012.03147.x},
  url = {https://doi.org/10.1111/j.1467-8659.2012.03147.x},
  urldate = {2022-09-03},
  abstract = {Material models are essential to the production of photo-realistic images. Measured BRDFs provide accurate representation with complex visual appearance, but have larger storage cost. Analytical BRDFs such as Cook-Torrance provide a compact representation but fail to represent the effects we observe with measured appearance. Accurately fitting an analytical BRDF to measured data remains a challenging problem. In this paper we introduce the SGD micro-facet distribution for Cook-Torrance BRDF. This distribution accurately models the behavior of most materials. As a consequence, we accurately represent all measured BRDFs using a single lobe. Our fitting procedure is stable and robust, and does not require manual tweaking of the parameters. © 2012 Wiley Periodicals, Inc.},
  keywords = {and texture,http://www.acm.org/class/1998/ I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Color,shading,shadowing}
}

@article{bagherNonParametricFactorMicrofacet2016,
  title = {A {{Non-Parametric Factor Microfacet Model}} for {{Isotropic BRDFs}}},
  author = {Bagher, Mahdi M. and Snyder, John and Nowrouzezahrai, Derek},
  date = {2016-07-28},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {35},
  number = {5},
  pages = {159:1--159:16},
  issn = {0730-0301},
  doi = {10.1145/2907941},
  url = {https://doi.org/10.1145/2907941},
  urldate = {2022-09-03},
  abstract = {We investigate the expressiveness of the microfacet model for isotropic bidirectional reflectance distribution functions (BRDFs) measured from real materials by introducing a non-parametric factor model that represents the model’s functional structure but abandons restricted parametric formulations of its factors. We propose a new objective based on compressive weighting that controls rendering error in high-dynamic-range BRDF fits better than previous factorization approaches. We develop a simple numerical procedure to minimize this objective and handle dependencies that arise between microfacet factors. Our method faithfully captures a more comprehensive set of materials than previous state-of-the-art parametric approaches yet remains compact (3.2KB per BRDF). We experimentally validate the benefit of the microfacet model over a naïve orthogonal factorization and show that fidelity for diffuse materials is modestly improved by fitting an unrestricted shadowing/masking factor. We also compare against a recent data-driven factorization approach [Bilgili et al. 2011] and show that our microfacet-based representation improves rendering accuracy for most materials while reducing storage by more than 10 ×.},
  keywords = {BRDF compression and factorization,microfacet theory,non-parametric models,robust statistics}
}

@article{bakoOfflineDeepImportance2019,
  title = {Offline {{Deep Importance Sampling}} for {{Monte Carlo Path Tracing}}},
  author = {Bako, Steve and Meyer, Mark and DeRose, Tony and Sen, Pradeep},
  date = {2019},
  journaltitle = {Computer Graphics Forum},
  volume = {38},
  number = {7},
  pages = {527--542},
  issn = {1467-8659},
  doi = {10.1111/cgf.13858},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13858},
  urldate = {2022-10-21},
  abstract = {Although modern path tracers are successfully being applied to many rendering applications, there is considerable interest to push them towards ever-decreasing sampling rates. As the sampling rate is substantially reduced, however, even Monte Carlo (MC) denoisers–which have been very successful at removing large amounts of noise–typically do not produce acceptable final results. As an orthogonal approach to this, we believe that good importance sampling of paths is critical for producing better-converged, path-traced images at low sample counts that can then, for example, be more effectively denoised. However, most recent importance-sampling techniques for guiding path tracing (an area known as “path guiding”) involve expensive online (per-scene) training and offer benefits only at high sample counts. In this paper, we propose an offline, scene-independent deep-learning approach that can importance sample first-bounce light paths for general scenes without the need of the costly online training, and can start guiding path sampling with as little as 1 sample per pixel. Instead of learning to “overfit” to the sampling distribution of a specific scene like most previous work, our data-driven approach is trained a priori on a set of training scenes on how to use a local neighborhood of samples with additional feature information to reconstruct the full incident radiance at a point in the scene, which enables first-bounce importance sampling for new test scenes. Our solution is easy to integrate into existing rendering pipelines without the need for retraining, as we demonstrate by incorporating it into both the Blender/Cycles and Mitsuba path tracers. Finally, we show how our offline, deep importance sampler (ODIS) increases convergence at low sample counts and improves the results of an off-the-shelf denoiser relative to other state-of-the-art sampling techniques.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13858}
}

@inproceedings{ballanMotionCaptureHands2012,
  title = {Motion {{Capture}} of {{Hands}} in {{Action Using Discriminative Salient Points}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2012},
  author = {Ballan, Luca and Taneja, Aparna and Gall, Jürgen and Van Gool, Luc and Pollefeys, Marc},
  editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {640--653},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33783-3_46},
  abstract = {Capturing the motion of two hands interacting with an object is a very challenging task due to the large number of degrees of freedom, self-occlusions, and similarity between the fingers, even in the case of multiple cameras observing the scene. In this paper we propose to use discriminatively learned salient points on the fingers and to estimate the finger-salient point associations simultaneously with the estimation of the hand pose. We introduce a differentiable objective function that also takes edges, optical flow and collisions into account. Our qualitative and quantitative evaluations show that the proposed approach achieves very accurate results for several challenging sequences containing hands and objects in action.},
  isbn = {978-3-642-33783-3},
  langid = {english},
  keywords = {Hand Tracking,Motion Capture,Particle Swarm Optimization,Salient Point,Virtual Node}
}

@inproceedings{barlaPraiseAlternativeBRDF2015,
  title = {In Praise of an Alternative {{BRDF}} Parametrization},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Material Appearance Modeling}}: {{Issues}} and {{Acquisition}}},
  author = {Barla, P. and Belcour, L. and Pacanowski, R.},
  date = {2015-06-23},
  series = {{{MAM}} '15},
  pages = {9--13},
  publisher = {{Eurographics Association}},
  location = {{Goslar, DEU}},
  abstract = {In this paper, we extend the work of Neumann et al. [NNSK99] and Stark et al. [SAS05] to a pair of 4D BRDF parameterizations with explicit changes of variables. We detail their mathematical properties and relationships to the commonly-used halfway/difference parametrization, and discuss their benefits and drawbacks using a few analytical test functions and measured BRDFs. Our preliminary study suggests that the alternative parametrization inspired by Stark et al. [SAS05] is superior, and should thus be considered in future work involving BRDFs.},
  isbn = {978-3-905674-83-5}
}

@misc{behlAlphaMAMLAdaptive2019,
  title = {Alpha {{MAML}}: {{Adaptive Model-Agnostic Meta-Learning}}},
  shorttitle = {Alpha {{MAML}}},
  author = {Behl, Harkirat Singh and Baydin, Atılım Güneş and Torr, Philip H. S.},
  date = {2019-05-17},
  number = {arXiv:1905.07435},
  eprint = {1905.07435},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.07435},
  url = {http://arxiv.org/abs/1905.07435},
  urldate = {2022-09-16},
  abstract = {Model-agnostic meta-learning (MAML) is a meta-learning technique to train a model on a multitude of learning tasks in a way that primes the model for few-shot learning of new tasks. The MAML algorithm performs well on few-shot learning problems in classification, regression, and fine-tuning of policy gradients in reinforcement learning, but comes with the need for costly hyperparameter tuning for training stability. We address this shortcoming by introducing an extension to MAML, called Alpha MAML, to incorporate an online hyperparameter adaptation scheme that eliminates the need to tune meta-learning and learning rates. Our results with the Omniglot database demonstrate a substantial reduction in the need to tune MAML training hyperparameters and improvement to training stability with less sensitivity to hyperparameter choice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{bemanaEikonalFieldsRefractive2022,
  title = {Eikonal {{Fields}} for {{Refractive Novel-View Synthesis}}},
  author = {Bemana, Mojtaba and Myszkowski, Karol and Frisvad, Jeppe Revall and Seidel, Hans-Peter and Ritschel, Tobias},
  date = {2022-06-15},
  number = {arXiv:2202.00948},
  eprint = {2202.00948},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.00948},
  urldate = {2022-06-25},
  abstract = {We tackle the problem of generating novel-view images from collections of 2D images showing refractive and reflective objects. Current solutions assume opaque or transparent light transport along straight paths following the emission-absorption model. Instead, we optimize for a field of 3D-varying Index of Refraction (IoR) and trace light through it that bends toward the spatial gradients of said IoR according to the laws of eikonal light transport.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@article{bemanaXFieldsImplicitNeural2020,
  ids = {bemanaXFieldsImplicitNeural2020a},
  title = {X-{{Fields}}: Implicit Neural View-, Light- and Time-Image Interpolation},
  shorttitle = {X-{{Fields}}},
  author = {Bemana, Mojtaba and Myszkowski, Karol and Seidel, Hans-Peter and Ritschel, Tobias},
  date = {2020-11-26},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {6},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3414685.3417827},
  url = {https://dl.acm.org/doi/10.1145/3414685.3417827},
  urldate = {2021-12-30},
  langid = {english}
}

@inproceedings{bergmanFastTrainingNeural2021,
  title = {Fast {{Training}} of {{Neural Lumigraph Representations}} Using {{Meta Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bergman, Alexander and Kellnhofer, Petr and Wetzstein, Gordon},
  date = {2021},
  volume = {34},
  pages = {172--186},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/01931a6925d3de09e5f87419d9d55055-Abstract.html},
  urldate = {2022-09-04},
  abstract = {Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.},
  keywords = {⛔ No DOI found}
}

@article{biDeepRelightableAppearance2021,
  title = {Deep Relightable Appearance Models for Animatable Faces},
  author = {Bi, Sai and Lombardi, Stephen and Saito, Shunsuke and Simon, Tomas and Wei, Shih-En and Mcphail, Kevyn and Ramamoorthi, Ravi and Sheikh, Yaser and Saragih, Jason},
  date = {2021-07-19},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {89:1--89:15},
  issn = {0730-0301},
  doi = {10.1145/3450626.3459829},
  url = {https://doi.org/10.1145/3450626.3459829},
  urldate = {2022-09-04},
  abstract = {We present a method for building high-fidelity animatable 3D face models that can be posed and rendered with novel lighting environments in real-time. Our main insight is that relightable models trained to produce an image lit from a single light direction can generalize to natural illumination conditions but are computationally expensive to render. On the other hand, efficient, high-fidelity face models trained with point-light data do not generalize to novel lighting conditions. We leverage the strengths of each of these two approaches. We first train an expensive but generalizable model on point-light illuminations, and use it to generate a training set of high-quality synthetic face images under natural illumination conditions. We then train an efficient model on this augmented dataset, reducing the generalization ability requirements. As the efficacy of this approach hinges on the quality of the synthetic data we can generate, we present a study of lighting pattern combinations for dynamic captures and evaluate their suitability for learning generalizable relightable models. Towards achieving the best possible quality, we present a novel approach for generating dynamic relightable faces that exceeds state-of-the-art performance. Our method is capable of capturing subtle lighting effects and can even generate compelling near-field relighting despite being trained exclusively with far-field lighting data. Finally, we motivate the utility of our model by animating it with images captured from VR-headset mounted cameras, demonstrating the first system for face-driven interactions in VR that uses a photorealistic relightable face model.},
  keywords = {appearance acquisition,face rendering,image-based rendering,neural rendering,relighting,view synthesis}
}

@inproceedings{blauPerceptionDistortionTradeoff2018,
  title = {The {{Perception-Distortion Tradeoff}}},
  author = {Blau, Yochai and Michaeli, Tomer},
  date = {2018},
  pages = {6228--6237},
  doi = {10.1109/cvpr.2018.00652},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Blau_The_Perception-Distortion_Tradeoff_CVPR_2018_paper.html},
  urldate = {2022-04-14},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{blinnModelsLightReflection1977,
  title = {Models of Light Reflection for Computer Synthesized Pictures},
  author = {Blinn, James F.},
  date = {1977-08},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {11},
  number = {2},
  pages = {192--198},
  issn = {0097-8930},
  doi = {10.1145/965141.563893},
  url = {https://dl.acm.org/doi/10.1145/965141.563893},
  urldate = {2022-09-02},
  abstract = {In the production of computer generated pictures of three dimensional objects, one stage of the calculation is the determination of the intensity of a given object once its visibility has been established. This is typically done by modelling the surface as a perfect diffuser, sometimes with a specular component added for the simulation of hilights. This paper presents a more accurate function for the generation of hilights which is based on some experimental measurements of how light reflects from real surfaces. It differs from previous models in that the intensity of the hilight changes with the direction of the light source. Also the position and shape of the hilights is somewhat different from that generated by simpler models. Finally, the hilight function generates different results when simulating metallic vs. nonmetallic surfaces. Many of the effects so generated are somewhat subtle and are apparent only during movie sequences. Some representative still frames from such movies are included.},
  langid = {english}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory  - {{COLT}} '92},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  date = {1992},
  pages = {144--152},
  publisher = {{ACM Press}},
  location = {{Pittsburgh, Pennsylvania, United States}},
  doi = {10.1145/130385.130401},
  url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
  urldate = {2022-04-05},
  eventtitle = {The Fifth Annual Workshop},
  isbn = {978-0-89791-497-0},
  langid = {english}
}

@article{botvinickRubberHandsFeel1998,
  title = {Rubber Hands ‘Feel’ Touch That Eyes See},
  author = {Botvinick, Matthew and Cohen, Jonathan},
  date = {1998-02},
  journaltitle = {Nature},
  volume = {391},
  number = {6669},
  pages = {756--756},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/35784},
  url = {https://www.nature.com/articles/35784},
  urldate = {2022-04-12},
  abstract = {Illusions have historically been of great use to psychology for what they can reveal about perceptual processes. We report here an illusion in which tactile sensations are referred to an alien limb. The effect reveals a three-way interaction between vision, touch and proprioception, and may supply evidence concerning the basis of bodily self-identification.},
  issue = {6669},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@misc{bouchardOnlineLearningSample2016,
  title = {Online {{Learning}} to {{Sample}}},
  author = {Bouchard, Guillaume and Trouillon, Théo and Perez, Julien and Gaidon, Adrien},
  date = {2016-03-15},
  number = {arXiv:1506.09016},
  eprint = {1506.09016},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1506.09016},
  urldate = {2022-06-25},
  abstract = {Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AWSGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and exploration policies are estimated at the same time, where our approach corresponds to an off-policy gradient algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@article{bradyGenBRDFDiscoveringNew2014,
  title = {{{genBRDF}}: Discovering New Analytic {{BRDFs}} with Genetic Programming},
  shorttitle = {{{genBRDF}}},
  author = {Brady, Adam and Lawrence, Jason and Peers, Pieter and Weimer, Westley},
  date = {2014-07-27},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {33},
  number = {4},
  pages = {114:1--114:11},
  issn = {0730-0301},
  doi = {10.1145/2601097.2601193},
  url = {https://doi.org/10.1145/2601097.2601193},
  urldate = {2022-09-03},
  abstract = {We present a framework for learning new analytic BRDF models through Genetic Programming that we call genBRDF. This approach to reflectance modeling can be seen as an extension of traditional methods that rely either on a phenomenological or empirical process. Our technique augments the human effort involved in deriving mathematical expressions that accurately characterize complex high-dimensional reflectance functions through a large-scale optimization. We present a number of analysis tools and data visualization techniques that are crucial to sifting through the large result sets produced by genBRDF in order to identify fruitful expressions. Additionally, we highlight several new models found by genBRDF that have not previously appeared in the BRDF literature. These new BRDF models are compact and more accurate than current state-of-the-art alternatives.},
  keywords = {analytic,BRDF,genetic programming,isotropic}
}

@inproceedings{burleyPhysicallybasedShadingDisney2012,
  title = {Physically-Based Shading at Disney},
  booktitle = {{{ACM SIGGRAPH}}},
  author = {Burley, Brent and Studios, Walt Disney Animation},
  date = {2012},
  volume = {2012},
  pages = {1--7},
  organization = {{vol. 2012}},
  keywords = {⛔ No DOI found}
}

@inproceedings{burnsHandSlowerEye2005,
  title = {The Hand Is Slower than the Eye: A Quantitative Exploration of Visual Dominance over Proprioception},
  shorttitle = {The Hand Is Slower than the Eye},
  booktitle = {{{IEEE Proceedings}}. {{VR}} 2005. {{Virtual Reality}}, 2005.},
  author = {Burns, E. and Razzaque, S. and Panter, A.T. and Whitton, M.C. and McCallus, M.R. and Brooks, F.P.},
  date = {2005-03},
  pages = {3--10},
  issn = {2375-5334},
  doi = {10.1109/VR.2005.1492747},
  abstract = {Without force feedback, a head-mounted display user's avatar may penetrate virtual objects. Some virtual environment designers prevent visual interpenetration, making the assumption that prevention improves user experience. However, preventing visual avatar interpenetration causes discrepancy between visual and proprioceptive cues. We investigated users' detection thresholds for visual interpenetration (the depth at which they see that two objects have interpenetrated) and sensory discrepancy (the displacement at which they notice mismatched visual and proprioceptive cues). We found that users are much less sensitive to visual-proprioceptive conflict than they are to visual interpenetration. We present our plan for using this result to create a better technique for dealing with virtual object penetration.},
  eventtitle = {{{IEEE Proceedings}}. {{VR}} 2005. {{Virtual Reality}}, 2005.},
  keywords = {Avatars,Chromium,Computer graphics,Displays,Force feedback,Layout,Object detection,User interfaces,Virtual environment,Virtual reality}
}

@article{camposORBSLAM3AccurateOpenSource2021,
  title = {{{ORB-SLAM3}}: {{An Accurate Open-Source Library}} for {{Visual}}, {{Visual}}–{{Inertial}}, and {{Multimap SLAM}}},
  shorttitle = {{{ORB-SLAM3}}},
  author = {Campos, Carlos and Elvira, Richard and Rodriguez, Juan J. Gomez and M. Montiel, Jose M. and D. Tardos, Juan},
  date = {2021-12},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {37},
  number = {6},
  pages = {1874--1890},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2021.3075644},
  url = {https://ieeexplore.ieee.org/document/9440682/},
  urldate = {2022-04-24}
}

@article{candesDecodingLinearProgramming2005,
  title = {Decoding by {{Linear Programming}}},
  author = {Candes, E.J. and Tao, T.},
  date = {2005-12},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {51},
  number = {12},
  pages = {4203--4215},
  issn = {0018-9448},
  doi = {10.1109/tit.2005.858979},
  url = {http://ieeexplore.ieee.org/document/1542412/},
  urldate = {2022-03-07},
  langid = {english}
}

@article{candesIntroductionCompressiveSampling2008,
  title = {An {{Introduction To Compressive Sampling}}},
  author = {Candes, E.J. and Wakin, M.B.},
  date = {2008-03},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {25},
  number = {2},
  pages = {21--30},
  issn = {1053-5888},
  doi = {10.1109/msp.2007.914731},
  url = {http://ieeexplore.ieee.org/document/4472240/},
  urldate = {2022-03-06}
}

@article{candesNearOptimalSignalRecovery2006,
  title = {Near-{{Optimal Signal Recovery From Random Projections}}: {{Universal Encoding Strategies}}?},
  shorttitle = {Near-{{Optimal Signal Recovery From Random Projections}}},
  author = {Candes, Emmanuel J. and Tao, Terence},
  date = {2006-12},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {52},
  number = {12},
  pages = {5406--5425},
  issn = {0018-9448},
  doi = {10.1109/tit.2006.885507},
  url = {http://ieeexplore.ieee.org/document/4016283/},
  urldate = {2022-03-06}
}

@article{candesRobustUncertaintyPrinciples2006,
  title = {Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information},
  shorttitle = {Robust Uncertainty Principles},
  author = {Candes, E.J. and Romberg, J. and Tao, T.},
  date = {2006-02},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {52},
  number = {2},
  pages = {489--509},
  issn = {0018-9448},
  doi = {10.1109/tit.2005.862083},
  url = {http://ieeexplore.ieee.org/document/1580791/},
  urldate = {2022-03-06}
}

@article{candesStableSignalRecovery2006,
  title = {Stable Signal Recovery from Incomplete and Inaccurate Measurements},
  author = {Candès, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
  date = {2006-08},
  journaltitle = {Communications on Pure and Applied Mathematics},
  shortjournal = {Comm. Pure Appl. Math.},
  volume = {59},
  number = {8},
  pages = {1207--1223},
  issn = {0010-3640, 1097-0312},
  doi = {10.1002/cpa.20124},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.20124},
  urldate = {2022-03-04},
  langid = {english}
}

@article{changLIBSVMLibrarySupport2011,
  title = {{{LIBSVM}}: {{A}} Library for Support Vector Machines},
  shorttitle = {{{LIBSVM}}},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  date = {2011-04},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  volume = {2},
  number = {3},
  pages = {1--27},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/1961189.1961199},
  url = {https://dl.acm.org/doi/10.1145/1961189.1961199},
  urldate = {2022-04-07},
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
  langid = {english}
}

@article{chenAsymptoticNumericalMethod2014,
  title = {An Asymptotic Numerical Method for Inverse Elastic Shape Design},
  author = {Chen, Xiang and Zheng, Changxi and Xu, Weiwei and Zhou, Kun},
  date = {2014-07-27},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {33},
  number = {4},
  pages = {95:1--95:11},
  issn = {0730-0301},
  doi = {10.1145/2601097.2601189},
  url = {https://doi.org/10.1145/2601097.2601189},
  urldate = {2022-04-19},
  abstract = {Inverse shape design for elastic objects greatly eases the design efforts by letting users focus on desired target shapes without thinking about elastic deformations. Solving this problem using classic iterative methods (e.g., Newton-Raphson methods), however, often suffers from slow convergence toward a desired solution. In this paper, we propose an asymptotic numerical method that exploits the underlying mathematical structure of specific nonlinear material models, and thus runs orders of magnitude faster than traditional Newton-type methods. We apply this method to compute rest shapes for elastic fabrication, where the rest shape of an elastic object is computed such that after physical fabrication the real object deforms into a desired shape. We illustrate the performance and robustness of our method through a series of elastic fabrication experiments.},
  keywords = {3D printing,elastic fabrication,finite element methods,nonlinear optimization}
}

@incollection{chenInvertibleNeuralBRDF2020,
  title = {Invertible {{Neural BRDF}} for {{Object Inverse Rendering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Chen, Zhe and Nobuhara, Shohei and Nishino, Ko},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12350},
  pages = {767--783},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58558-7_45},
  url = {https://link.springer.com/10.1007/978-3-030-58558-7_45},
  urldate = {2022-05-15},
  isbn = {978-3-030-58557-0 978-3-030-58558-7},
  langid = {english}
}

@inproceedings{chenInvertibleNeuralBRDF2020a,
  title = {Invertible {{Neural BRDF}} for {{Object Inverse Rendering}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Chen, Zhe and Nobuhara, Shohei and Nishino, Ko},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {767--783},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58558-7_45},
  abstract = {We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems.},
  isbn = {978-3-030-58558-7},
  langid = {english},
  keywords = {BRDF,Illumination estimation,Inverse rendering,Reflectance}
}

@article{chenStudySMOtypeDecomposition2006,
  title = {A Study on {{SMO-type}} Decomposition Methods for Support Vector Machines},
  author = {Chen, Pai-Hsuen and Fan, Rong-En and Lin, Chih-Jen},
  date = {2006},
  journaltitle = {IEEE transactions on neural networks},
  volume = {17},
  number = {4},
  pages = {893--908},
  doi = {10.1109/tnn.2006.875973}
}

@article{chih-jenlinAsymptoticConvergenceSMO2002,
  title = {Asymptotic Convergence of an {{SMO}} Algorithm without Any Assumptions},
  author = {{Chih-Jen Lin}},
  year = {Jan./2002},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {13},
  number = {1},
  pages = {248--250},
  issn = {10459227},
  doi = {10.1109/72.977319},
  url = {http://ieeexplore.ieee.org/document/977319/},
  urldate = {2022-04-07}
}

@article{chih-jenlinFormalAnalysisStopping2002,
  ids = {chih-jenlinFormalAnalysisStopping2002a},
  title = {A Formal Analysis of Stopping Criteria of Decomposition Methods for Support Vector Machines},
  author = {{Chih-Jen Lin}},
  date = {2002-09},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {13},
  number = {5},
  pages = {1045--1052},
  issn = {1045-9227},
  doi = {10.1109/tnn.2002.1031937},
  url = {http://ieeexplore.ieee.org/document/1031937/},
  urldate = {2022-04-01},
  langid = {english}
}

@misc{cohenRiemannianConvexPotential2021,
  title = {Riemannian {{Convex Potential Maps}}},
  author = {Cohen, Samuel and Amos, Brandon and Lipman, Yaron},
  date = {2021-06-18},
  number = {arXiv:2106.10272},
  eprint = {2106.10272},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.10272},
  urldate = {2022-05-30},
  abstract = {Modeling distributions on Riemannian manifolds is a crucial component in understanding non-Euclidean data that arises, e.g., in physics and geology. The budding approaches in this space are limited by representational and computational tradeoffs. We propose and study a class of flows that uses convex potentials from Riemannian optimal transport. These are universal and can model distributions on any compact Riemannian manifold without requiring domain knowledge of the manifold to be integrated into the architecture. We demonstrate that these flows can model standard distributions on spheres, and tori, on synthetic and geological data. Our source code is freely available online at http://github.com/facebookresearch/rcpm},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{cookReflectanceModelComputer1982,
  title = {A Reflectance Model for Computer Graphics},
  author = {Cook, Robert L and Torrance, Kenneth E.},
  date = {1982},
  journaltitle = {ACM Transactions on Graphics (ToG)},
  volume = {1},
  number = {1},
  pages = {7--24},
  publisher = {{ACM New York, NY, USA}},
  doi = {10.1145/357290.357293}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/bf00994018},
  url = {http://link.springer.com/10.1007/BF00994018},
  urldate = {2022-04-05},
  langid = {english}
}

@article{crowderLinearConvergenceConjugate1972,
  title = {Linear {{Convergence}} of the {{Conjugate Gradient Method}}},
  author = {Crowder, Harlan and Wolfe, Philip},
  date = {1972-07},
  journaltitle = {IBM Journal of Research and Development},
  shortjournal = {IBM J. Res. \& Dev.},
  volume = {16},
  number = {4},
  pages = {431--433},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/rd.164.0431},
  url = {http://ieeexplore.ieee.org/document/5391446/},
  urldate = {2022-02-20}
}

@inproceedings{daiScanNetRichlyAnnotated3D2017,
  title = {{{ScanNet}}: {{Richly-Annotated 3D Reconstructions}} of {{Indoor Scenes}}},
  shorttitle = {{{ScanNet}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nießner, Matthias},
  date = {2017-07},
  pages = {2432--2443},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2017.261},
  abstract = {A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available - current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowd-sourced semantic annotation.We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Image reconstruction,Semantics,Sensors,Solid modeling,Surface reconstruction,Three-dimensional displays,Two dimensional displays}
}

@article{danaDeviceConvenientMeasurement2004,
  title = {Device for Convenient Measurement of Spatially Varying Bidirectional Reflectance},
  author = {Dana, Kristin J. and Wang, Jing},
  date = {2004-01-01},
  journaltitle = {Journal of the Optical Society of America A},
  shortjournal = {J. Opt. Soc. Am. A},
  volume = {21},
  number = {1},
  pages = {1},
  issn = {1084-7529, 1520-8532},
  doi = {10.1364/JOSAA.21.000001},
  url = {https://opg.optica.org/abstract.cfm?URI=josaa-21-1-1},
  urldate = {2022-09-01},
  langid = {english}
}

@misc{deleuContinuousTimeMetaLearningForward2022,
  title = {Continuous-{{Time Meta-Learning}} with {{Forward Mode Differentiation}}},
  author = {Deleu, Tristan and Kanaa, David and Feng, Leo and Kerg, Giancarlo and Bengio, Yoshua and Lajoie, Guillaume and Bacon, Pierre-Luc},
  date = {2022-03-02},
  number = {arXiv:2203.01443},
  eprint = {2203.01443},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.01443},
  url = {http://arxiv.org/abs/2203.01443},
  urldate = {2022-09-04},
  abstract = {Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{dellaertNeuralVolumeRendering2021,
  title = {Neural {{Volume Rendering}}: {{NeRF And Beyond}}},
  shorttitle = {Neural {{Volume Rendering}}},
  author = {Dellaert, Frank and Yen-Chen, Lin},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2101.05204},
  url = {https://arxiv.org/abs/2101.05204},
  urldate = {2022-06-10},
  abstract = {Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also the year in which neural volume rendering exploded onto the scene, triggered by the impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to capture this excitement, Frank on a blog post (Dellaert, 2020) and Yen-Chen in a Github collection (Yen-Chen, 2020). This note is an annotated bibliography of the relevant papers, and we posted the associated bibtex file on the repository.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Graphics (cs.GR)}
}

@article{denbrokRapidMaterialCapture2018,
  title = {Rapid Material Capture through Sparse and Multiplexed Measurements},
  author = {den Brok, Dennis and Weinmann, Michael and Klein, Reinhard},
  options = {useprefix=true},
  date = {2018-06},
  journaltitle = {Computers \& Graphics},
  shortjournal = {Computers \& Graphics},
  volume = {73},
  pages = {26--36},
  issn = {00978493},
  doi = {10.1016/j.cag.2018.03.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0097849318300360},
  urldate = {2022-09-04},
  langid = {english}
}

@inproceedings{dengReconstructingTranslucentObjects2022,
  title = {Reconstructing {{Translucent Objects}} Using {{Differentiable Rendering}}},
  booktitle = {{{ACM SIGGRAPH}} 2022 {{Conference Proceedings}}},
  author = {Deng, Xi and Luan, Fujun and Walter, Bruce and Bala, Kavita and Marschner, Steve},
  date = {2022-07-27},
  series = {{{SIGGRAPH}} '22},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3528233.3530714},
  url = {https://doi.org/10.1145/3528233.3530714},
  urldate = {2022-10-06},
  abstract = {Inverse rendering is a powerful approach to modeling objects from photographs, and we extend previous techniques to handle translucent materials that exhibit subsurface scattering. Representing translucency using a heterogeneous bidirectional scattering-surface reflectance distribution function (BSSRDF), we extend the framework of path-space differentiable rendering to accommodate both surface and subsurface reflection. This introduces new types of paths requiring new methods for sampling moving discontinuities in material space that arise from visibility and moving geometry. We use this differentiable rendering method in an end-to-end approach that jointly recovers heterogeneous translucent materials (represented by a BSSRDF) and detailed geometry of an object (represented by a mesh) from a sparse set of measured 2D images in a coarse-to-fine framework incorporating Laplacian preconditioning for the geometry. To efficiently optimize our models in the presence of the Monte Carlo noise introduced by the BSSRDF integral, we introduce a dual-buffer method for evaluating the L2 image loss. This efficiently avoids potential bias in gradient estimation due to the correlation of estimates for image pixels and their derivatives and enables correct convergence of the optimizer even when using low sample counts in the renderer. We validate our derivatives by comparing against finite differences and demonstrate the effectiveness of our technique by comparing inverse-rendering performance with previous methods. We show superior reconstruction quality on a set of synthetic and real-world translucent objects as compared to previous methods that model only surface reflection.},
  isbn = {978-1-4503-9337-9},
  keywords = {appearance acquisition,differentiable rendering,ray tracing,subsurface scattering}
}

@article{deschaintreFlexibleSVBRDFCapture2019,
  title = {Flexible {{SVBRDF Capture}} with a {{Multi-Image Deep Network}}},
  author = {Deschaintre, Valentin and Aittala, Miika and Durand, Fredo and Drettakis, George and Bousseau, Adrien},
  date = {2019},
  journaltitle = {Computer Graphics Forum},
  volume = {38},
  number = {4},
  pages = {1--13},
  issn = {1467-8659},
  doi = {10.1111/cgf.13765},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13765},
  urldate = {2022-10-06},
  abstract = {Empowered by deep learning, recent methods for material capture can estimate a spatially-varying reflectance from a single photograph. Such lightweight capture is in stark contrast with the tens or hundreds of pictures required by traditional optimization-based approaches. However, a single image is often simply not enough to observe the rich appearance of real-world materials. We present a deep-learning method capable of estimating material appearance from a variable number of uncalibrated and unordered pictures captured with a handheld camera and flash. Thanks to an order-independent fusing layer, this architecture extracts the most useful information from each picture, while benefiting from strong priors learned from data. The method can handle both view and light direction variation without calibration. We show how our method improves its prediction with the number of input pictures, and reaches high quality reconstructions with as little as 1 to 10 images - a sweet spot between existing single-image and complex multi-image approaches.},
  langid = {english},
  keywords = {• Computing methodologies → Reflectance modeling,Appearance capture,CCS Concepts,Deep learning,Image processing,Material capture,SVBRDF},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13765}
}

@article{deschaintreGuidedFinetuningLargescale2020,
  title = {Guided Fine-Tuning for Large-Scale Material Transfer},
  author = {Deschaintre, Valentin and Drettakis, George and Bousseau, Adrien},
  date = {2020},
  journaltitle = {Computer Graphics Forum (Proceedings of the Eurographics Symposium on Rendering)},
  volume = {39},
  number = {4},
  url = {http://www-sop.inria.fr/reves/Basilic/2020/DDB20},
  keywords = {⛔ No DOI found,appearance capture,deep learning,fine tuning,material capture,material transfer,SVBRDF}
}

@article{deschaintreSingleimageSVBRDFCapture2018,
  title = {Single-Image {{SVBRDF}} Capture with a Rendering-Aware Deep Network},
  author = {Deschaintre, Valentin and Aittala, Miika and Durand, Fredo and Drettakis, George and Bousseau, Adrien},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {4},
  pages = {128:1--128:15},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201378},
  url = {https://doi.org/10.1145/3197517.3201378},
  urldate = {2022-09-02},
  abstract = {Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs.},
  keywords = {appearance capture,deep learning,material capture,SVBRDF}
}

@misc{dinhDensityEstimationUsing2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  date = {2017-02-27},
  number = {arXiv:1605.08803},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.08803},
  url = {http://arxiv.org/abs/1605.08803},
  urldate = {2022-05-28},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@misc{dinhNICENonlinearIndependent2015,
  title = {{{NICE}}: {{Non-linear Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  date = {2015-04-10},
  number = {arXiv:1410.8516},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1410.8516},
  url = {http://arxiv.org/abs/1410.8516},
  urldate = {2022-05-28},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{diolatzisActiveExplorationNeural2022,
  title = {Active {{Exploration}} for {{Neural Global Illumination}} of {{Variable Scenes}}},
  author = {Diolatzis, Stavros and Philip, Julien and Drettakis, George},
  date = {2022-03-09},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  pages = {3522735},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3522735},
  url = {https://dl.acm.org/doi/10.1145/3522735},
  urldate = {2022-05-11},
  abstract = {Neural rendering algorithms introduce a fundamentally new approach for photorealistic rendering, typically by learning a neural representation of illumination on large numbers of ground truth images. When training for a given               variable               scene, i.e., changing objects, materials, lights and viewpoint, the space                                \textbackslash (\textbackslash mathcal \{D\} \textbackslash )                              ~of possible training data instances quickly becomes unmanageable as the dimensions of variable parameters increase. We introduce a novel               Active Exploration               method using Markov Chain Monte Carlo, which               explores                                \textbackslash (\textbackslash mathcal \{D\} \textbackslash )                              ~, generating samples (i.e., ground truth renderings) that best help training and interleaves training and on-the-fly sample data generation. We introduce a self-tuning sample reuse strategy to minimize the expensive step of rendering training samples. We apply our approach on a neural generator that learns to render novel scene instances given an explicit parameterization of the scene configuration. Our results show that Active Exploration trains our network much more efficiently than uniformly sampling, and together with our resolution enhancement approach, achieves better quality than uniform sampling at convergence. Our method allows interactive rendering of hard light transport paths (e.g., complex caustics) – that require very high samples counts to be captured – and provides dynamic scene navigation and manipulation, after training for 5-18 hours~depending on required quality and variations.},
  langid = {english}
}

@misc{donahueAdversarialFeatureLearning2017,
  title = {Adversarial {{Feature Learning}}},
  author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
  date = {2017-04-03},
  number = {arXiv:1605.09782},
  eprint = {1605.09782},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1605.09782},
  urldate = {2022-05-27},
  abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{donohoCompressedSensing2006,
  title = {Compressed Sensing},
  author = {Donoho, D.L.},
  date = {2006-04},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {52},
  number = {4},
  pages = {1289--1306},
  issn = {0018-9448},
  doi = {10.1109/tit.2006.871582},
  url = {http://ieeexplore.ieee.org/document/1614066/},
  urldate = {2022-03-06}
}

@inproceedings{dorseyDigitalModelingAppearance2005,
  title = {Digital Modeling of the Appearance of Materials},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Courses}} on   - {{SIGGRAPH}} '05},
  author = {Dorsey, Julie and Rushmeier, Holly},
  date = {2005},
  pages = {1},
  publisher = {{ACM Press}},
  location = {{Los Angeles, California}},
  doi = {10.1145/1198555.1198694},
  url = {http://portal.acm.org/citation.cfm?doid=1198555.1198694},
  urldate = {2022-02-02},
  eventtitle = {{{ACM SIGGRAPH}} 2005 {{Courses}}},
  langid = {english}
}

@article{dupuyAdaptiveParameterizationEfficient2018,
  title = {An Adaptive Parameterization for Efficient Material Acquisition and Rendering},
  author = {Dupuy, Jonathan and Jakob, Wenzel},
  date = {2018-12-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {6},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3272127.3275059},
  url = {https://dl.acm.org/doi/10.1145/3272127.3275059},
  urldate = {2022-05-15},
  abstract = {One of the key ingredients of any physically based rendering system is a detailed specification characterizing the interaction of light and matter of all materials present in a scene, typically via the Bidirectional Reflectance Distribution Function (BRDF). Despite their utility, access to real-world BRDF datasets remains limited: this is because measurements involve scanning a four-dimensional domain at sufficient resolution, a tedious and often infeasibly time-consuming process.             We propose a new parameterization that automatically adapts to the behavior of a material, warping the underlying 4D domain so that most of the volume maps to regions where the BRDF takes on non-negligible values, while irrelevant regions are strongly compressed. This adaptation only requires a brief 1D or 2D measurement of the material's retro-reflective properties. Our parameterization is unified in the sense that it combines several steps that previously required intermediate data conversions: the same mapping can simultaneously be used for BRDF acquisition, storage, and it supports efficient Monte Carlo sample generation.             We observe that the above desiderata are satisfied by a core operation present in modern rendering systems, which maps uniform variates to direction samples that are proportional to an analytic BRDF. Based on this insight, we define our adaptive parameterization as an invertible, retro-reflectively driven mapping between the parametric and directional domains. We are able to create noise-free renderings of existing BRDF datasets after conversion into our representation with the added benefit that the warped data is significantly more compact, requiring 16KiB and 544KiB per spectral channel for isotropic and anisotropic specimens, respectively.             Finally, we show how to modify an existing gonio-photometer to provide the needed retro-reflection measurements. Acquisition then proceeds within a 4D space that is warped by our parameterization. We demonstrate the efficacy of this scheme by acquiring the first set of spectral BRDFs of surfaces exhibiting arbitrary roughness, including anisotropy.},
  langid = {english}
}

@inproceedings{durkanNeuralSplineFlows2019,
  title = {Neural {{Spline Flows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html},
  urldate = {2022-06-01},
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
  keywords = {⛔ No DOI found}
}

@inproceedings{eadeMonocularGraphSLAM2010,
  title = {Monocular Graph {{SLAM}} with Complexity Reduction},
  booktitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Eade, E and Fong, P and Munich, M E},
  date = {2010-10},
  pages = {3017--3024},
  publisher = {{IEEE}},
  location = {{Taipei}},
  doi = {10.1109/iros.2010.5649205},
  url = {http://ieeexplore.ieee.org/document/5649205/},
  urldate = {2022-03-26},
  eventtitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2010)},
  isbn = {978-1-4244-6674-0}
}

@article{fanLearningWhatData2017,
  title = {Learning {{What Data}} to {{Learn}}},
  author = {Fan, Yang and Tian, Fei and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1702.08635},
  url = {https://arxiv.org/abs/1702.08635},
  urldate = {2022-04-25},
  abstract = {Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \textbackslash emph\{\textbackslash textbf\{N\}eural \textbackslash textbf\{D\}ata \textbackslash textbf\{F\}ilter\} (\textbackslash textbf\{NDF\}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations.},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@unpublished{fanNeuralBRDFsRepresentation2021,
  title = {Neural {{BRDFs}}: {{Representation}} and {{Operations}}},
  shorttitle = {Neural {{BRDFs}}},
  author = {Fan, Jiahui and Wang, Beibei and Hašan, Miloš and Yang, Jian and Yan, Ling-Qi},
  date = {2021-11-14},
  eprint = {2111.03797},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2111.03797},
  url = {http://arxiv.org/abs/2111.03797},
  urldate = {2022-01-19},
  abstract = {Bidirectional reflectance distribution functions (BRDFs) are pervasively used in computer graphics to produce realistic physically-based appearance. In recent years, several works explored using neural networks to represent BRDFs, taking advantage of neural networks' high compression rate and their ability to fit highly complex functions. However, once represented, the BRDFs will be fixed and therefore lack flexibility to take part in follow-up operations. In this paper, we present a form of "Neural BRDF algebra", and focus on both representation and operations of BRDFs at the same time. We propose a representation neural network to compress BRDFs into latent vectors, which is able to represent BRDFs accurately. We further propose several operations that can be applied solely in the latent space, such as layering and interpolation. Spatial variation is straightforward to achieve by using textures of latent vectors. Furthermore, our representation can be efficiently evaluated and sampled, providing a competitive solution to more expensive Monte Carlo layering approaches.},
  archiveprefix = {arXiv},
  keywords = {.skimmed,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@inproceedings{fanNeuralLayeredBRDFs2022,
  title = {Neural Layered {{BRDFs}}},
  booktitle = {Proceedings of {{SIGGRAPH}} 2022},
  author = {Fan, Jiahui and Wang, Beibei and Hašan, Miloš and Yang, Jian and Yan, Ling-Qi},
  date = {2022},
  doi = {10.1145/3528233.3530732}
}

@article{fanWorkingSetSelection2005,
  title = {Working {{Set Selection Using Second Order Information}} for {{Training Support Vector Machines}}},
  author = {Fan, Rong-En and Chen, Pai-Hsuen and Lin, Chih-Jen},
  date = {2005},
  journaltitle = {Journal of Machine Learning Research},
  volume = {6},
  number = {63},
  pages = {1889--1918},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v6/fan05a.html},
  urldate = {2022-04-01},
  abstract = {Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using first order information.},
  keywords = {⛔ No DOI found}
}

@article{filipTemplateBasedSamplingAnisotropic2014,
  title = {Template-{{Based Sampling}} of {{Anisotropic BRDFs}}},
  author = {Filip, J. and Vávra, R.},
  date = {2014-10-01},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Comput. Graph. Forum},
  volume = {33},
  number = {7},
  pages = {91--99},
  issn = {0167-7055},
  doi = {10.1111/cgf.12477},
  url = {https://doi.org/10.1111/cgf.12477},
  urldate = {2022-09-03},
  abstract = {BRDFs are commonly used to represent given materials' appearance in computer graphics and related fields. Although, in the recent past, BRDFs have been extensively measured, compressed, and fitted by a variety of analytical models, most research has been primarily focused on simplified isotropic BRDFs. In this paper, we present a unique database of 150 BRDFs representing a wide range of materials; the majority exhibiting anisotropic behavior. Since time-consuming BRDF measurement represents a major obstacle in the digital material appearance reproduction pipeline, we tested several approaches estimating a very limited set of samples capable of high quality appearance reconstruction. Initially, we aligned all measured BRDFs according to the location of the anisotropic highlights. Then we propose an adaptive sampling method based on analysis of the measured BRDFs. For each BRDF, a unique sampling pattern was computed, given a predefined count of samples. Further, template-based methods are introduced based on reusing of the precomputed sampling patterns. This approach enables a more efficient measurement of unknown BRDFs while preserving the visual fidelity for the majority of tested materials. Our method exhibits better performance and stability than competing sparse sampling approaches; especially for higher numbers of samples.},
  keywords = {and texture,Categories and Subject Descriptors according to ACM CCS,I.3.4 [Computer Graphics]: Digitization and Image Capture-Reflectance,I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Color,shading,shadowing}
}

@unpublished{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017-07-18},
  eprint = {1703.03400},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1703.03400},
  url = {http://arxiv.org/abs/1703.03400},
  urldate = {2022-01-19},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{finnOnlineMetaLearning2019,
  title = {Online {{Meta-Learning}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  date = {2019-05-24},
  pages = {1920--1930},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/finn19a.html},
  urldate = {2022-01-19},
  abstract = {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@unpublished{fischerMetappearanceMetaLearningVisual2022,
  title = {Metappearance: {{Meta-Learning}} for {{Visual Appearance Reproduction}}},
  shorttitle = {Metappearance},
  author = {Fischer, Michael and Ritschel, Tobias},
  date = {2022-04-19},
  eprint = {2204.08993},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.08993},
  urldate = {2022-05-12},
  abstract = {There currently are two main approaches to reproducing visual appearance using Machine Learning (ML): The first is training models that generalize over different instances of a problem, e.g., different images from a dataset. Such models learn priors over the data corpus and use this knowledge to provide fast inference with little input, often as a one-shot operation. However, this generality comes at the cost of fidelity, as such methods often struggle to achieve the final quality required. The second approach does not train a model that generalizes across the data, but overfits to a single instance of a problem, e.g., a flash image of a material. This produces detailed and high-quality results, but requires time-consuming training and is, as mere non-linear function fitting, unable to exploit previous experience. Techniques such as fine-tuning or auto-decoders combine both approaches but are sequential and rely on per-exemplar optimization. We suggest to combine both techniques end-to-end using meta-learning: We over-fit onto a single problem instance in an inner loop, while also learning how to do so efficiently in an outer-loop that builds intuition over many optimization runs. We demonstrate this concept to be versatile and efficient, applying it to RGB textures, Bi-directional Reflectance Distribution Functions (BRDFs), or Spatially-varying BRDFs (svBRDFs).},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Graphics}
}

@inproceedings{flynnDeepViewViewSynthesis2019,
  title = {{{DeepView}}: {{View Synthesis With Learned Gradient Descent}}},
  shorttitle = {{{DeepView}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Flynn, John and Broxton, Michael and Debevec, Paul and DuVall, Matthew and Fyffe, Graham and Overbeck, Ryan and Snavely, Noah and Tucker, Richard},
  date = {2019-06},
  pages = {2362--2371},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00247},
  abstract = {We present a novel approach to view synthesis using multiplane images (MPIs). Building on recent advances in learned gradient descent, our algorithm generates an MPI from a set of sparse camera viewpoints. The resulting method incorporates occlusion reasoning, improving performance on challenging scene features such as object boundaries, lighting reflections, thin structures, and scenes with high depth complexity. We show that our method achieves high-quality, state-of-the-art results on two datasets: the Kalantari light field dataset, and a new camera array dataset, Spaces, which we make publicly available.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {3D from Multiview and Sensors,Computational Photography,Deep Learning,Image and Video Synthesis,Optimization Methods}
}

@inproceedings{freyDoesWakesleepAlgorithm1995,
  title = {Does the {{Wake-sleep Algorithm Produce Good Density Estimators}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Frey, Brendan J and Hinton, Geoffrey E and Dayan, Peter},
  date = {1995},
  volume = {8},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1995/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html},
  urldate = {2022-05-08},
  abstract = {The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a rel(cid:173) atively efficient method of fitting  a multilayer stochastic generative  model to high-dimensional data. In addition to  the top-down connec(cid:173) tions in the generative model, it makes use of bottom-up connections for  approximating the probability distribution over the hidden units given  the data, and it trains these bottom-up connections using a simple delta  rule. We use a variety of synthetic and real data sets to compare the per(cid:173) formance of the wake-sleep algorithm with Monte Carlo and mean field  methods for fitting  the same generative model and also compare it with  other models that are less powerful but easier to fit.},
  keywords = {⛔ No DOI found}
}

@article{fuchsAdaptiveSamplingReflectance2007,
  title = {Adaptive Sampling of Reflectance Fields},
  author = {Fuchs, Martin and Blanz, Volker and Lensch, Hendrik P.A. and Seidel, Hans-Peter},
  date = {2007-06},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {26},
  number = {2},
  pages = {10},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/1243980.1243984},
  url = {https://dl.acm.org/doi/10.1145/1243980.1243984},
  urldate = {2022-09-01},
  abstract = {Image-based relighting achieves high quality in rendering, but it requires a large number of measurements of the reflectance field. This article discusses sampling techniques that improve on the trade-offs between measurement effort and reconstruction quality.             Specifically, we (i) demonstrate that sampling with point lights and from a sparse set of incoming light directions creates artifacts which can be reduced significantly by employing extended light sources for sampling, (ii) propose a sampling algorithm which incrementally chooses light directions adapted to the properties of the reflectance field being measured, thus capturing significant features faster than fixed-pattern sampling, and (iii) combine reflectance fields from two different light domain resolutions.             We present an automated measurement setup for well-defined angular distributions of the incident, indirect illumination. It is based on programmable spotlights with controlled aperture that illuminate the walls around the scene.},
  langid = {english}
}

@article{gaoDeepInverseRendering2019,
  title = {Deep Inverse Rendering for High-Resolution {{SVBRDF}} Estimation from an Arbitrary Number of Images},
  author = {GAO, DUAN and Li, Xiao and Dong, Yue and Peers, Pieter and Xu, Kun and Tong, Xin},
  date = {2019-07-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {4},
  pages = {134:1--134:15},
  issn = {0730-0301},
  doi = {10.1145/3306346.3323042},
  url = {https://doi.org/10.1145/3306346.3323042},
  urldate = {2022-09-02},
  abstract = {In this paper we present a unified deep inverse rendering framework for estimating the spatially-varying appearance properties of a planar exemplar from an arbitrary number of input photographs, ranging from just a single photograph to many photographs. The precision of the estimated appearance scales from plausible when the input photographs fails to capture all the reflectance information, to accurate for large input sets. A key distinguishing feature of our framework is that it directly optimizes for the appearance parameters in a latent embedded space of spatially-varying appearance, such that no handcrafted heuristics are needed to regularize the optimization. This latent embedding is learned through a fully convolutional auto-encoder that has been designed to regularize the optimization. Our framework not only supports an arbitrary number of input photographs, but also at high resolution. We demonstrate and evaluate our deep inverse rendering solution on a wide variety of publicly available datasets.},
  keywords = {auto-encoder,deep learning,material capture,SVBRDF}
}

@inproceedings{ge3DHandShape2019,
  title = {{{3D Hand Shape}} and {{Pose Estimation From}} a {{Single RGB Image}}},
  author = {Ge, Liuhao and Ren, Zhou and Li, Yuncheng and Xue, Zehao and Wang, Yingying and Cai, Jianfei and Yuan, Junsong},
  date = {2019},
  pages = {10833--10842},
  doi = {10.1109/cvpr.2019.01109},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Ge_3D_Hand_Shape_and_Pose_Estimation_From_a_Single_RGB_CVPR_2019_paper.html},
  urldate = {2022-04-11},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@misc{gemiciNormalizingFlowsRiemannian2016,
  title = {Normalizing {{Flows}} on {{Riemannian Manifolds}}},
  author = {Gemici, Mevlana C. and Rezende, Danilo and Mohamed, Shakir},
  date = {2016-11-09},
  number = {arXiv:1611.02304},
  eprint = {1611.02304},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1611.02304},
  urldate = {2022-05-29},
  abstract = {We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces \$\textbackslash mathbf\{R\}\^n\$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere \$\textbackslash mathbf\{S\}\^n\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{georgievLightTransportSimulation2012,
  title = {Light Transport Simulation with Vertex Connection and Merging},
  author = {Georgiev, Iliyan and Křivánek, Jaroslav and Davidovič, Tomáš and Slusallek, Philipp},
  date = {2012-11-01},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {31},
  number = {6},
  pages = {192:1--192:10},
  issn = {0730-0301},
  doi = {10.1145/2366145.2366211},
  url = {https://doi.org/10.1145/2366145.2366211},
  urldate = {2022-08-14},
  abstract = {Developing robust light transport simulation algorithms that are capable of dealing with arbitrary input scenes remains an elusive challenge. Although efficient global illumination algorithms exist, an acceptable approximation error in a reasonable amount of time is usually only achieved for specific types of input scenes. To address this problem, we present a reformulation of photon mapping as a bidirectional path sampling technique for Monte Carlo light transport simulation. The benefit of our new formulation is twofold. First, it makes it possible, for the first time, to explain in a formal manner the relative efficiency of photon mapping and bidirectional path tracing, which have so far been considered conceptually incompatible solutions to the light transport problem. Second, it allows for a seamless integration of the two methods into a more robust combined rendering algorithm via multiple importance sampling. A progressive version of this algorithm is consistent and efficiently handles a wide variety of lighting conditions, ranging from direct illumination, diffuse and glossy inter-reflections, to specular-diffuse-specular light transport. Our analysis shows that this algorithm inherits the high asymptotic performance from bidirectional path tracing for most light path types, while benefiting from the efficiency of photon mapping for specular-diffuse-specular lighting effects.},
  keywords = {bidirectional path tracing,density estimation,global illumination,importance sampling,light transport,photon mapping}
}

@article{georgoulisReflectanceNaturalIllumination2018,
  title = {Reflectance and {{Natural Illumination}} from {{Single-Material Specular Objects Using Deep Learning}}},
  author = {Georgoulis, Stamatios and Rematas, Konstantinos and Ritschel, Tobias and Gavves, Efstratios and Fritz, Mario and Van Gool, Luc and Tuytelaars, Tinne},
  date = {2018-08},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {40},
  number = {8},
  pages = {1932--1947},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2017.2742999},
  abstract = {In this paper, we present a method that estimates reflectance and illumination information from a single image depicting a single-material specular object from a given class under natural illumination. We follow a data-driven, learning-based approach trained on a very large dataset, but in contrast to earlier work we do not assume one or more components (shape, reflectance, or illumination) to be known. We propose a two-step approach, where we first estimate the object's reflectance map, and then further decompose it into reflectance and illumination. For the first step, we introduce a Convolutional Neural Network (CNN) that directly predicts a reflectance map from the input image itself, as well as an indirect scheme that uses additional supervision, first estimating surface orientation and afterwards inferring the reflectance map using a learning-based sparse data interpolation technique. For the second step, we suggest a CNN architecture to reconstruct both Phong reflectance parameters and high-resolution spherical illumination maps from the reflectance map. We also propose new datasets to train these CNNs. We demonstrate the effectiveness of our approach for both steps by extensive quantitative and qualitative evaluation in both synthetic and real data as well as through numerous applications, that show improvements over the state-of-the-art.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {convolutional neural networks,intrinsic images,Lighting,natural illumination,reflectance,Reflectance maps,Shape,specular shading,Three-dimensional displays,Training,Two dimensional displays}
}

@inproceedings{ghoshBRDFAcquisitionBasis2007,
  title = {{{BRDF Acquisition}} with {{Basis Illumination}}},
  booktitle = {2007 {{IEEE}} 11th {{International Conference}} on {{Computer Vision}}},
  author = {Ghosh, Abhijeet and Achutha, Shruthi and Heidrich, Wolfgang and O'Toole, Matthew},
  date = {2007},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Rio de Janeiro, Brazil}},
  doi = {10.1109/ICCV.2007.4408935},
  url = {http://ieeexplore.ieee.org/document/4408935/},
  urldate = {2022-09-01},
  eventtitle = {2007 {{IEEE}} 11th {{International Conference}} on {{Computer Vision}}},
  isbn = {978-1-4244-1630-1}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010-03-31},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2022-09-05},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  urldate = {2022-04-14},
  abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
  keywords = {⛔ No DOI found}
}

@article{granskogNeuralSceneGraph2021,
  title = {Neural Scene Graph Rendering},
  author = {Granskog, Jonathan and Schnabel, Till N. and Rousselle, Fabrice and Novák, Jan},
  date = {2021-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3450626.3459848},
  url = {https://dl.acm.org/doi/10.1145/3450626.3459848},
  urldate = {2022-01-24},
  abstract = {We present a neural scene graph---a modular and controllable representation of scenes with elements that are learned from data. We focus on the forward rendering problem, where the scene graph is provided by the user and references learned elements. The elements correspond to geometry and material definitions of scene objects and constitute the leaves of the graph; we store them as high-dimensional vectors. The position and appearance of scene objects can be adjusted in an artist-friendly manner via familiar transformations, e.g. translation, bending, or color hue shift, which are stored in the inner nodes of the graph. In order to apply a (non-linear) transformation to a learned vector, we adopt the concept of linearizing a problem by lifting it into higher dimensions: we first encode the transformation into a high-dimensional matrix and then apply it by standard matrix-vector multiplication. The transformations are encoded using neural networks. We render the scene graph using a streaming neural renderer, which can handle graphs with a varying number of objects, and thereby facilitates scalability. Our results demonstrate a precise control over the learned object representations in a number of animated 2D and 3D scenes. Despite the limited visual complexity, our work presents a step towards marrying traditional editing mechanisms with learned representations, and towards high-quality, controllable neural rendering.},
  langid = {english}
}

@book{griewankEvaluatingDerivativesPrinciples2008,
  title = {Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation},
  author = {Griewank, Andreas and Walther, Andrea},
  date = {2008},
  edition = {2},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717761},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898717761},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
  isbn = {978-0-89871-659-7},
  keywords = {.skimmed}
}

@inproceedings{griffithsCuriositydriven3DObject2021a,
  title = {Curiosity-Driven {{3D Object Detection Without Labels}}},
  booktitle = {2021 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Griffiths, David and Boehm, Jan and Ritschel, Tobias},
  date = {2021-12},
  pages = {525--534},
  issn = {2475-7888},
  doi = {10.1109/3DV53792.2021.00062},
  abstract = {In this paper we set out to solve the task of 6-DOF 3D object detection from 2D images, where the only supervision is a geometric representation of the objects we aim to find. In doing so, we remove the need for 6-DOF labels (i.e. position, orientation etc.), allowing our network to be trained on unlabeled images in a self-supervised manner. We achieve this through a neural network which learns an explicit scene parameterization which is subsequently passed into a differentiable renderer. We analyze why analysis-by-synthesis-like losses for supervision of 3D scene structure using differentiable rendering is not practical, as it almost always gets stuck in local minima of visual ambiguities. This can be overcome by a novel form of training, where an additional network is employed to steer the optimization itself to explore the entire parameter space i.e. to be curious, and hence, to resolve those ambiguities and find workable minima.},
  eventtitle = {2021 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  keywords = {n/a,Neural networks,Object detection,Rendering (computer graphics),Task analysis,Three-dimensional displays,Training,Visualization}
}

@article{guarneraBRDFRepresentationAcquisition2016a,
  title = {{{BRDF Representation}} and {{Acquisition}}},
  author = {Guarnera, D. and Guarnera, G.c. and Ghosh, A. and Denk, C. and Glencross, M.},
  date = {2016},
  journaltitle = {Computer Graphics Forum},
  volume = {35},
  number = {2},
  pages = {625--650},
  issn = {1467-8659},
  doi = {10.1111/cgf.12867},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12867},
  urldate = {2022-09-03},
  abstract = {Photorealistic rendering of real world environments is important in a range of different areas; including Visual Special effects, Interior/Exterior Modelling, Architectural Modelling, Cultural Heritage, Computer Games and Automotive Design. Currently, rendering systems are able to produce photorealistic simulations of the appearance of many real-world materials. In the real world, viewer perception of objects depends on the lighting and object/material/surface characteristics, the way a surface interacts with the light and on how the light is reflected, scattered, absorbed by the surface and the impact these characteristics have on material appearance. In order to re-produce this, it is necessary to understand how materials interact with light. Thus the representation and acquisition of material models has become such an active research area. This survey of the state-of-the-art of BRDF Representation and Acquisition presents an overview of BRDF (Bidirectional Reflectance Distribution Function) models used to represent surface/material reflection characteristics, and describes current acquisition methods for the capture and rendering of photorealistic materials.},
  langid = {english},
  keywords = {and texture,Categories and Subject Descriptors (according to ACM CCS),I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation,I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Colour,I.6.8 Computer Graphics: Types of simulation—Monte Carlo,shading,shadowing},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12867}
}

@article{guoMaterialGANReflectanceCapture2020,
  title = {{{MaterialGAN}}: Reflectance Capture Using a Generative {{SVBRDF}} Model},
  shorttitle = {{{MaterialGAN}}},
  author = {Guo, Yu and Smith, Cameron and Hašan, Miloš and Sunkavalli, Kalyan and Zhao, Shuang},
  date = {2020-11-26},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {6},
  pages = {254:1--254:13},
  issn = {0730-0301},
  doi = {10.1145/3414685.3417779},
  url = {https://doi.org/10.1145/3414685.3417779},
  urldate = {2022-09-02},
  abstract = {We address the problem of reconstructing spatially-varying BRDFs from a small set of image measurements. This is a fundamentally under-constrained problem, and previous work has relied on using various regularization priors or on capturing many images to produce plausible results. In this work, we present MaterialGAN, a deep generative convolutional network based on StyleGAN2, trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN can be used as a powerful material prior in an inverse rendering framework: we optimize in its latent representation to generate material maps that match the appearance of the captured images when rendered. We demonstrate this framework on the task of reconstructing SVBRDFs from images captured under flash illumination using a hand-held mobile phone. Our method succeeds in producing plausible material maps that accurately reproduce the target images, and outperforms previous state-of-the-art material capture methods in evaluations on both synthetic and real data. Furthermore, our GAN-based latent space allows for high-level semantic material editing operations such as generating material variations and material morphing.},
  keywords = {generative adversarial network,SVBRDF capture}
}

@article{guoNeural3DScene2022,
  title = {Neural {{3D Scene Reconstruction}} with the {{Manhattan-world Assumption}}},
  author = {Guo, Haoyu and Peng, Sida and Lin, Haotong and Wang, Qianqian and Zhang, Guofeng and Bao, Hujun and Zhou, Xiaowei},
  date = {2022-05-05},
  doi = {10.48550/arXiv.2205.02836},
  url = {https://arxiv.org/abs/2205.02836v2},
  urldate = {2022-06-13},
  abstract = {This paper addresses the challenge of reconstructing 3D indoor scenes from multi-view images. Many previous works have shown impressive reconstruction results on textured objects, but they still have difficulty in handling low-textured planar regions, which are common in indoor scenes. An approach to solving this issue is to incorporate planer constraints into the depth map estimation in multi-view stereo-based methods, but the per-view plane estimation and depth optimization lack both efficiency and multi-view consistency. In this work, we show that the planar constraints can be conveniently integrated into the recent implicit neural representation-based reconstruction methods. Specifically, we use an MLP network to represent the signed distance function as the scene geometry. Based on the Manhattan-world assumption, planar constraints are employed to regularize the geometry in floor and wall regions predicted by a 2D semantic segmentation network. To resolve the inaccurate segmentation, we encode the semantics of 3D points with another MLP and design a novel loss that jointly optimizes the scene geometry and semantics in 3D space. Experiments on ScanNet and 7-Scenes datasets show that the proposed method outperforms previous methods by a large margin on 3D reconstruction quality. The code is available at https://zju3dv.github.io/manhattan\_sdf.},
  langid = {english}
}

@unpublished{haHyperNetworks2016,
  title = {{{HyperNetworks}}},
  author = {Ha, David and Dai, Andrew and Le, Quoc V.},
  date = {2016-12-01},
  eprint = {1609.09106},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1609.09106},
  url = {http://arxiv.org/abs/1609.09106},
  urldate = {2022-01-19},
  abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{hanMEgATrackMonochromeEgocentric2020,
  title = {{{MEgATrack}}: Monochrome Egocentric Articulated Hand-Tracking for Virtual Reality},
  shorttitle = {{{MEgATrack}}},
  author = {Han, Shangchen and Liu, Beibei and Cabezas, Randi and Twigg, Christopher D. and Zhang, Peizhao and Petkau, Jeff and Yu, Tsz-Ho and Tai, Chun-Jung and Akbay, Muzaffer and Wang, Zheng and Nitzan, Asaf and Dong, Gang and Ye, Yuting and Tao, Lingling and Wan, Chengde and Wang, Robert},
  date = {2020-07-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {4},
  pages = {87:87:1--87:87:13},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392452},
  url = {https://doi.org/10.1145/3386569.3392452},
  urldate = {2022-04-19},
  abstract = {We present a system for real-time hand-tracking to drive virtual and augmented reality (VR/AR) experiences. Using four fisheye monochrome cameras, our system generates accurate and low-jitter 3D hand motion across a large working volume for a diverse set of users. We achieve this by proposing neural network architectures for detecting hands and estimating hand keypoint locations. Our hand detection network robustly handles a variety of real world environments. The keypoint estimation network leverages tracking history to produce spatially and temporally consistent poses. We design scalable, semi-automated mechanisms to collect a large and diverse set of ground truth data using a combination of manual annotation and automated tracking. Additionally, we introduce a detection-by-tracking method that increases smoothness while reducing the computational cost; the optimized system runs at 60Hz on PC and 30Hz on a mobile processor. Together, these contributions yield a practical system for capturing a user's hands and is the default feature on the Oculus Quest VR headset powering input and social presence.},
  keywords = {hand tracking,motion capture,virtual reality}
}

@inproceedings{hassonLearningJointReconstruction2019,
  title = {Learning {{Joint Reconstruction}} of {{Hands}} and {{Manipulated Objects}}},
  author = {Hasson, Yana and Varol, Gul and Tzionas, Dimitrios and Kalevatykh, Igor and Black, Michael J. and Laptev, Ivan and Schmid, Cordelia},
  date = {2019},
  pages = {11807--11816},
  doi = {10.1109/cvpr.2019.01208},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Hasson_Learning_Joint_Reconstruction_of_Hands_and_Manipulated_Objects_CVPR_2019_paper.html},
  urldate = {2022-04-11},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{heComprehensivePhysicalModel1991,
  title = {A Comprehensive Physical Model for Light Reflection},
  author = {He, Xiao D. and Torrance, Kenneth E. and Sillion, François X. and Greenberg, Donald P.},
  date = {1991-07-01},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {25},
  number = {4},
  pages = {175--186},
  issn = {0097-8930},
  doi = {10.1145/127719.122738},
  url = {https://doi.org/10.1145/127719.122738},
  urldate = {2022-09-08},
  abstract = {A new general reflectance model for computer graphics is presented. The model is based on physical optics and describes specular, directional diffuse, and uniform diffuse reflection by a surface. The reflected light pattern depends on wavelength, incidence angle, two surface roughness parameters, and surface refractive index. The formulation is self consistent in terms of polarization, surface roughness, masking/shadowing, and energy. The model applies to a wide range of materials and surface finishes and provides a smooth transition from diffuse-like to specular reflection as the wavelength and incidence angle are increased or the surface roughness is decreased. The model is analytic and suitable for Computer Graphics applications. Predicted reflectance distributions compare favorably with experiment. The model is applied to metallic, nonmetallic, and plastic materials, with smooth and rough surfaces.},
  keywords = {comparison with experiment,reflectance model,specular and diffuse reflection}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/cvpr.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization}
}

@inproceedings{heDelvingDeepRectifiers2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12},
  pages = {1026--1034},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Adaptation models,Biological neural networks,Computational modeling,Gaussian distribution,Testing,Training}
}

@unpublished{heitzSlicedWassersteinLoss2021,
  title = {A {{Sliced Wasserstein Loss}} for {{Neural Texture Synthesis}}},
  author = {Heitz, Eric and Vanhoey, Kenneth and Chambon, Thomas and Belcour, Laurent},
  date = {2021-03-11},
  eprint = {2006.07229},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.07229},
  url = {http://arxiv.org/abs/2006.07229},
  urldate = {2022-01-19},
  abstract = {We address the problem of computing a textural loss based on the statistics extracted from the feature activations of a convolutional neural network optimized for object recognition (e.g. VGG-19). The underlying mathematical problem is the measure of the distance between two distributions in feature space. The Gram-matrix loss is the ubiquitous approximation for this problem but it is subject to several shortcomings. Our goal is to promote the Sliced Wasserstein Distance as a replacement for it. It is theoretically proven,practical, simple to implement, and achieves results that are visually superior for texture synthesis by optimization or training generative neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{henzlerEscapingPlatoCave2019,
  title = {Escaping {{Plato}}’s {{Cave}}: {{3D Shape From Adversarial Rendering}}},
  shorttitle = {Escaping {{Plato}}’s {{Cave}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Henzler, Philipp and Mitra, Niloy and Ritschel, Tobias},
  date = {2019-10},
  pages = {9983--9992},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.01008},
  abstract = {We introduce PLATONICGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i. e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on accurated (e. g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PLATONICGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PLATONICGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Cameras,Generators,Image reconstruction,Rendering (computer graphics),Shape,Three-dimensional displays,Two dimensional displays}
}

@article{henzlerGenerativeModellingBRDF2021a,
  ids = {henzlerGenerativeModellingBRDF2021},
  title = {Generative Modelling of {{BRDF}} Textures from Flash Images},
  author = {Henzler, Philipp and Deschaintre, Valentin and Mitra, Niloy J. and Ritschel, Tobias},
  date = {2021-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {6},
  eprint = {2102.11861},
  eprinttype = {arxiv},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3478513.3480507},
  url = {https://dl.acm.org/doi/10.1145/3478513.3480507},
  urldate = {2022-10-12},
  abstract = {We learn a latent space for easy capture, consistent interpolation, and efficient reproduction of visual material appearance. When users provide a photo of a stationary natural material captured under flashlight illumination, first it is converted into a latent material code. Then, in the second step, conditioned on the material code, our method produces an infinite and diverse spatial field of BRDF model parameters (diffuse albedo, normals, roughness, specular albedo) that subsequently allows rendering in complex scenes and illuminations, matching the appearance of the input photograph. Technically, we jointly embed all flash images into a latent space using a convolutional encoder, and -conditioned on these latent codes- convert random spatial fields into fields of BRDF parameters using a convolutional neural network (CNN). We condition these BRDF parameters to match the visual characteristics (statistics and spectra of visual features) of the input under matching light. A user study compares our approach favorably to previous work, even those with access to BRDF supervision. Project webpage: https://henzler.github.io/publication/neuralmaterial/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@inproceedings{henzlerLearningNeural3D2020,
  title = {Learning a {{Neural 3D Texture Space From 2D Exemplars}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Henzler, Philipp and Mitra, Niloy J. and Ritschel, Tobias},
  date = {2020-06},
  pages = {8353--8361},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/cvpr42600.2020.00838},
  url = {https://ieeexplore.ieee.org/document/9156317/},
  urldate = {2021-12-30},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5}
}

@inproceedings{henzlerUnsupervisedLearning3D2021,
  title = {Unsupervised {{Learning}} of {{3D Object Categories From Videos}} in the {{Wild}}},
  author = {Henzler, Philipp and Reizenstein, Jeremy and Labatut, Patrick and Shapovalov, Roman and Ritschel, Tobias and Vedaldi, Andrea and Novotny, David},
  date = {2021},
  pages = {4700--4709},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Henzler_Unsupervised_Learning_of_3D_Object_Categories_From_Videos_in_the_CVPR_2021_paper.html},
  urldate = {2021-12-30},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english}
}

@article{hermosillaDeeplearningLatentSpace2019,
  title = {Deep-Learning the {{Latent Space}} of {{Light Transport}}},
  author = {Hermosilla, P. and Maisch, S. and Ritschel, T. and Ropinski, T.},
  date = {2019},
  journaltitle = {Computer Graphics Forum},
  volume = {38},
  number = {4},
  pages = {207--217},
  issn = {1467-8659},
  doi = {10.1111/cgf.13783},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13783},
  urldate = {2022-10-06},
  abstract = {We suggest a method to directly deep-learn light transport, i. e., the mapping from a 3D geometry-illumination-material configuration to a shaded 2D image. While many previous learning methods have employed 2D convolutional neural networks applied to images, we show for the first time that light transport can be learned directly in 3D. The benefit of 3D over 2D is, that the former can also correctly capture illumination effects related to occluded and/or semi-transparent geometry. To learn 3D light transport, we represent the 3D scene as an unstructured 3D point cloud, which is later, during rendering, projected to the 2D output image. Thus, we suggest a two-stage operator comprising a 3D network that first transforms the point cloud into a latent representation, which is later on projected to the 2D output image using a dedicated 3D-2D network in a second step. We will show that our approach results in improved quality in terms of temporal coherence while retaining most of the computational efficiency of common 2D methods. As a consequence, the proposed two stage-operator serves as a valuable extension to modern deferred shading approaches.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13783}
}

@unpublished{hospedalesMetaLearningNeuralNetworks2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  date = {2020-11-07},
  eprint = {2004.05439},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2004.05439},
  url = {http://arxiv.org/abs/2004.05439},
  urldate = {2022-03-07},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{huangNeuralAutoregressiveFlows2018,
  title = {Neural {{Autoregressive Flows}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  date = {2018-07-03},
  pages = {2078--2087},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/huang18d.html},
  urldate = {2022-05-28},
  abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{huDeepBRDFDeepRepresentation2020,
  title = {{{DeepBRDF}}: {{A Deep Representation}} for {{Manipulating Measured BRDF}}},
  shorttitle = {{{DeepBRDF}}},
  author = {Hu, Bingyang and Guo, Jie and Chen, Yanjun and Li, Mengtian and Guo, Yanwen},
  date = {2020-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {39},
  number = {2},
  pages = {157--166},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13920},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13920},
  urldate = {2022-01-19},
  langid = {english}
}

@inproceedings{huMetaSRMagnificationArbitraryNetwork2019,
  title = {Meta-{{SR}}: {{A Magnification-Arbitrary Network}} for {{Super-Resolution}}},
  shorttitle = {Meta-{{SR}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hu, Xuecai and Mu, Haoyuan and Zhang, Xiangyu and Wang, Zilei and Tan, Tieniu and Sun, Jian},
  date = {2019-06},
  pages = {1575--1584},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00167},
  abstract = {Recent research on super-resolution has achieved great success due to the development of deep convolutional neural networks (DCNNs). However, super-resolution of arbitrary scale factor has been ignored for a long time. Most previous researchers regard super-resolution of differentscale factors as independent tasks. They train a specific model for each scale factor which is inefficient in computing, and prior work only take the super-resolution of several integer scale factors into consideration. In this work,we propose a novel method called Meta-SR to firstly solve super-resolution of arbitrary scale factor (including non-integer scale factors) with a single model. In our Meta-SR,the Meta-Upscale Module is proposed to replace the traditional upscale module. For arbitrary scale factor, the Meta-Upscale Module dynamically predicts the weights of the up-scale filters by taking the scale factor as input and use these weights to generate the HR image of arbitrary size. For any low-resolution image, our Meta-SR can continuously zoomin it with arbitrary scale factor by only using a single model.We evaluated the proposed method through extensive experiments on widely used benchmark datasets on single image super-resolution. The experimental results show the superiority of our Meta-Upscale.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Computational Photography,Low-level Vision}
}

@misc{ignatovRealTimeQuantizedImage2021,
  title = {Real-{{Time Quantized Image Super-Resolution}} on {{Mobile NPUs}}, {{Mobile AI}} 2021 {{Challenge}}: {{Report}}},
  shorttitle = {Real-{{Time Quantized Image Super-Resolution}} on {{Mobile NPUs}}, {{Mobile AI}} 2021 {{Challenge}}},
  author = {Ignatov, Andrey and Timofte, Radu and Denna, Maurizio and Younes, Abdel and Lek, Andrew and Ayazoglu, Mustafa and Liu, Jie and Du, Zongcai and Guo, Jiaming and Zhou, Xueyi and Jia, Hao and Yan, Youliang and Zhang, Zexin and Chen, Yixin and Peng, Yunbo and Lin, Yue and Zhang, Xindong and Zeng, Hui and Zeng, Kun and Li, Peirong and Liu, Zhihuang and Xue, Shiqi and Wang, Shengpeng},
  date = {2021-05-17},
  number = {arXiv:2105.07825},
  eprint = {2105.07825},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.07825},
  urldate = {2022-05-26},
  abstract = {Image super-resolution is one of the most popular computer vision problems with many important applications to mobile devices. While many solutions have been proposed for this task, they are usually not optimized even for common smartphone AI hardware, not to mention more constrained smart TV platforms that are often supporting INT8 inference only. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based image super-resolution solutions that can demonstrate a real-time performance on mobile or edge NPUs. For this, the participants were provided with the DIV2K dataset and trained quantized models to do an efficient 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated NPU capable of accelerating quantized neural networks. The proposed solutions are fully compatible with all major mobile AI accelerators and are capable of reconstructing Full HD images under 40-60 ms while achieving high fidelity results. A detailed description of all models developed in the challenge is provided in this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@misc{ignatovRealTimeVideoSuperResolution2021,
  title = {Real-{{Time Video Super-Resolution}} on {{Smartphones}} with {{Deep Learning}}, {{Mobile AI}} 2021 {{Challenge}}: {{Report}}},
  shorttitle = {Real-{{Time Video Super-Resolution}} on {{Smartphones}} with {{Deep Learning}}, {{Mobile AI}} 2021 {{Challenge}}},
  author = {Ignatov, Andrey and Romero, Andres and Kim, Heewon and Timofte, Radu and Ho, Chiu Man and Meng, Zibo and Lee, Kyoung Mu and Chen, Yuxiang and Wang, Yutong and Long, Zeyu and Wang, Chenhao and Chen, Yifei and Xu, Boshen and Gu, Shuhang and Duan, Lixin and Li, Wen and Bofei, Wang and Diankai, Zhang and Chengjian, Zheng and Shaoli, Liu and Si, Gao and Xiaofeng, Zhang and Kaidi, Lu and Tianyu, Xu and Hui, Zheng and Gao, Xinbo and Wang, Xiumei and Guo, Jiaming and Zhou, Xueyi and Jia, Hao and Yan, Youliang},
  date = {2021-05-17},
  number = {arXiv:2105.08826},
  eprint = {2105.08826},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.08826},
  urldate = {2022-05-26},
  abstract = {Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs. The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@inproceedings{isolaImageToImageTranslationConditional2017,
  title = {Image-{{To-Image Translation With Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2017},
  pages = {1125--1134},
  url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html},
  urldate = {2022-04-14},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{jacobsenMetadescentOnlineContinual2019,
  title = {Meta-Descent for Online, Continual Prediction},
  booktitle = {Proceedings of the {{Thirty-Third AAAI Conference}} on {{Artificial Intelligence}} and {{Thirty-First Innovative Applications}} of {{Artificial Intelligence Conference}} and {{Ninth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}},
  author = {Jacobsen, Andrew and Schlegel, Matthew and Linke, Cameron and Degris, Thomas and White, Adam and White, Martha},
  date = {2019-01-27},
  series = {{{AAAI}}'19/{{IAAI}}'19/{{EAAI}}'19},
  pages = {3943--3950},
  publisher = {{AAAI Press}},
  location = {{Honolulu, Hawaii, USA}},
  doi = {10.1609/aaai.v33i01.33013943},
  url = {https://doi.org/10.1609/aaai.v33i01.33013943},
  urldate = {2022-09-16},
  abstract = {This paper investigates different vector step-size adaptation approaches for non-stationary online, continual prediction problems. Vanilla stochastic gradient descent can be considerably improved by scaling the update with a vector of appropriately chosen step-sizes. Many methods, including Ada-Grad, RMSProp, and AMSGrad, keep statistics about the learning process to approximate a second order update—a vector approximation of the inverse Hessian. Another family of approaches use meta-gradient descent to adapt the step-size parameters to minimize prediction error. These metadescent strategies are promising for non-stationary problems, but have not been as extensively explored as quasi-second order methods. We first derive a general, incremental metadescent algorithm, called AdaGain, designed to be applicable to a much broader range of algorithms, including those with semi-gradient updates or even those with accelerations, such as RMSProp. We provide an empirical comparison of methods from both families. We conclude that methods from both families can perform well, but in non-stationary prediction problems the meta-descent methods exhibit advantages. Our method is particularly robust across several prediction problems, and is competitive with the state-of-the-art method on a large-scale, time-series prediction problem on real data from a mobile robot.},
  isbn = {978-1-57735-809-1}
}

@software{jakobMitsubaRenderer2010,
  title = {Mitsuba Renderer},
  author = {Jakob, Wenzel},
  date = {2010},
  url = {http://www.mitsuba-renderer.org},
  annotation = {http://www.mitsuba-renderer.org}
}

@software{jakobMitsubaRenderer2022,
  title = {Mitsuba 3 Renderer},
  author = {Jakob, Wenzel and Speierer, Sébastien and Roussel, Nicolas and Nimier-David, Merlin and Vicini, Delio and Zeltner, Tizian and Nicolet, Baptiste and Crespo, Miguel and Leroy, Vincent and Zhang, Ziyi},
  date = {2022},
  url = {https://mitsuba-renderer.org},
  version = {3.0.1},
  annotation = {https://mitsuba-renderer.org}
}

@article{jianboshiNormalizedCutsImage2000,
  title = {Normalized Cuts and Image Segmentation},
  author = {{Jianbo Shi} and Malik, J.},
  year = {Aug./2000},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
  volume = {22},
  number = {8},
  pages = {888--905},
  issn = {01628828},
  doi = {10.1109/34.868688},
  url = {http://ieeexplore.ieee.org/document/868688/},
  urldate = {2022-05-15}
}

@inproceedings{jiangRecognizingVectorGraphics2021,
  title = {Recognizing {{Vector Graphics}} without {{Rasterization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {JIANG, XINYANG and LIU, LU and Shan, Caihua and Shen, Yifei and Dong, Xuanyi and Li, Dongsheng},
  date = {2021},
  volume = {34},
  pages = {24569--24580},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2021/hash/cdf1035c34ec380218a8cc9a43d438f9-Abstract.html},
  urldate = {2022-04-15},
  abstract = {In this paper, we consider a different data format for images: vector graphics. In contrast to raster graphics which are widely used in image recognition, vector graphics can be scaled up or down into any resolution without aliasing or information loss, due to the analytic representation of the primitives in the document. Furthermore, vector graphics are able to give extra structural information on how low-level elements group together to form high level shapes or structures. These merits of graphic vectors have not been fully leveraged in existing methods.  To explore this data format, we target on the fundamental recognition tasks: object localization and classification. We propose an efficient CNN-free pipeline that does not render the graphic into pixels (i.e. rasterization), and takes textual document of the vector graphics as input, called YOLaT (You Only Look at Text). YOLaT builds multi-graphs to model the structural and spatial information in vector graphics, and a dual-stream graph neural network is proposed to detect objects from the graph. Our experiments show that by directly operating on vector graphics, YOLaT outperforms raster-graphic based object detection baselines in terms of both average precision and efficiency.},
  keywords = {⛔ No DOI found}
}

@article{jinDeepConvolutionalNeural2017,
  title = {Deep {{Convolutional Neural Network}} for {{Inverse Problems}} in {{Imaging}}},
  author = {Jin, Kyong Hwan and McCann, Michael T. and Froustey, Emmanuel and Unser, Michael},
  date = {2017-09},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {26},
  number = {9},
  pages = {4509--4522},
  issn = {1941-0042},
  doi = {10.1109/tip.2017.2713099},
  abstract = {In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 × 512 image on the GPU.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {biomedical imaging,biomedical signal processing,computed tomography,Computed tomography,Convolution,image reconstruction,Image reconstruction,Image restoration,Inverse problems,Iterative methods,magnetic resonance imaging,Neural networks,reconstruction algorithms,tomography}
}

@article{joachimsMakingLargeScaleSupport1999,
  title = {Making {{Large-Scale Support Vector Machine Learning Practical}}},
  author = {Joachims, Thorsten},
  date = {1999},
  journaltitle = {Advances in kernel methods: support vector learning},
  pages = {169},
  publisher = {{MIT press}},
  keywords = {⛔ No DOI found}
}

@article{kaddourProbabilisticActiveMetaLearning2020,
  title = {Probabilistic {{Active Meta-Learning}}},
  author = {Kaddour, Jean and Sæmundsson, Steindór and Deisenroth, Marc Peter},
  date = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/arxiv.2007.08949},
  url = {https://arxiv.org/abs/2007.08949},
  urldate = {2022-03-31},
  abstract = {Data-efficient learning algorithms are essential in many practical applications where data collection is expensive, e.g., in robotics due to the wear and tear. To address this problem, meta-learning algorithms use prior experience about tasks to learn new, related tasks efficiently. Typically, a set of training tasks is assumed given or randomly chosen. However, this setting does not take into account the sequential nature that naturally arises when training a model from scratch in real-life: how do we collect a set of training tasks in a data-efficient manner? In this work, we introduce task selection based on prior experience into a meta-learning algorithm by conceptualizing the learner and the active meta-learning setting using a probabilistic latent variable model. We provide empirical evidence that our approach improves data-efficiency when compared to strong baselines on simulated robotic experiments.},
  version = {2},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{kangEfficientReflectanceCapture2018,
  title = {Efficient Reflectance Capture Using an Autoencoder},
  author = {Kang, Kaizhang and Chen, Zimin and Wang, Jiaping and Zhou, Kun and Wu, Hongzhi},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {4},
  pages = {127:1--127:10},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201279},
  url = {https://doi.org/10.1145/3197517.3201279},
  urldate = {2022-09-04},
  abstract = {We propose a novel framework that automatically learns the lighting patterns for efficient reflectance acquisition, as well as how to faithfully reconstruct spatially varying anisotropic BRDFs and local frames from measurements under such patterns. The core of our framework is an asymmetric deep autoencoder, consisting of a nonnegative, linear encoder which directly corresponds to the lighting patterns used in physical acquisition, and a stacked, nonlinear decoder which computationally recovers the BRDF information from captured photographs. The autoencoder is trained with a large amount of synthetic reflectance data, and can adapt to various factors, including the geometry of the setup and the properties of appearance. We demonstrate the effectiveness of our framework on a wide range of physical materials, using as few as 16 \textasciitilde{} 32 lighting patterns, which correspond to 12 \textasciitilde{} 25 seconds of acquisition time. We also validate our results with the ground truth data and captured photographs. Our framework is useful for increasing the efficiency in both novel and existing acquisition setups.},
  keywords = {lighting patterns,optimal sampling,reflectance acquisition,SV-BRDF}
}

@article{kangLearningEfficientIllumination2019,
  title = {Learning Efficient Illumination Multiplexing for Joint Capture of Reflectance and Shape},
  author = {Kang, Kaizhang and Xie, Cihui and He, Chengan and Yi, Mingqi and Gu, Minyi and Chen, Zimin and Zhou, Kun and Wu, Hongzhi},
  date = {2019-12-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {6},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3355089.3356492},
  url = {https://dl.acm.org/doi/10.1145/3355089.3356492},
  urldate = {2022-09-04},
  abstract = {We propose a novel framework that automatically learns the lighting patterns for efficient, joint acquisition of unknown reflectance and shape. The core of our framework is a deep neural network, with a shared linear encoder that directly corresponds to the lighting patterns used in physical acquisition, as well as non-linear decoders that output per-pixel normal and diffuse / specular information from photographs. We exploit the diffuse and normal information from multiple views to reconstruct a detailed 3D shape, and then fit BRDF parameters to the diffuse / specular information, producing texture maps as reflectance results. We demonstrate the effectiveness of the framework with physical objects that vary considerably in reflectance and shape, acquired with as few as 16 \textasciitilde{} 32 lighting patterns that correspond to 7 \textasciitilde{} 15 seconds of per-view acquisition time. Our framework is useful for optimizing the efficiency in both novel and existing setups, as it can automatically adapt to various factors, including the geometry / the lighting layout of the device and the properties of appearance.},
  langid = {english}
}

@inproceedings{karnewar3inGANLearning3D2022,
  title = {{{3inGAN}}: {{Learning}} a {{3D}} Generative Model from Images of a Self-Similar Scene},
  booktitle = {Proc. {{3D}} Vision ({{3DV}})},
  author = {Karnewar, Animesh and Ritschel, Tobias and Wang, Oliver and Mitra, Niloy},
  date = {2022},
  keywords = {⛔ No DOI found}
}

@misc{karnewarReLUFieldsLittle2022,
  title = {{{ReLU Fields}}: {{The Little Non-linearity That Could}}},
  shorttitle = {{{ReLU Fields}}},
  author = {Karnewar, Animesh and Ritschel, Tobias and Wang, Oliver and Mitra, Niloy J.},
  date = {2022-05-22},
  eprint = {2205.10824},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3528233.3530707},
  url = {http://arxiv.org/abs/2205.10824},
  urldate = {2022-06-25},
  abstract = {In many recent works, multi-layer perceptions (MLPs) have been shown to be suitable for modeling complex spatially-varying functions including images and 3D scenes. Although the MLPs are able to represent complex scenes with unprecedented quality and memory footprint, this expressive power of the MLPs, however, comes at the cost of long training and inference times. On the other hand, bilinear/trilinear interpolation on regular grid based representations can give fast training and inference times, but cannot match the quality of MLPs without requiring significant additional memory. Hence, in this work, we investigate what is the smallest change to grid-based representations that allows for retaining the high fidelity result of MLPs while enabling fast reconstruction and rendering times. We introduce a surprisingly simple change that achieves this task -- simply allowing a fixed non-linearity (ReLU) on interpolated grid values. When combined with coarse to-fine optimization, we show that such an approach becomes competitive with the state-of-the-art. We report results on radiance fields, and occupancy fields, and compare against multiple existing alternatives. Code and data for the paper are available at https://geometry.cs.ucl.ac.uk/projects/2022/relu\_fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@unpublished{katharopoulosNotAllSamples2019,
  title = {Not {{All Samples Are Created Equal}}: {{Deep Learning}} with {{Importance Sampling}}},
  shorttitle = {Not {{All Samples Are Created Equal}}},
  author = {Katharopoulos, Angelos and Fleuret, François},
  date = {2019-10-28},
  eprint = {1803.00942},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1803.00942},
  url = {http://arxiv.org/abs/1803.00942},
  urldate = {2022-04-01},
  abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{keerthiConvergenceGeneralizedSMO2002,
  title = {Convergence of a {{Generalized SMO Algorithm}} for {{SVM Classifier Design}}},
  author = {Keerthi, S. S. and Gilbert, E. G.},
  date = {2002-03-11},
  journaltitle = {Machine Language},
  shortjournal = {Mach. Learn.},
  volume = {46},
  number = {1-3},
  pages = {351--360},
  issn = {0885-6125},
  doi = {10.1023/a:1012431217818},
  url = {https://doi.org/10.1023/A:1012431217818},
  urldate = {2022-04-01},
  abstract = {Convergence of a generalized version of the modified SMO algorithms given by Keerthi et al. for SVM classifier design is proved. The convergence results are also extended to modified SMO algorithms for solving ν-SVM classifier problems.},
  keywords = {convergence,SMO algorithm,support vector machine}
}

@article{keerthiImprovementsPlattSMO2001,
  title = {Improvements to {{Platt}}'s {{SMO Algorithm}} for {{SVM Classifier Design}}},
  author = {Keerthi, S. S. and Shevade, S. K. and Bhattacharyya, C. and Murthy, K. R. K.},
  date = {2001-03-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {13},
  number = {3},
  pages = {637--649},
  issn = {0899-7667},
  doi = {10.1162/089976601300014493},
  url = {https://doi.org/10.1162/089976601300014493},
  urldate = {2022-04-01},
  abstract = {This article points out an important source of inefficiency in Platt's sequential minimal optimization (SMO) algorithm that is caused by the use of a single threshold value. Using clues from the KKT conditions for the dual problem, two threshold parameters are employed to derive modifications of SMO. These modified algorithms perform significantly faster than the original SMO on all benchmark data sets tried.}
}

@article{kelemenSimpleRobustMutation2002,
  title = {A {{Simple}} and {{Robust Mutation Strategy}} for the {{Metropolis Light Transport Algorithm}}},
  author = {Kelemen, Csaba and Szirmay-Kalos, László and Antal, György and Csonka, Ferenc},
  date = {2002},
  journaltitle = {Computer Graphics Forum},
  volume = {21},
  number = {3},
  pages = {531--540},
  issn = {1467-8659},
  doi = {10.1111/1467-8659.t01-1-00703},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.t01-1-00703},
  urldate = {2022-06-25},
  abstract = {This paper presents a new mutation strategy for the Metropolis light transport algorithm, which works in the unit cube of pseudo-random numbers instead of mutating in the path space. This transformation makes the integrand have lower variation and thus increases the acceptance probability of the mutated samples. Higher acceptance ratio, in turn, reduces the correlation of the samples, which increases the speed of convergence. We use both local mutations that choose a new random sample in the neighborhood of the previous one, and global mutations that make “large steps”, and find the samples independently. Local mutations smooth out the result, while global mutations guarantee the ergodicity of the process. Due to the fact that samples are generated independently in large steps, this method can also be considered as a combination of the Metropolis algorithm with a classical random walk. If we use multiple importance sampling for this combination, the combined method will be as good at bright regions as the Metropolis algorithm and at dark regions as random walks. The resulting scheme is robust, efficient, but most importantly, is easy to implement and to combine with an arbitrary random-walk algorithm.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-8659.t01-1-00703}
}

@inproceedings{khamisLearningEfficientModel2015,
  title = {Learning an {{Efficient Model}} of {{Hand Shape Variation From Depth Images}}},
  author = {Khamis, Sameh and Taylor, Jonathan and Shotton, Jamie and Keskin, Cem and Izadi, Shahram and Fitzgibbon, Andrew},
  date = {2015},
  pages = {2540--2548},
  doi = {10.1109/CVPR.2015.7298869},
  url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Khamis_Learning_an_Efficient_2015_CVPR_paper.html},
  urldate = {2022-04-11},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2022-09-05},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@unpublished{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2014-05-01},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2021-12-31},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  date = {2018-07-10},
  number = {arXiv:1807.03039},
  eprint = {1807.03039},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.03039},
  url = {http://arxiv.org/abs/1807.03039},
  urldate = {2022-05-28},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kingmaImprovedVariationalInference2016,
  title = {Improved {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2016/hash/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Abstract.html},
  urldate = {2022-05-28},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  keywords = {⛔ No DOI found}
}

@article{kobyzevNormalizingFlowsIntroduction2021,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  date = {2021-11-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {43},
  number = {11},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  pages = {3964--3979},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  url = {http://arxiv.org/abs/1908.09257},
  urldate = {2022-05-09},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kohlerFullsphericalDeviceSimultaneous2013,
  title = {A Full-Spherical Device for Simultaneous Geometry and Reflectance Acquisition},
  booktitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Kohler, Johannes and Noll, Tobias and Reis, Gerd and Stricker, Didier},
  date = {2013-01},
  pages = {355--362},
  publisher = {{IEEE}},
  location = {{Clearwater Beach, FL, USA}},
  doi = {10.1109/WACV.2013.6475040},
  url = {https://ieeexplore.ieee.org/document/6475040},
  urldate = {2022-09-01},
  eventtitle = {2013 {{IEEE Workshop}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-4673-5054-9 978-1-4673-5053-2 978-1-4673-5052-5}
}

@inproceedings{kretzschmarEfficientInformationtheoreticGraph2011,
  title = {Efficient Information-Theoretic Graph Pruning for Graph-Based {{SLAM}} with Laser Range Finders},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kretzschmar, H. and Stachniss, C. and Grisetti, G.},
  date = {2011-09},
  pages = {865--871},
  publisher = {{IEEE}},
  location = {{San Francisco, CA}},
  doi = {10.1109/iros.2011.6094414},
  url = {http://ieeexplore.ieee.org/document/6094414/},
  urldate = {2022-03-25},
  eventtitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2011)},
  isbn = {978-1-61284-456-5 978-1-61284-454-1 978-1-61284-455-8}
}

@article{kretzschmarInformationtheoreticCompressionPose2012,
  title = {Information-Theoretic Compression of Pose Graphs for Laser-Based {{SLAM}}},
  author = {Kretzschmar, Henrik and Stachniss, Cyrill},
  date = {2012-09},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {31},
  number = {11},
  pages = {1219--1230},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364912455072},
  url = {http://journals.sagepub.com/doi/10.1177/0278364912455072},
  urldate = {2022-03-25},
  abstract = {In graph-based simultaneous localization and mapping (SLAM), the pose graph grows over time as the robot gathers information about the environment. An ever growing pose graph, however, prevents long-term mapping with mobile robots. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the mutual information between the laser measurements and the map to discard the measurements that are expected to provide only a small amount of information. Our method subsequently marginalizes out the nodes from the pose graph that correspond to the discarded laser measurements. To maintain a sparse pose graph that allows for efficient map optimization, our approach applies an approximate marginalization technique that is based on Chow–Liu trees. Our contributions allow the robot to effectively restrict the size of the pose graph. Alternatively, the robot is able to maintain a pose graph that does not grow unless the robot explores previously unobserved parts of the environment. Real-world experiments demonstrate that our approach to pose graph compression is well suited for long-term mobile robot mapping.},
  langid = {english}
}

@article{kretzschmarLifelongMapLearning2010,
  title = {Lifelong {{Map Learning}} for {{Graph-based SLAM}} in {{Static Environments}}},
  author = {Kretzschmar, Henrik and Grisetti, Giorgio and Stachniss, Cyrill},
  date = {2010-09},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {24},
  number = {3},
  pages = {199--206},
  issn = {0933-1875, 1610-1987},
  doi = {10.1007/s13218-010-0034-2},
  url = {http://link.springer.com/10.1007/s13218-010-0034-2},
  urldate = {2022-03-26},
  langid = {english}
}

@inproceedings{kuoM2SGDLearningLearn2020,
  title = {{{M2SGD}}: {{Learning}} to {{Learn Important Weights}}},
  shorttitle = {{{M2SGD}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Kuo, Nicholas I-Hsien and Harandi, Mehrtash and Fourrier, Nicolas and Walder, Christian and Ferraro, Gabriela and Suominen, Hanna},
  date = {2020-06},
  pages = {957--964},
  issn = {2160-7516},
  doi = {10.1109/CVPRW50498.2020.00126},
  abstract = {Meta-learning concerns rapid knowledge acquisition. One popular approach cast optimisation as a learning problem and it has been shown that learnt neural optimisers updated base learners more quickly than their handcrafted counterparts. In this paper, we learn an optimisation rule that sparsely updates the learner parameters and removes redundant weights. We present Masked Meta-SGD (M2SGD), a neural optimiser which is not only capable of updating learners quickly, but also capable of removing 83.71\% weights for ResNet20s.We release our codes at https://github.com/Nic5472K/CLVISION2020\_CVPR\_M2SGD.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  keywords = {Australia,Computer architecture,Image recognition,Optimization,Recurrent neural networks,Task analysis,Training}
}

@inproceedings{kurzGeometrybasedGraphPruning2021,
  title = {Geometry-Based {{Graph Pruning}} for {{Lifelong SLAM}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kurz, Gerhard and Holoch, Matthias and Biber, Peter},
  date = {2021-09},
  pages = {3313--3320},
  issn = {2153-0866},
  doi = {10.1109/iros51168.2021.9636530},
  abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
  eventtitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {Costs,Density functional theory,Intelligent robots,Simultaneous localization and mapping,Standards,Three-dimensional displays,Trajectory}
}

@article{kuznetsovNeuMIPMultiresolutionNeural2021,
  title = {{{NeuMIP}}: Multi-Resolution Neural Materials},
  shorttitle = {{{NeuMIP}}},
  author = {Kuznetsov, Alexandr and Mullia, Krishna and Xu, Zexiang and Hašan, Miloš and Ramamoorthi, Ravi},
  date = {2021-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3450626.3459795},
  url = {https://dl.acm.org/doi/10.1145/3450626.3459795},
  urldate = {2022-01-24},
  abstract = {We propose NeuMIP, a neural method for representing and rendering a variety of material appearances at different scales. Classical prefiltering (mipmapping) methods work well on simple material properties such as diffuse color, but fail to generalize to normals, self-shadowing, fibers or more complex microstructures and reflectances. In this work, we generalize traditional mipmap pyramids to pyramids of neural textures, combined with a fully connected network. We also introduce neural offsets, a novel method which enables rendering materials with intricate parallax effects without any tessellation. This generalizes classical parallax mapping, but is trained without supervision by any explicit heightfield. Neural materials within our system support a 7-dimensional query, including position, incoming and outgoing direction, and the desired filter kernel size. The materials have small storage (on the order of standard mipmapping except with more texture channels), and can be integrated within common Monte-Carlo path tracing systems. We demonstrate our method on a variety of materials, resulting in complex appearance across levels of detail, with accurate parallax, self-shadowing, and other effects.},
  langid = {english}
}

@inproceedings{lafortuneNonlinearApproximationReflectance1997,
  title = {Non-Linear Approximation of Reflectance Functions},
  booktitle = {Proceedings of the 24th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Lafortune, Eric PF and Foo, Sing-Choong and Torrance, Kenneth E and Greenberg, Donald P},
  date = {1997},
  pages = {117--126},
  keywords = {⛔ No DOI found}
}

@article{lafortuneUsingModifiedPhong1994,
  title = {Using the Modified {{Phong}} Reflectance Model for Physically Based Rendering},
  author = {Lafortune, Eric P. and Willems, Yves D.},
  date = {1994-11},
  pages = {19--19},
  url = {http://graphics.cs.kuleuven.be/publications/Phong/Phong_paper.ps},
  urldate = {2022-09-08},
  abstract = {This text discusses a few aspects of re ectance models in physically based rendering: The first section presents the de nition of the bidirectional reflection distribution function (brdf) of a surface and its physical properties. On a more practical level, the next section discusses models to represent brdfs and their desired properties in general for Monte Carlo algorithms. The third section goes into details about a specific reflectance model, the modified Phong brdf, with its definition, its properties and its use. We show how this model can be correctly integrated in importance sampling schemes for physically based Monte Carlo rendering algorithms. The fourth section is devoted to alternative parameter spaces in which reflectance models can be sampled, either deterministically or stochastically. The last section discusses an important implementational issue, more specifically the problem of verifying the implementation of a reflectance model.}
}

@article{lakeHumanlevelConceptLearning2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  date = {2015-12-11},
  journaltitle = {Science},
  volume = {350},
  number = {6266},
  pages = {1332--1338},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aab3050},
  url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
  urldate = {2022-09-03}
}

@inproceedings{larochelleNeuralAutoregressiveDistribution2011,
  title = {The Neural Autoregressive Distribution Estimator},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  author = {Larochelle, Hugo and Murray, Iain},
  date = {2011},
  pages = {29--37},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  keywords = {⛔ No DOI found}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  url = {http://ieeexplore.ieee.org/document/726791/},
  urldate = {2022-04-04}
}

@article{lenschPlannedSamplingSpatially2003,
  title = {Planned {{Sampling}} of {{Spatially Varying BRDFs}}},
  author = {Lensch, Hendrik P.A. and Lang, Jochen and Sa, Asla M. and Seidel, Hans-Peter},
  date = {2003-09},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {22},
  number = {3},
  pages = {473--482},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/1467-8659.00695},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/1467-8659.00695},
  urldate = {2022-09-01},
  langid = {english}
}

@article{lepetitEPnPAccurateSolution2009,
  title = {{{EPnP}}: {{An Accurate O}}(n) {{Solution}} to the {{PnP Problem}}},
  shorttitle = {{{EPnP}}},
  author = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
  date = {2009-02},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {81},
  number = {2},
  pages = {155--166},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-008-0152-6},
  url = {http://link.springer.com/10.1007/s11263-008-0152-6},
  urldate = {2022-05-03},
  langid = {english}
}

@inproceedings{lewisPoseSpaceDeformation2000,
  title = {Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation},
  shorttitle = {Pose Space Deformation},
  booktitle = {Proceedings of the 27th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Lewis, J. P. and Cordner, Matt and Fong, Nickson},
  date = {2000-07-01},
  series = {{{SIGGRAPH}} '00},
  pages = {165--172},
  publisher = {{ACM Press/Addison-Wesley Publishing Co.}},
  location = {{USA}},
  doi = {10.1145/344779.344862},
  url = {https://doi.org/10.1145/344779.344862},
  urldate = {2022-04-18},
  abstract = {Pose space deformation generalizes and improves upon both shape interpolation and common skeleton-driven deformation techniques. This deformation approach proceeds from the observation that several types of deformation can be uniformly represented as mappings from a pose space, defined by either an underlying skeleton or a more abstract system of parameters, to displacements in the object local coordinate frames. Once this uniform representation is identified, previously disparate deformation types can be accomplished within a single unified approach. The advantages of this algorithm include improved expressive power and direct manipulation of the desired shapes yet the performance associated with traditional shape interpolation is achievable. Appropriate applications include animation of facial and body deformation for entertainment, telepresence, computer gaming, and other applications where direct sculpting of deformations is desired or where real-time synthesis of a deforming model is required.},
  isbn = {978-1-58113-208-3},
  keywords = {animation,applications,deformation,facial animation,morphing}
}

@article{liDifferentiableMonteCarlo2018,
  title = {Differentiable {{Monte Carlo}} Ray Tracing through Edge Sampling},
  author = {Li, Tzu-Mao and Aittala, Miika and Durand, Frédo and Lehtinen, Jaakko},
  date = {2018-12-04},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {6},
  pages = {222:1--222:11},
  issn = {0730-0301},
  doi = {10.1145/3272127.3275109},
  url = {https://doi.org/10.1145/3272127.3275109},
  urldate = {2022-10-25},
  abstract = {Gradient-based methods are becoming increasingly important for computer graphics, machine learning, and computer vision. The ability to compute gradients is crucial to optimization, inverse problems, and deep learning. In rendering, the gradient is required with respect to variables such as camera parameters, light sources, scene geometry, or material appearance. However, computing the gradient of rendering is challenging because the rendering integral includes visibility terms that are not differentiable. Previous work on differentiable rendering has focused on approximate solutions. They often do not handle secondary effects such as shadows or global illumination, or they do not provide the gradient with respect to variables other than pixel coordinates. We introduce a general-purpose differentiable ray tracer, which, to our knowledge, is the first comprehensive solution that is able to compute derivatives of scalar functions over a rendered image with respect to arbitrary scene parameters such as camera pose, scene geometry, materials, and lighting parameters. The key to our method is a novel edge sampling algorithm that directly samples the Dirac delta functions introduced by the derivatives of the discontinuous integrand. We also develop efficient importance sampling methods based on spatial hierarchies. Our method can generate gradients in times running from seconds to minutes depending on scene complexity and desired precision. We interface our differentiable ray tracer with the deep learning library PyTorch and show prototype applications in inverse rendering and the generation of adversarial examples for neural networks.},
  keywords = {differentiable programming,inverse rendering,ray tracing}
}

@misc{liDifferentiableVisualComputing2019,
  title = {Differentiable {{Visual Computing}}},
  author = {Li, Tzu-Mao},
  date = {2019-05-27},
  number = {arXiv:1904.12228},
  eprint = {1904.12228},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.12228},
  url = {http://arxiv.org/abs/1904.12228},
  urldate = {2022-10-19},
  abstract = {Derivatives of computer graphics, image processing, and deep learning algorithms have tremendous use in guiding parameter space searches, or solving inverse problems. As the algorithms become more sophisticated, we no longer only need to differentiate simple mathematical functions, but have to deal with general programs which encode complex transformations of data. This dissertation introduces three tools for addressing the challenges that arise when obtaining and applying the derivatives for complex graphics algorithms. Traditionally, practitioners have been constrained to composing programs with a limited set of operators, or hand-deriving derivatives. We extend the image processing language Halide with reverse-mode automatic differentiation, and the ability to automatically optimize the gradient computations. This enables automatic generation of the gradients of arbitrary Halide programs, at high performance, with little programmer effort. In 3D rendering, the gradient is required with respect to variables such as camera parameters, geometry, and appearance. However, computing the gradient is challenging because the rendering integral includes visibility terms that are not differentiable. We introduce, to our knowledge, the first general-purpose differentiable ray tracer that solves the full rendering equation, while correctly taking the geometric discontinuities into account. Finally, we demonstrate that the derivatives of light path throughput can also be useful for guiding sampling in forward rendering. Simulating light transport in the presence of multi-bounce glossy effects and motion in 3D rendering is challenging due to the hard-to-sample high-contribution areas. We present a Markov Chain Monte Carlo rendering algorithm that extends Metropolis Light Transport by automatically and explicitly adapting to the local integrand, thereby increasing sampling efficiency.},
  archiveprefix = {arXiv},
  keywords = {.skimmed,📝,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@online{LightProbeImage,
  title = {Light {{Probe Image Gallery}}},
  url = {http://www.pauldebevec.com/Probes/},
  urldate = {2022-09-12},
  annotation = {http://www.pauldebevec.com/Probes/}
}

@unpublished{liMetaSGDLearningLearn2017,
  title = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few-Shot Learning}}},
  shorttitle = {Meta-{{SGD}}},
  author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  date = {2017-09-28},
  eprint = {1707.09835},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1707.09835},
  url = {http://arxiv.org/abs/1707.09835},
  urldate = {2022-01-19},
  abstract = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{linLinearConvergenceDecomposition2001,
  title = {Linear Convergence of a Decomposition Method for Support Vector Machines},
  author = {Lin, Chih-Jen},
  date = {2001},
  publisher = {{Citeseer}},
  keywords = {⛔ No DOI found}
}

@inproceedings{listSVMoptimizationSteepestdescentLine2009,
  title = {{{SVM-optimization}} and Steepest-Descent Line Search},
  booktitle = {Proceedings of the 22nd {{Annual Conference}} on {{Computational Learning Theory}}},
  author = {List, Nikolas and Simon, Hans Ulrich},
  date = {2009},
  publisher = {{Citeseer}},
  keywords = {⛔ No DOI found}
}

@article{liuCurveFusionReconstructingThin2018,
  title = {{{CurveFusion}}: Reconstructing Thin Structures from {{RGBD}} Sequences},
  shorttitle = {{{CurveFusion}}},
  author = {Liu, Lingjie and Chen, Nenglun and Ceylan, Duygu and Theobalt, Christian and Wang, Wenping and Mitra, Niloy J.},
  date = {2018-12-04},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {6},
  pages = {218:1--218:12},
  issn = {0730-0301},
  doi = {10.1145/3272127.3275097},
  url = {https://doi.org/10.1145/3272127.3275097},
  urldate = {2022-10-06},
  abstract = {We introduce CurveFusion, the first approach for high quality scanning of thin structures at interactive rates using a handheld RGBD camera. Thin filament-like structures are mathematically just 1D curves embedded in R3, and integration-based reconstruction works best when depth sequences (from the thin structure parts) are fused using the object's (unknown) curve skeleton. Thus, using the complementary but noisy color and depth channels, CurveFusion first automatically identifies point samples on potential thin structures and groups them into bundles, each being a group of a fixed number of aligned consecutive frames. Then, the algorithm extracts per-bundle skeleton curves using L1 axes, and aligns and iteratively merges the L1 segments from all the bundles to form the final complete curve skeleton. Thus, unlike previous methods, reconstruction happens via integration along a data-dependent fusion primitive, i.e., the extracted curve skeleton. We extensively evaluate CurveFusion on a range of challenging examples, different scanner and calibration settings, and present high fidelity thin structure reconstructions previously just not possible from raw RGBD sequences.},
  keywords = {curve reconstruction,data fusion,L1 axis,RGBD scans}
}

@misc{liuLearningLearnSample2022,
  title = {Learning to {{Learn}} and {{Sample BRDFs}}},
  author = {Liu, Chen and Fischer, Michael and Ritschel, Tobias},
  date = {2022-10-07},
  number = {arXiv:2210.03510},
  eprint = {2210.03510},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03510},
  url = {http://arxiv.org/abs/2210.03510},
  urldate = {2022-10-10},
  abstract = {We propose a method to accelerate the joint process of physically acquiring and learning neural Bi-directional Reflectance Distribution Function (BRDF) models. While BRDF learning alone can be accelerated by meta-learning, acquisition remains slow as it relies on a mechanical process. We show that meta-learning can be extended to optimize the physical sampling pattern, too. After our method has been meta-trained for a set of fully-sampled BRDFs, it is able to quickly train on new BRDFs with up to five orders of magnitude fewer physical acquisition samples at similar quality. Our approach also extends to other linear and non-linear BRDF models, which we show in an extensive evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@inproceedings{liuMaterialEditingUsing2017,
  title = {Material {{Editing Using}} a {{Physically Based Rendering Network}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Guilin and Ceylan, Duygu and Yumer, Ersin and Yang, Jimei and Lien, Jyh-Ming},
  date = {2017-10},
  pages = {2280--2288},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.248},
  abstract = {The ability to edit materials of objects in images is desirable by many content creators. However, this is an extremely challenging task as it requires to disentangle intrinsic physical properties of an image. We propose an end-to-end network architecture that replicates the forward image formation process to accomplish this task. Specifically, given a single image, the network first predicts intrinsic properties, i.e. shape, illumination, and material, which are then provided to a rendering layer. This layer performs in-network image synthesis, thereby enabling the network to understand the physics behind the image formation process. The proposed rendering layer is fully differentiable, supports both diffuse and specular materials, and thus can be applicable in a variety of problem settings. We demonstrate a rich set of visually plausible material editing examples and provide an extensive comparative study.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Image decomposition,Lighting,Material properties,Network architecture,Rendering (computer graphics),Shape,Surface treatment}
}

@article{loperSMPLSkinnedMultiperson2015,
  title = {{{SMPL}}: A Skinned Multi-Person Linear Model},
  shorttitle = {{{SMPL}}},
  author = {Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J.},
  date = {2015-10-26},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {34},
  number = {6},
  pages = {248:1--248:16},
  issn = {0730-0301},
  doi = {10.1145/2816795.2818013},
  url = {https://doi.org/10.1145/2816795.2818013},
  urldate = {2022-04-19},
  abstract = {We present a learned model of human body shape and pose-dependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model (SMPL) is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of SMPL using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-SCAPE model trained on the same data. We also extend SMPL to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, SMPL is compatible with existing rendering engines and we make it available for research purposes.},
  keywords = {blendshapes,body shape,skinning,soft-tissue}
}

@inproceedings{lorensenMarchingCubesHigh1987,
  title = {Marching Cubes: {{A}} High Resolution {{3D}} Surface Construction Algorithm},
  shorttitle = {Marching Cubes},
  booktitle = {Proceedings of the 14th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '87},
  author = {Lorensen, William E. and Cline, Harvey E.},
  date = {1987},
  pages = {163--169},
  publisher = {{ACM Press}},
  location = {{Not Known}},
  doi = {10.1145/37401.37422},
  url = {http://portal.acm.org/citation.cfm?doid=37401.37422},
  urldate = {2022-06-13},
  eventtitle = {The 14th Annual Conference},
  isbn = {978-0-89791-227-3},
  langid = {english}
}

@unpublished{loshchilovOnlineBatchSelection2016,
  title = {Online {{Batch Selection}} for {{Faster Training}} of {{Neural Networks}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2016-04-25},
  eprint = {1511.06343},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.1511.06343},
  url = {http://arxiv.org/abs/1511.06343},
  urldate = {2022-05-05},
  abstract = {Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. As the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. Our experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control}
}

@misc{loshchilovSGDRStochasticGradient2017,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2017-05-03},
  number = {arXiv:1608.03983},
  eprint = {1608.03983},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1608.03983},
  url = {http://arxiv.org/abs/1608.03983},
  urldate = {2022-09-21},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control}
}

@article{loubetReparameterizingDiscontinuousIntegrands2019,
  title = {Reparameterizing Discontinuous Integrands for Differentiable Rendering},
  author = {Loubet, Guillaume and Holzschuch, Nicolas and Jakob, Wenzel},
  date = {2019-11-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {6},
  pages = {228:1--228:14},
  issn = {0730-0301},
  doi = {10.1145/3355089.3356510},
  url = {https://doi.org/10.1145/3355089.3356510},
  urldate = {2022-10-25},
  abstract = {Differentiable rendering has recently opened the door to a number of challenging inverse problems involving photorealistic images, such as computational material design and scattering-aware reconstruction of geometry and materials from photographs. Differentiable rendering algorithms strive to estimate partial derivatives of pixels in a rendered image with respect to scene parameters, which is difficult because visibility changes are inherently non-differentiable. We propose a new technique for differentiating path-traced images with respect to scene parameters that affect visibility, including the position of cameras, light sources, and vertices in triangle meshes. Our algorithm computes the gradients of illumination integrals by applying changes of variables that remove or strongly reduce the dependence of the position of discontinuities on differentiable scene parameters. The underlying parameterization is created on the fly for each integral and enables accurate gradient estimates using standard Monte Carlo sampling in conjunction with automatic differentiation. Importantly, our approach does not rely on sampling silhouette edges, which has been a bottleneck in previous work and tends to produce high-variance gradients when important edges are found with insufficient probability in scenes with complex visibility and high-resolution geometry. We show that our method only requires a few samples to produce gradients with low bias and variance for challenging cases such as glossy reflections and shadows. Finally, we use our differentiable path tracer to reconstruct the 3D geometry and materials of several real-world objects from a set of reference photographs.},
  keywords = {differentiable rendering,discontinuous integrands,inverse rendering,path tracing,stochastic gradient descent}
}

@article{lowBRDFModelsAccurate2012,
  title = {{{BRDF}} Models for Accurate and Efficient Rendering of Glossy Surfaces},
  author = {Löw, Joakim and Kronander, Joel and Ynnerman, Anders and Unger, Jonas},
  date = {2012-02-02},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {31},
  number = {1},
  pages = {9:1--9:14},
  issn = {0730-0301},
  doi = {10.1145/2077341.2077350},
  url = {https://doi.org/10.1145/2077341.2077350},
  urldate = {2022-09-03},
  abstract = {This article presents two new parametric models of the Bidirectional Reflectance Distribution Function (BRDF), one inspired by the Rayleigh-Rice theory for light scattering from optically smooth surfaces, and one inspired by micro-facet theory. The models represent scattering from a wide range of glossy surface types with high accuracy. In particular, they enable representation of types of surface scattering which previous parametric models have had trouble modeling accurately. In a study of the scattering behavior of measured reflectance data, we investigate what key properties are needed for a model to accurately represent scattering from glossy surfaces. We investigate different parametrizations and how well they match the behavior of measured BRDFs. We also examine the scattering curves which are represented in parametric models by different distribution functions. Based on the insights gained from the study, the new models are designed to provide accurate fittings to the measured data. Importance sampling schemes are developed for the new models, enabling direct use in existing production pipelines. In the resulting renderings we show that the visual quality achieved by the models matches that of the measured data.},
  keywords = {BRDF,global illumination,gloss,importance sampling,Monte Carlo,Rayleigh-Rice}
}

@article{loweDistinctiveImageFeatures2004,
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  author = {Lowe, David G.},
  date = {2004},
  journaltitle = {International journal of computer vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  publisher = {{Springer}},
  doi = {10.1023/B:VISI.0000029664.99615.94}
}

@inproceedings{loweObjectRecognitionLocal1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D.G.},
  date = {1999},
  pages = {1150-1157 vol.2},
  publisher = {{IEEE}},
  location = {{Kerkyra, Greece}},
  doi = {10.1109/ICCV.1999.790410},
  url = {http://ieeexplore.ieee.org/document/790410/},
  urldate = {2022-05-17},
  eventtitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  isbn = {978-0-7695-0164-2}
}

@article{lustigSparseMRIApplication2007,
  title = {Sparse {{MRI}}: {{The}} Application of Compressed Sensing for Rapid {{MR}} Imaging},
  shorttitle = {Sparse {{MRI}}},
  author = {Lustig, Michael and Donoho, David and Pauly, John M.},
  date = {2007-12},
  journaltitle = {Magnetic Resonance in Medicine},
  shortjournal = {Magn. Reson. Med.},
  volume = {58},
  number = {6},
  pages = {1182--1195},
  issn = {07403194, 15222594},
  doi = {10.1002/mrm.21391},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/mrm.21391},
  urldate = {2022-03-08},
  langid = {english}
}

@article{margossianReviewAutomaticDifferentiation2019,
  title = {A {{Review}} of Automatic Differentiation and Its Efficient Implementation},
  author = {Margossian, Charles C.},
  date = {2019-07},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  shortjournal = {WIREs Data Mining Knowl Discov},
  volume = {9},
  number = {4},
  eprint = {1811.05031},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/WIDM.1305},
  url = {http://arxiv.org/abs/1811.05031},
  urldate = {2022-10-19},
  abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software,Statistics - Computation}
}

@article{marquardtAlgorithmLeastSquaresEstimation1963,
  title = {An {{Algorithm}} for {{Least-Squares Estimation}} of {{Nonlinear Parameters}}},
  author = {Marquardt, Donald W.},
  date = {1963-06},
  journaltitle = {Journal of the Society for Industrial and Applied Mathematics},
  shortjournal = {Journal of the Society for Industrial and Applied Mathematics},
  volume = {11},
  number = {2},
  pages = {431--441},
  issn = {0368-4245, 2168-3484},
  doi = {10.1137/0111030},
  url = {http://epubs.siam.org/doi/10.1137/0111030},
  urldate = {2022-05-03},
  langid = {english}
}

@article{marschnerImageBasedBRDFMeasurement1999,
  title = {Image-{{Based BRDF Measurement Including Human Skin}}},
  author = {Marschner, Stephen R. and Westin, Stephen H. and Lafortune, Eric P. F. and Torrance, Kenneth E. and Greenberg, Donald P.},
  date = {1999},
  journaltitle = {Eurographics Workshop on Rendering},
  pages = {14 pages},
  publisher = {{The Eurographics Association}},
  issn = {1727-3463},
  doi = {10.2312/EGWR/EGWR99/131-144},
  url = {http://diglib.eg.org/handle/10.2312/EGWR.EGWR99.131-144},
  urldate = {2022-09-01},
  abstract = {We present a new image-based process for measuring the bidirectional reflectance of homogeneous surfaces rapidly, completely, and accurately. For simple sample shapes (spheres and cylinders) the method requires only a digital camera and a stable light source. Adding a 3D scanner allows a wide class of curved near-convex objects to be measured. With measurements for a variety of materials from paints to human skin, we demonstrate the new methods ability to achieve high resolution and accuracy over a large domain of illumination and reflection directions. We verify our measurements by tests of internal consistency and by comparison against measurements made using a gonioreflectometer.},
  isbn = {9783211833827}
}

@article{martelAcornAdaptiveCoordinate2021,
  title = {Acorn: Adaptive Coordinate Networks for Neural Scene Representation},
  shorttitle = {Acorn},
  author = {Martel, Julien N. P. and Lindell, David B. and Lin, Connor Z. and Chan, Eric R. and Monteiro, Marco and Wetzstein, Gordon},
  date = {2021-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3450626.3459785},
  url = {https://dl.acm.org/doi/10.1145/3450626.3459785},
  urldate = {2022-06-18},
  abstract = {Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000X compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.},
  langid = {english}
}

@article{martin-bruallaNeRFWildNeural2020,
  title = {{{NeRF}} in the {{Wild}}: {{Neural Radiance Fields}} for {{Unconstrained Photo Collections}}},
  shorttitle = {{{NeRF}} in the {{Wild}}},
  author = {Martin-Brualla, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
  date = {2020-08-05},
  doi = {10.48550/arXiv.2008.02268},
  url = {https://arxiv.org/abs/2008.02268v3},
  urldate = {2022-06-13},
  abstract = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
  langid = {english}
}

@article{matsapeyDesignGoniospectrophotometerOptical2013,
  title = {Design of a Gonio-Spectro-Photometer for Optical Characterization of Gonio-Apparent Materials},
  author = {Matsapey, N and Faucheu, J and Flury, M and Delafosse, D},
  date = {2013-06-01},
  journaltitle = {Measurement Science and Technology},
  shortjournal = {Meas. Sci. Technol.},
  volume = {24},
  number = {6},
  pages = {065901},
  issn = {0957-0233, 1361-6501},
  doi = {10.1088/0957-0233/24/6/065901},
  url = {https://iopscience.iop.org/article/10.1088/0957-0233/24/6/065901},
  urldate = {2022-09-02}
}

@article{matusikDatadrivenReflectanceModel2003,
  title = {A Data-Driven Reflectance Model},
  author = {Matusik, Wojciech and Pfister, Hanspeter and Brand, Matt and McMillan, Leonard},
  date = {2003-07},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {22},
  number = {3},
  pages = {759--769},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/882262.882343},
  url = {https://dl.acm.org/doi/10.1145/882262.882343},
  urldate = {2022-01-03},
  abstract = {We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.},
  langid = {english}
}

@article{matusikEfficientIsotropicBRDF2003,
  title = {Efficient {{Isotropic BRDF Measurement}}},
  author = {Matusik, Wojciech and Pfister, Hanspeter and Brand, Matthew and McMillan, Leonard},
  date = {2003},
  journaltitle = {Eurographics Workshop on Rendering},
  pages = {8 pages},
  publisher = {{The Eurographics Association}},
  issn = {1727-3463},
  doi = {10.2312/EGWR/EGWR03/241-248},
  url = {http://diglib.eg.org/handle/10.2312/EGWR.EGWR03.241-248},
  urldate = {2022-08-31},
  abstract = {In this paper we present novel reflectance measurement procedures that require fewer total measurements than standard uniform sampling approaches. First, we acquire densely sampled reflectance data for a large collection of different materials. Using these densely sampled measurements we analyze the general surface reflectance function to determine the local signal variation at each point in the function's domain. We then use wavelet analysis to derive a common basis for all of the acquired reflectance functions as well as a corresponding non-uniform sampling pattern that corresponds to all non-zero wavelet coefficients. Second, we show that the reflectance of an arbitrary material can be represented as a linear combination of the surface reflectance functions. Furthermore, our analysis provides a reduced set of sampling points that permits us to robustly estimate the coefficients of this linear combination. These procedures dramatically shorten the acquisition time for isotropic reflectance measurements. We present a detailed description and analysis of our measurement approaches and sampling strategies.},
  isbn = {9783905673036}
}

@inproceedings{maximovDeepAppearanceMaps2019,
  ids = {maximovDeepAppearanceMaps2019a},
  title = {Deep {{Appearance Maps}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Maximov, Maxim and Ritschel, Tobias and Leal-Taixe, Laura and Fritz, Mario},
  date = {2019-10},
  pages = {8728--8737},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00882},
  url = {https://ieeexplore.ieee.org/document/9010700/},
  urldate = {2022-05-10},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8}
}

@inproceedings{melaxDynamicsBased3D2013,
  title = {Dynamics Based {{3D}} Skeletal Hand Tracking},
  booktitle = {Proceedings of the {{ACM SIGGRAPH Symposium}} on {{Interactive 3D Graphics}} and {{Games}} - {{I3D}} '13},
  author = {Melax, Stan and Keselman, Leonid and Orsten, Sterling},
  date = {2013},
  pages = {184},
  publisher = {{ACM Press}},
  location = {{Orlando, Florida}},
  doi = {10.1145/2448196.2448232},
  url = {http://dl.acm.org/citation.cfm?doid=2448196.2448232},
  urldate = {2022-04-12},
  eventtitle = {The {{ACM SIGGRAPH Symposium}}},
  isbn = {978-1-4503-1956-0},
  langid = {english}
}

@unpublished{meshryNeuralRerenderingWild2019,
  title = {Neural {{Rerendering}} in the {{Wild}}},
  author = {Meshry, Moustafa and Goldman, Dan B. and Khamis, Sameh and Hoppe, Hugues and Pandey, Rohit and Snavely, Noah and Martin-Brualla, Ricardo},
  date = {2019-04-08},
  eprint = {1904.04290},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.04290},
  urldate = {2022-01-04},
  abstract = {We explore total scene capture -- recording, modeling, and rerendering a scene under varying appearance such as season and time of day. Starting from internet photos of a tourist landmark, we apply traditional 3D reconstruction to register the photos and approximate the scene as a point cloud. For each photo, we render the scene points into a deep framebuffer, and train a neural network to learn the mapping of these initial renderings to the actual photos. This rerendering network also takes as input a latent appearance vector and a semantic mask indicating the location of transient objects like pedestrians. The model is evaluated on several datasets of publicly available images spanning a broad range of illumination conditions. We create short videos demonstrating realistic manipulation of the image viewpoint, appearance, and semantic labeling. We also compare results with prior work on scene reconstruction from internet photos.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics}
}

@inproceedings{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {405--421},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58452-8_24},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ,ϕ)(θ,ϕ)(\textbackslash theta ,\textbackslash phi )) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  isbn = {978-3-030-58452-8},
  langid = {english},
  keywords = {3D deep learning,Image-based rendering,Scene representation,View synthesis,Volume rendering}
}

@unpublished{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11-06},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1411.1784},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2022-04-14},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{montesOverviewBRDFModels2012,
  title = {An Overview of {{BRDF}} Models},
  author = {Montes, Rosana and Ureña, Carlos},
  date = {2012},
  journaltitle = {University of Grenada, Technical Report LSI-2012-001},
  keywords = {.skimmed,⛔ No DOI found}
}

@inproceedings{muellerGANeratedHandsRealTime2018,
  title = {{{GANerated Hands}} for {{Real-Time 3D Hand Tracking From Monocular RGB}}},
  author = {Mueller, Franziska and Bernard, Florian and Sotnychenko, Oleksandr and Mehta, Dushyant and Sridhar, Srinath and Casas, Dan and Theobalt, Christian},
  date = {2018},
  pages = {49--59},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Mueller_GANerated_Hands_for_CVPR_2018_paper.html},
  urldate = {2022-04-18},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{muellerRealtimePoseShape2019,
  title = {Real-Time Pose and Shape Reconstruction of Two Interacting Hands with a Single Depth Camera},
  author = {Mueller, Franziska and Davis, Micah and Bernard, Florian and Sotnychenko, Oleksandr and Verschoor, Mickeal and Otaduy, Miguel A. and Casas, Dan and Theobalt, Christian},
  date = {2019-08-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {4},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3322958},
  url = {https://dl.acm.org/doi/10.1145/3306346.3322958},
  urldate = {2022-04-11},
  abstract = {We present a novel method for real-time pose and shape reconstruction of two strongly interacting hands. Our approach is the first two-hand tracking solution that combines an extensive list of favorable properties, namely it is marker-less, uses a single consumer-level depth camera, runs in real time, handles inter- and intra-hand collisions, and automatically adjusts to the user's hand shape. In order to achieve this, we embed a recent parametric hand pose and shape model and a dense correspondence predictor based on a deep neural network into a suitable energy minimization framework. For training the correspondence prediction network, we synthesize a two-hand dataset based on physical simulations that includes both hand pose and shape annotations while at the same time avoiding inter-hand penetrations. To achieve real-time rates, we phrase the model fitting in terms of a nonlinear least-squares problem so that the energy can be optimized based on a highly efficient GPU-based Gauss-Newton optimizer. We show state-of-the-art results in scenes that exceed the complexity level demonstrated by previous work, including tight two-hand grasps, significant inter-hand occlusions, and gesture interaction.               1},
  langid = {english}
}

@misc{mullerInstantNeuralGraphics2022b,
  title = {Instant {{Neural Graphics Primitives}} with a {{Multiresolution Hash Encoding}}},
  author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  date = {2022-05-04},
  eprint = {2201.05989},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3528223.3530127},
  url = {http://arxiv.org/abs/2201.05989},
  urldate = {2022-06-14},
  abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of \$\{1920\textbackslash!\textbackslash times\textbackslash!1080\}\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@article{mullerNeuralImportanceSampling2019,
  title = {Neural {{Importance Sampling}}},
  author = {Müller, Thomas and Mcwilliams, Brian and Rousselle, Fabrice and Gross, Markus and Novák, Jan},
  date = {2019-10-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {5},
  pages = {1--19},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3341156},
  url = {https://dl.acm.org/doi/10.1145/3341156},
  urldate = {2022-05-13},
  abstract = {We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the Kullback-Leibler and the χ               2               divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.},
  langid = {english}
}

@article{mullerRealtimeNeuralRadiance2021,
  title = {Real-Time Neural Radiance Caching for Path Tracing},
  author = {Müller, Thomas and Rousselle, Fabrice and Novák, Jan and Keller, Alexander},
  date = {2021-07-19},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {36:1--36:16},
  issn = {0730-0301},
  doi = {10.1145/3450626.3459812},
  url = {https://doi.org/10.1145/3450626.3459812},
  urldate = {2022-10-06},
  abstract = {We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead---about 2.6ms on full HD resolution---thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.},
  keywords = {deep learning,neural networks,path tracing,radiance caching,real-time,rendering}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2017,
  title = {{{ORB-SLAM2}}: {{An Open-Source SLAM System}} for {{Monocular}}, {{Stereo}}, and {{RGB-D Cameras}}},
  shorttitle = {{{ORB-SLAM2}}},
  author = {Mur-Artal, Raul and Tardos, Juan D.},
  date = {2017-10},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {33},
  number = {5},
  pages = {1255--1262},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2017.2705103},
  url = {http://ieeexplore.ieee.org/document/7946260/},
  urldate = {2022-04-24}
}

@article{mur-artalORBSLAMVersatileAccurate2015,
  title = {{{ORB-SLAM}}: {{A Versatile}} and {{Accurate Monocular SLAM System}}},
  shorttitle = {{{ORB-SLAM}}},
  author = {Mur-Artal, Raul and Montiel, J. M. M. and Tardos, Juan D.},
  date = {2015-10},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {31},
  number = {5},
  pages = {1147--1163},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2015.2463671},
  url = {https://ieeexplore.ieee.org/document/7219438/},
  urldate = {2022-04-24}
}

@article{naumannOptimalJacobianAccumulation2008,
  title = {Optimal {{Jacobian}} Accumulation Is {{NP-complete}}},
  author = {Naumann, Uwe},
  date = {2008-04-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  volume = {112},
  number = {2},
  pages = {427--441},
  issn = {1436-4646},
  doi = {10.1007/s10107-006-0042-z},
  url = {https://doi.org/10.1007/s10107-006-0042-z},
  urldate = {2022-10-17},
  abstract = {We show that the problem of accumulating Jacobian matrices by using a minimal number of floating-point operations is NP-complete by reduction from Ensemble Computation. The proof makes use of the fact that, deviating from the state-of-the-art assumption, algebraic dependences can exist between the local partial derivatives. It follows immediately that the same problem for directional derivatives, adjoints, and higher derivatives is NP-complete, too.},
  langid = {english},
  keywords = {26B10,68Q17,Automatic differentiation,Complexity,NP-completeness}
}

@inproceedings{needellStochasticGradientDescent2014,
  title = {Stochastic {{Gradient Descent}}, {{Weighted Sampling}}, and the {{Randomized Kaczmarz}} Algorithm},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Needell, Deanna and Ward, Rachel and Srebro, Nati},
  date = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2014/hash/f29c21d4897f78948b91f03172341b7b-Abstract.html},
  urldate = {2022-04-10},
  abstract = {We improve a recent gurantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.},
  keywords = {⛔ No DOI found}
}

@article{nganExperimentalAnalysisBRDF2005,
  title = {Experimental {{Analysis}} of {{BRDF Models}}},
  author = {Ngan, Addy and Durand, Frédo and Matusik, Wojciech},
  date = {2005},
  journaltitle = {Eurographics Symposium on Rendering (2005)},
  pages = {10 pages},
  publisher = {{The Eurographics Association}},
  issn = {1727-3463},
  doi = {10.2312/egwr/egsr05/117-126},
  url = {http://diglib.eg.org/handle/10.2312/EGWR.EGSR05.117-126},
  urldate = {2021-12-18},
  abstract = {The Bidirectional Reflectance Distribution Function (BRDF) describes the appearance of a material by its interaction with light at a surface point. A variety of analytical models have been proposed to represent BRDFs. However, analysis of these models has been scarce due to the lack of high-resolution measured data. In this work we evaluate several well-known analytical models in terms of their ability to fit measured BRDFs. We use an existing high-resolution data set of a hundred isotropic materials and compute the best approximation for each analytical model. Furthermore, we have built a new setup for efficient acquisition of anisotropic BRDFs, which allows us to acquire anisotropic materials at high resolution. We have measured four samples of anisotropic materials (brushed aluminum, velvet, and two satins). Based on the numerical errors, function plots, and rendered images we provide insights into the performance of the various models. We conclude that for most isotropic materials physically-based analytic reflectance models can represent their appearance quite well. We illustrate the important difference between the two common ways of defining the specular lobe: around the mirror direction and with respect to the half-vector. Our evaluation shows that the latter gives a more accurate shape for the reflection lobe. Our analysis of anisotropic materials indicates current parametric reflectance models cannot represent their appearances faithfully in many cases. We show that using a sampled microfacet distribution computed from measurements improves the fit and qualitatively reproduces the measurements.},
  isbn = {9783905673234},
  langid = {english}
}

@unpublished{nicholFirstOrderMetaLearningAlgorithms2018,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  date = {2018-10-22},
  eprint = {1803.02999},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1803.02999},
  url = {http://arxiv.org/abs/1803.02999},
  urldate = {2022-05-11},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix = {arXiv},
  keywords = {.skimmed,Computer Science - Machine Learning}
}

@report{nicodemusGeometricalConsiderationsNomenclature1977,
  title = {Geometrical Considerations and Nomenclature for Reflectance},
  author = {Nicodemus, F E and Richmond, J C and Hsia, J J and Ginsberg, I W and Limperis, T},
  date = {1977},
  edition = {0},
  number = {NBS MONO 160},
  pages = {NBS MONO 160},
  institution = {{National Bureau of Standards}},
  location = {{Gaithersburg, MD}},
  doi = {10.6028/NBS.MONO.160},
  url = {https://nvlpubs.nist.gov/nistpubs/Legacy/MONO/nbsmonograph160.pdf},
  urldate = {2022-09-02},
  langid = {english}
}

@article{nielsenOptimalMinimalBRDF2015,
  title = {On Optimal, Minimal {{BRDF}} Sampling for Reflectance Acquisition},
  author = {Nielsen, Jannik Boll and Jensen, Henrik Wann and Ramamoorthi, Ravi},
  date = {2015-11-04},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {34},
  number = {6},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2816795.2818085},
  url = {https://dl.acm.org/doi/10.1145/2816795.2818085},
  urldate = {2022-05-15},
  langid = {english}
}

@article{nimier-davidMaterialLightingReconstruction2021,
  title = {Material and {{Lighting Reconstruction}} for {{Complex Indoor Scenes}} with {{Texture-space Differentiable Rendering}}},
  author = {Nimier-David, Merlin and Dong, Zhao and Jakob, Wenzel and Kaplanyan, Anton},
  date = {2021},
  journaltitle = {Eurographics Symposium on Rendering - DL-only Track},
  pages = {12 pages},
  publisher = {{The Eurographics Association}},
  issn = {1727-3463},
  doi = {10.2312/SR.20211292},
  url = {https://diglib.eg.org/handle/10.2312/sr20211292},
  urldate = {2022-10-06},
  abstract = {Modern geometric reconstruction techniques achieve impressive levels of accuracy in indoor environments. However, such captured data typically keeps lighting and materials entangled. It is then impossible to manipulate the resulting scenes in photorealistic settings, such as augmented / mixed reality and robotics simulation. Moreover, various imperfections in the captured data, such as missing detailed geometry, camera misalignment, uneven coverage of observations, etc., pose challenges for scene recovery. To address these challenges, we present a robust optimization pipeline based on differentiable rendering to recover physically based materials and illumination, leveraging RGB and geometry captures. We introduce a novel texture-space sampling technique and carefully chosen inductive priors to help guide reconstruction, avoiding low-quality or implausible local minima. Our approach enables robust and high-resolution reconstruction of complex materials and illumination in captured indoor scenes. This enables a variety of applications including novel view synthesis, scene editing, local \&amp; global relighting, synthetic data augmentation, and other photorealistic manipulations.},
  isbn = {9783038681571},
  version = {073-084},
  keywords = {Computing methodologies --&gt; Reconstruction,Mixed / augmented reality,Ray tracing,Virtual reality}
}

@article{nimier-davidRadiativeBackpropagationAdjoint2020,
  title = {Radiative Backpropagation: An Adjoint Method for Lightning-Fast Differentiable Rendering},
  shorttitle = {Radiative Backpropagation},
  author = {Nimier-David, Merlin and Speierer, Sébastien and Ruiz, Benoît and Jakob, Wenzel},
  date = {2020-07-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {4},
  pages = {146:146:1--146:146:15},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392406},
  url = {https://doi.org/10.1145/3386569.3392406},
  urldate = {2022-08-01},
  abstract = {Physically based differentiable rendering has recently evolved into a powerful tool for solving inverse problems involving light. Methods in this area perform a differentiable simulation of the physical process of light transport and scattering to estimate partial derivatives relating scene parameters to pixels in the rendered image. Together with gradient-based optimization, such algorithms have interesting applications in diverse disciplines, e.g., to improve the reconstruction of 3D scenes, while accounting for interreflection and transparency, or to design meta-materials with specified optical properties. The most versatile differentiable rendering algorithms rely on reverse-mode differentiation to compute all requested derivatives at once, enabling optimization of scene descriptions with millions of free parameters. However, a severe limitation of the reverse-mode approach is that it requires a detailed transcript of the computation that is subsequently replayed to back-propagate derivatives to the scene parameters. The transcript of typical renderings is extremely large, exceeding the available system memory by many orders of magnitude, hence current methods are limited to simple scenes rendered at low resolutions and sample counts. We introduce radiative backpropagation, a fundamentally different approach to differentiable rendering that does not require a transcript, greatly improving its scalability and efficiency. Our main insight is that reverse-mode propagation through a rendering algorithm can be interpreted as the solution of a continuous transport problem involving the partial derivative of radiance with respect to the optimization objective. This quantity is "emitted" by sensors, "scattered" by the scene, and eventually "received" by objects with differentiable parameters. Differentiable rendering then decomposes into two separate primal and adjoint simulation steps that scale to complex scenes rendered at high resolutions. We also investigated biased variants of this algorithm and find that they considerably improve both runtime and convergence speed. We showcase an efficient GPU implementation of radiative backpropagation and compare its performance and the quality of its gradients to prior work.},
  keywords = {differentiable rendering,global illumination,ray tracing}
}

@misc{nowozinFGANTrainingGenerative2016,
  title = {F-{{GAN}}: {{Training Generative Neural Samplers}} Using {{Variational Divergence Minimization}}},
  shorttitle = {F-{{GAN}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  date = {2016-06-02},
  number = {arXiv:1606.00709},
  eprint = {1606.00709},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1606.00709},
  urldate = {2022-05-27},
  abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@inproceedings{oikonomidisEfficientModelbased3D2011,
  title = {Efficient Model-Based {{3D}} Tracking of Hand Articulations Using {{Kinect}}.},
  booktitle = {{{BmVC}}},
  author = {Oikonomidis, Iason and Kyriazis, Nikolaos and Argyros, Antonis A.},
  date = {2011},
  volume = {1},
  number = {2},
  pages = {3},
  doi = {10.5244/C.25.101}
}

@misc{olivaTransformationAutoregressiveNetworks2018,
  title = {Transformation {{Autoregressive Networks}}},
  author = {Oliva, Junier B. and Dubey, Avinava and Zaheer, Manzil and Póczos, Barnabás and Salakhutdinov, Ruslan and Xing, Eric P. and Schneider, Jeff},
  date = {2018-10-23},
  number = {arXiv:1801.09819},
  eprint = {1801.09819},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1801.09819},
  urldate = {2022-05-28},
  abstract = {The fundamental task of general density estimation \$p(x)\$ has been of keen interest to machine learning. In this work, we attempt to systematically characterize methods for density estimation. Broadly speaking, most of the existing methods can be categorized into either using: \textbackslash textit\{a\}) autoregressive models to estimate the conditional factors of the chain rule, \$p(x\_\{i\}\textbackslash, |\textbackslash, x\_\{i-1\}, \textbackslash ldots)\$; or \textbackslash textit\{b\}) non-linear transformations of variables of a simple base distribution. Based on the study of the characteristics of these categories, we propose multiple novel methods for each category. For example we proposed RNN based transformations to model non-Markovian dependencies. Further, through a comprehensive study over both real world and synthetic data, we show for that jointly leveraging transformations of variables and autoregressive conditional models, results in a considerable improvement in performance. We illustrate the use of our models in outlier detection and image modeling. Finally we introduce a novel data driven framework for learning a family of distributions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning}
}

@inproceedings{orenGeneralizationLambertReflectance1994,
  title = {Generalization of {{Lambert}}'s Reflectance Model},
  booktitle = {Proceedings of the 21st Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Oren, Michael and Nayar, Shree K.},
  date = {1994-07-24},
  series = {{{SIGGRAPH}} '94},
  pages = {239--246},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/192161.192213},
  url = {https://doi.org/10.1145/192161.192213},
  urldate = {2022-09-08},
  abstract = {Lambert's model for body reflection is widely used in computer graphics. It is used extensively by rendering techniques such as radiosity and ray tracing. For several real-world objects, however, Lambert's model can prove to be a very inaccurate approximation to the body reflectance. While the brightness of a Lambertian surface is independent of viewing direction, that of a rough surface increases as the viewing direction approaches the light source direction. In this paper, a comprehensive model is developed that predicts body reflectance from rough surfaces. The surface is modeled as a collection of Lambertian facets. It is shown that such a surface is inherently non-Lambertian due to the foreshortening of the surface facets. Further, the model accounts for complex geometric and radiometric phenomena such as masking, shadowing, and interreflections between facets. Several experiments have been conducted on samples of rough diffuse surfaces, such as, plaster, sand, clay, and cloth. All these surface demonstrate significant deviation from Lambertian behavior. The reflectance measurements obtained are in strong agreement with the reflectance predicted by the model.},
  isbn = {978-0-89791-667-7},
  keywords = {BRDF,Lambert's model,moon reflectance,reflection models,rough surfaces}
}

@inproceedings{oswaldLearningWhereLearn2022,
  title = {Learning Where to Learn: {{Gradient}} Sparsity in Meta and Continual Learning},
  shorttitle = {Learning Where to Learn},
  author = {Oswald, Johannes Von and Zhao, Dominic and Kobayashi, Seijin and Schug, Simon and Caccia, Massimo and Zucchet, Nicolas and Sacramento, Joao},
  date = {2022-01-15},
  url = {https://openreview.net/forum?id=8p46f7pYckL},
  urldate = {2022-09-17},
  abstract = {Finding neural network weights that generalize well from small datasets is difficult. A promising approach is to learn a weight initialization such that a small number of weight changes results in...},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english}
}

@inproceedings{papamakariosMaskedAutoregressiveFlow2017,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html},
  urldate = {2022-05-28},
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  keywords = {⛔ No DOI found}
}

@misc{papamakariosNormalizingFlowsProbabilistic2021,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  date = {2021-04-08},
  number = {arXiv:1912.02762},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.02762},
  url = {http://arxiv.org/abs/1912.02762},
  urldate = {2022-05-23},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{parkDeepSDFLearningContinuous2019,
  title = {{{DeepSDF}}: {{Learning Continuous Signed Distance Functions}} for {{Shape Representation}}},
  shorttitle = {{{DeepSDF}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  date = {2019-06},
  pages = {165--174},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/cvpr.2019.00025},
  url = {https://ieeexplore.ieee.org/document/8954065/},
  urldate = {2022-03-07},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8}
}

@inproceedings{parkLEDGloveBasedInteractions2006,
  title = {{{LED-Glove Based Interactions}} in {{Multi-Modal Displays}} for {{Teleconferencing}}},
  booktitle = {16th {{International Conference}} on {{Artificial Reality}} and {{Telexistence--Workshops}} ({{ICAT}}'06)},
  author = {Park, Jun and Yoon, Yeo-lip},
  date = {2006-11},
  pages = {395--399},
  publisher = {{IEEE}},
  location = {{Hangzhou}},
  doi = {10.1109/ICAT.2006.80},
  url = {https://ieeexplore.ieee.org/document/4089280/},
  urldate = {2022-04-18},
  eventtitle = {16th {{International Conference}} on {{Artificial Reality}} and {{Telexistence-Workshops}} ({{ICAT}}'06)},
  isbn = {978-0-7695-2754-3}
}

@inproceedings{parkMetaCurvature2019,
  title = {Meta-{{Curvature}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Park, Eunbyung and Oliva, Junier B},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/57c0531e13f40b91b3b0f1a30b529a1d-Abstract.html},
  urldate = {2022-09-16},
  abstract = {We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the model-agnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model's parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task specific techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.},
  keywords = {⛔ No DOI found}
}

@inproceedings{parkSeeingWorldBag2020,
  title = {Seeing the {{World}} in a {{Bag}} of {{Chips}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Park, Jeong Joon and Holynski, Aleksander and Seitz, Steven M.},
  date = {2020-06},
  pages = {1414--1424},
  issn = {2575-7075},
  doi = {10.1109/CVPR42600.2020.00149},
  abstract = {We address the dual problems of novel view synthesis and environment reconstruction from hand-held RGBD sensors. Our contributions include 1) modeling highly specular objects, 2) modeling inter-reflections and Fresnel effects, and 3) enabling surface light field reconstruction with the same input needed to reconstruct shape alone. In cases where scene surface has a strong mirror-like material component, we generate highly detailed environment images, revealing room composition, objects, people, buildings, and trees visible through windows. Our approach yields state of the art view synthesis techniques, operates on low dynamic range imagery, and is robust to geometric and calibration errors.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Cameras,Estimation,Geometry,Image reconstruction,Lighting,Rendering (computer graphics),Surface reconstruction}
}

@article{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017},
  keywords = {⛔ No DOI found}
}

@article{peckPuttingYourselfSkin2013,
  title = {Putting Yourself in the Skin of a Black Avatar Reduces Implicit Racial Bias},
  author = {Peck, Tabitha C. and Seinfeld, Sofia and Aglioti, Salvatore M. and Slater, Mel},
  date = {2013-09-01},
  journaltitle = {Consciousness and Cognition},
  shortjournal = {Consciousness and Cognition},
  volume = {22},
  number = {3},
  pages = {779--787},
  issn = {1053-8100},
  doi = {10.1016/j.concog.2013.04.016},
  url = {https://www.sciencedirect.com/science/article/pii/S1053810013000597},
  urldate = {2022-04-12},
  abstract = {Although it has been shown that immersive virtual reality (IVR) can be used to induce illusions of ownership over a virtual body (VB), information on whether this changes implicit interpersonal attitudes is meager. Here we demonstrate that embodiment of light-skinned participants in a dark-skinned VB significantly reduced implicit racial bias against dark-skinned people, in contrast to embodiment in light-skinned, purple-skinned or with no VB. 60 females participated in this between-groups experiment, with a VB substituting their own, with full-body visuomotor synchrony, reflected also in a virtual mirror. A racial Implicit Association Test (IAT) was administered at least three days prior to the experiment, and immediately after the IVR exposure. The change from pre- to post-experience IAT scores suggests that the dark-skinned embodied condition decreased implicit racial bias more than the other conditions. Thus, embodiment may change negative interpersonal attitudes and thus represent a powerful tool for exploring such fundamental psychological and societal phenomena.},
  langid = {english},
  keywords = {Body ownership,Embodiment,IAT,Implicit Association Test,Racial bias,Virtual environment,Virtual reality}
}

@article{petkovaIfWereYou2008,
  title = {If {{I Were You}}: {{Perceptual Illusion}} of {{Body Swapping}}},
  shorttitle = {If {{I Were You}}},
  author = {Petkova, Valeria I. and Ehrsson, H. Henrik},
  date = {2008-12-03},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {3},
  number = {12},
  pages = {e3832},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0003832},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0003832},
  urldate = {2022-04-12},
  abstract = {The concept of an individual swapping his or her body with that of another person has captured the imagination of writers and artists for decades. Although this topic has not been the subject of investigation in science, it exemplifies the fundamental question of why we have an ongoing experience of being located inside our bodies. Here we report a perceptual illusion of body-swapping that addresses directly this issue. Manipulation of the visual perspective, in combination with the receipt of correlated multisensory information from the body was sufficient to trigger the illusion that another person's body or an artificial body was one's own. This effect was so strong that people could experience being in another person's body when facing their own body and shaking hands with it. Our results are of fundamental importance because they identify the perceptual processes that produce the feeling of ownership of one's body.},
  langid = {english},
  keywords = {Abdomen,Arms,Body limbs,Emotions,Experimental design,Hands,Sensory perception,Vision}
}

@article{petkovaWhenRightFeels2009,
  title = {When {{Right Feels Left}}: {{Referral}} of {{Touch}} and {{Ownership}} between the {{Hands}}},
  shorttitle = {When {{Right Feels Left}}},
  author = {Petkova, Valeria I. and Ehrsson, H. Henrik},
  date = {2009-09-09},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {4},
  number = {9},
  pages = {e6933},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0006933},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006933},
  urldate = {2022-04-12},
  abstract = {Feeling touch on a body part is paradigmatically considered to require stimulation of tactile afferents from the body part in question, at least in healthy non-synaesthetic individuals. In contrast to this view, we report a perceptual illusion where people experience “phantom touches” on a right rubber hand when they see it brushed simultaneously with brushes applied to their left hand. Such illusory duplication and transfer of touch from the left to the right hand was only elicited when a homologous (i.e., left and right) pair of hands was brushed in synchrony for an extended period of time. This stimulation caused the majority of our participants to perceive the right rubber hand as their own and to sense two distinct touches – one located on the right rubber hand and the other on their left (stimulated) hand. This effect was supported by quantitative subjective reports in the form of questionnaires, behavioral data from a task in which participants pointed to the felt location of their right hand, and physiological evidence obtained by skin conductance responses when threatening the model hand. Our findings suggest that visual information augments subthreshold somatosensory responses in the ipsilateral hemisphere, thus producing a tactile experience from the non-stimulated body part. This finding is important because it reveals a new bilateral multisensory mechanism for tactile perception and limb ownership.},
  langid = {english},
  keywords = {Body limbs,Hands,Rubber,Sensory perception,Skin physiology,Tactile sensation,Touch,Vision}
}

@book{pharrPhysicallyBasedRendering2016,
  title = {Physically Based Rendering: {{From}} Theory to Implementation (3rd Ed.)},
  author = {Pharr, Matt and Jakob, Wenzel and Humphreys, Greg},
  date = {2016-10},
  edition = {3rd},
  pages = {1266},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{San Francisco, CA, USA}},
  isbn = {978-0-12-800645-0}
}

@article{phongIlluminationComputerGenerated1975,
  title = {Illumination for Computer Generated Pictures},
  author = {Phong, Bui Tuong},
  date = {1975},
  journaltitle = {Communications of the ACM},
  volume = {18},
  number = {6},
  pages = {311--317},
  publisher = {{ACM New York, NY, USA}},
  doi = {10.1145/360825.360839}
}

@article{piryankovaOwningOverweightUnderweight2014,
  title = {Owning an {{Overweight}} or {{Underweight Body}}: {{Distinguishing}} the {{Physical}}, {{Experienced}} and {{Virtual Body}}},
  shorttitle = {Owning an {{Overweight}} or {{Underweight Body}}},
  author = {Piryankova, Ivelina V. and Wong, Hong Yu and Linkenauger, Sally A. and Stinson, Catherine and Longo, Matthew R. and Bülthoff, Heinrich H. and Mohler, Betty J.},
  date = {2014-08-01},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {8},
  pages = {e103428},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0103428},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0103428},
  urldate = {2022-04-12},
  abstract = {Our bodies are the most intimately familiar objects we encounter in our perceptual environment. Virtual reality provides a unique method to allow us to experience having a very different body from our own, thereby providing a valuable method to explore the plasticity of body representation. In this paper, we show that women can experience ownership over a whole virtual body that is considerably smaller or larger than their physical body. In order to gain a better understanding of the mechanisms underlying body ownership, we use an embodiment questionnaire, and introduce two new behavioral response measures: an affordance estimation task (indirect measure of body size) and a body size estimation task (direct measure of body size). Interestingly, after viewing the virtual body from first person perspective, both the affordance and the body size estimation tasks indicate a change in the perception of the size of the participant's experienced body. The change is biased by the size of the virtual body (overweight or underweight). Another novel aspect of our study is that we distinguish between the physical, experienced and virtual bodies, by asking participants to provide affordance and body size estimations for each of the three bodies separately. This methodological point is important for virtual reality experiments investigating body ownership of a virtual body, because it offers a better understanding of which cues (e.g. visual, proprioceptive, memory, or a combination thereof) influence body perception, and whether the impact of these cues can vary between different setups.},
  langid = {english},
  keywords = {Arms,Hip,Overweight,Physiological parameters,Sensory cues,Sensory perception,Touch,Vision}
}

@inproceedings{plattFastTrainingSupport1998,
  title = {Fast {{Training}} of {{Support Vector Machines Using Sequential Minimal Optimization}}},
  author = {Platt, John},
  date = {1998-01-01},
  url = {https://www.microsoft.com/en-us/research/publication/fast-training-of-support-vector-machines-using-sequential-minimal-optimization/},
  urldate = {2022-04-01},
  abstract = {This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which […]},
  eventtitle = {Advances in {{Kernel Methods}} - {{Support Vector Learning}}},
  langid = {american}
}

@article{plattSequentialMinimalOptimization1998,
  title = {Sequential {{Minimal Optimization}}: {{A Fast Algorithm}} for {{Training Support Vector Machines}}},
  shorttitle = {Sequential {{Minimal Optimization}}},
  author = {Platt, John},
  date = {1998-04-21},
  url = {https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/},
  urldate = {2022-04-01},
  abstract = {This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which […]},
  langid = {american},
  keywords = {⛔ No DOI found}
}

@inproceedings{qianRealtimeRobustHand2014,
  title = {Realtime and {{Robust Hand Tracking}} from {{Depth}}},
  author = {Qian, Chen and Sun, Xiao and Wei, Yichen and Tang, Xiaoou and Sun, Jian},
  date = {2014},
  pages = {1106--1113},
  doi = {10.1109/cvpr.2014.145},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Qian_Realtime_and_Robust_2014_CVPR_paper.html},
  urldate = {2022-04-10},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@unpublished{raghuRapidLearningFeature2020,
  title = {Rapid {{Learning}} or {{Feature Reuse}}? {{Towards Understanding}} the {{Effectiveness}} of {{MAML}}},
  shorttitle = {Rapid {{Learning}} or {{Feature Reuse}}?},
  author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  date = {2020-02-12},
  eprint = {1909.09157},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1909.09157},
  url = {http://arxiv.org/abs/1909.09157},
  urldate = {2022-05-11},
  abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of a MAML-trained network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{rahamanSpectralBiasNeural2018,
  title = {On the {{Spectral Bias}} of {{Neural Networks}}},
  author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
  date = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1806.08734},
  url = {https://arxiv.org/abs/1806.08734},
  urldate = {2022-06-12},
  abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with \$100\textbackslash\%\$ accuracy. In this work, we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets \textbackslash emph\{easier\} with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.},
  version = {3},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)}
}

@article{rainerNeuralBTFCompression2019,
  title = {Neural {{BTF Compression}} and {{Interpolation}}},
  author = {Rainer, Gilles and Jakob, Wenzel and Ghosh, Abhijeet and Weyrich, Tim},
  date = {2019},
  journaltitle = {Computer Graphics Forum},
  volume = {38},
  number = {2},
  pages = {235--244},
  issn = {1467-8659},
  doi = {10.1111/cgf.13633},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13633},
  urldate = {2021-12-18},
  abstract = {The Bidirectional Texture Function (BTF) is a data-driven solution to render materials with complex appearance. A typical capture contains tens of thousands of images of a material sample under varying viewing and lighting conditions. While capable of faithfully recording complex light interactions in the material, the main drawback is the massive memory requirement, both for storing and rendering, making effective compression of BTF data a critical component in practical applications. Common compression schemes used in practice are based on matrix factorization techniques, which preserve the discrete format of the original dataset. While this approach generalizes well to different materials, rendering with the compressed dataset still relies on interpolating between the closest samples. Depending on the material and the angular resolution of the BTF, this can lead to blurring and ghosting artefacts. An alternative approach uses analytic model fitting to approximate the BTF data, using continuous functions that naturally interpolate well, but whose expressive range is often not wide enough to faithfully recreate materials with complex non-local lighting effects (subsurface scattering, inter-reflections, shadowing and masking…). In light of these observations, we propose a neural network-based BTF representation inspired by autoencoders: our encoder compresses each texel to a small set of latent coefficients, while our decoder additionally takes in a light and view direction and outputs a single RGB vector at a time. This allows us to continuously query reflectance values in the light and view hemispheres, eliminating the need for linear interpolation between discrete samples. We train our architecture on fabric BTFs with a challenging appearance and compare to standard PCA as a baseline. We achieve competitive compression ratios and high-quality interpolation/extrapolation without blurring or ghosting artifacts.},
  langid = {english},
  keywords = {• Computing methodologies → Reflectance modeling,CCS Concepts,Image compression,Image-based rendering,Neural networks}
}

@article{rainerNeuralPrecomputedRadiance2022,
  title = {Neural {{Precomputed Radiance Transfer}}},
  author = {Rainer, Gilles and Bousseau, Adrien and Ritschel, Tobias and Drettakis, George},
  date = {2022-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {41},
  number = {2},
  pages = {365--378},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14480},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14480},
  urldate = {2022-06-25},
  langid = {english}
}

@article{rainerUnifiedNeuralEncoding2020,
  title = {Unified {{Neural Encoding}} of {{BTFs}}},
  author = {Rainer, Gilles and Ghosh, Abhijeet and Jakob, Wenzel and Weyrich, Tim},
  date = {2020-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {39},
  number = {2},
  pages = {167--178},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13921},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13921},
  urldate = {2022-01-19},
  langid = {english}
}

@article{raissiPhysicsinformedNeuralNetworks2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
  date = {2019-02-01},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2018.10.045},
  url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
  urldate = {2022-05-11},
  abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
  langid = {english},
  keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge–Kutta methods}
}

@inproceedings{rajeswaranMetaLearningImplicitGradients2019,
  title = {Meta-{{Learning}} with {{Implicit Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html},
  urldate = {2022-03-07},
  keywords = {⛔ No DOI found}
}

@inproceedings{raviOptimizationModelFewshot2017,
  title = {Optimization as a Model for Few-Shot Learning},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Ravi, Sachin and Larochelle, Hugo},
  date = {2017},
  url = {https://openreview.net/forum?id=rJY0-Kcll},
  keywords = {⛔ No DOI found}
}

@inproceedings{rehgVisualTrackingHigh1994,
  title = {Visual Tracking of High {{DOF}} Articulated Structures: {{An}} Application to Human Hand Tracking},
  shorttitle = {Visual Tracking of High {{DOF}} Articulated Structures},
  booktitle = {Computer {{Vision}} — {{ECCV}} '94},
  author = {Rehg, James M. and Kanade, Takeo},
  editor = {Eklundh, Jan-Olof},
  date = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {35--46},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0028333},
  abstract = {Passive sensing of human hand and limb motion is important for a wide range of applications from human-computer interaction to athletic performance measurement. High degree of freedom articulated mechanisms like the human hand are difficult to track because of their large state space and complex image appearance. This article describes a model-based hand tracking system, called DigitEyes, that can recover the state of a 27 DOF hand model from ordinary gray scale images at speeds of up to 10 Hz.},
  isbn = {978-3-540-48400-4},
  langid = {english},
  keywords = {Hand Tracking,Human Hand,Kinematic Chain,Residual Vector,Visual Tracking}
}

@misc{reizensteinCommonObjects3D2021,
  title = {Common {{Objects}} in {{3D}}: {{Large-Scale Learning}} and {{Evaluation}} of {{Real-life 3D Category Reconstruction}}},
  shorttitle = {Common {{Objects}} in {{3D}}},
  author = {Reizenstein, Jeremy and Shapovalov, Roman and Henzler, Philipp and Sbordone, Luca and Labatut, Patrick and Novotny, David},
  date = {2021-09-01},
  number = {arXiv:2109.00512},
  eprint = {2109.00512},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.00512},
  urldate = {2022-06-22},
  abstract = {Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale "in-the-wild" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{rematasDeepReflectanceMaps2016,
  title = {Deep Reflectance Maps},
  booktitle = {{{CVPR}}},
  author = {Rematas, Konstantinos and Ritschel, Tobias and Fritz, Mario and Gavves, Efstratios and Tuytelaars, Tinne},
  date = {2016},
  doi = {10.1109/CVPR.2016.488}
}

@article{renGlobalIlluminationRadiance2013,
  title = {Global Illumination with Radiance Regression Functions},
  author = {Ren, Peiran and Wang, Jiaping and Gong, Minmin and Lin, Stephen and Tong, Xin and Guo, Baining},
  date = {2013-07-21},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {32},
  number = {4},
  pages = {130:1--130:12},
  issn = {0730-0301},
  doi = {10.1145/2461912.2462009},
  url = {https://doi.org/10.1145/2461912.2462009},
  urldate = {2022-10-25},
  abstract = {We present radiance regression functions for fast rendering of global illumination in scenes with dynamic local light sources. A radiance regression function (RRF) represents a non-linear mapping from local and contextual attributes of surface points, such as position, viewing direction, and lighting condition, to their indirect illumination values. The RRF is obtained from precomputed shading samples through regression analysis, which determines a function that best fits the shading data. For a given scene, the shading samples are precomputed by an offline renderer. The key idea behind our approach is to exploit the nonlinear coherence of the indirect illumination data to make the RRF both compact and fast to evaluate. We model the RRF as a multilayer acyclic feed-forward neural network, which provides a close functional approximation of the indirect illumination and can be efficiently evaluated at run time. To effectively model scenes with spatially variant material properties, we utilize an augmented set of attributes as input to the neural network RRF to reduce the amount of inference that the network needs to perform. To handle scenes with greater geometric complexity, we partition the input space of the RRF model and represent the subspaces with separate, smaller RRFs that can be evaluated more rapidly. As a result, the RRF model scales well to increasingly complex scene geometry and material variation. Because of its compactness and ease of evaluation, the RRF model enables real-time rendering with full global illumination effects, including changing caustics and multiple-bounce high-frequency glossy interreflections.},
  keywords = {global illumination,neural network,non-linear regression,real time rendering}
}

@inproceedings{rezendeNormalizingFlowsTori2020,
  title = {Normalizing {{Flows}} on {{Tori}} and {{Spheres}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Rezende, Danilo Jimenez and Papamakarios, George and Racaniere, Sebastien and Albergo, Michael and Kanwar, Gurtej and Shanahan, Phiala and Cranmer, Kyle},
  date = {2020-11-21},
  pages = {8083--8092},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/rezende20a.html},
  urldate = {2022-05-29},
  abstract = {Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@inproceedings{rezendeVariationalInferenceNormalizing2015,
  ids = {rezendeVariationalInferenceNormalizing2015a},
  title = {Variational Inference with Normalizing Flows},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2015-07-06},
  series = {{{ICML}}'15},
  pages = {1530--1538},
  publisher = {{JMLR.org}},
  location = {{Lille, France}},
  doi = {10.48550/arXiv.1505.05770},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}

@inproceedings{romeiroPassiveReflectometry2008,
  title = {Passive {{Reflectometry}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2008},
  author = {Romeiro, Fabiano and Vasilyev, Yuriy and Zickler, Todd},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {859--872},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-88693-8_63},
  abstract = {Different materials reflect light in different ways, so reflectance is a useful surface descriptor. Existing systems for measuring reflectance are cumbersome, however, and although the process can be streamlined using cameras, projectors and clever catadioptrics, it generally requires complex infrastructure. In this paper we propose a simpler method for inferring reflectance from images, one that eliminates the need for active lighting and exploits natural illumination instead. The method’s distinguishing property is its ability to handle a broad class of isotropic reflectance functions, including those that are neither radially-symmetric nor well-represented by low-parameter reflectance models. The key to the approach is a bi-variate representation of isotropic reflectance that enables a tractable inference algorithm while maintaining generality. The resulting method requires only a camera, a light probe, and as little as one HDR image of a known, curved, homogeneous surface.},
  isbn = {978-3-540-88693-8},
  langid = {english},
  keywords = {Bilateral Symmetry,Grazing Angle,Input Image,Light Probe,Synthetic Image}
}

@article{romeroEmbodiedHandsModeling2017,
  title = {Embodied Hands: Modeling and Capturing Hands and Bodies Together},
  shorttitle = {Embodied Hands},
  author = {Romero, Javier and Tzionas, Dimitrios and Black, Michael J.},
  date = {2017-11-20},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {36},
  number = {6},
  pages = {1--17},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3130800.3130883},
  url = {https://dl.acm.org/doi/10.1145/3130800.3130883},
  urldate = {2022-04-11},
  abstract = {Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of               hands and bodies interacting together               and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called               MANO               (               hand Model with Articulated and Non-rigid defOrmations               ). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes at http://mano.is.tue.mpg.de.},
  langid = {english}
}

@incollection{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} – {{MICCAI}} 2015},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9351},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-24574-4_28},
  url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
  urldate = {2022-04-18},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english}
}

@inproceedings{rostenMachineLearningHighSpeed2006,
  title = {Machine {{Learning}} for {{High-Speed Corner Detection}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2006},
  author = {Rosten, Edward and Drummond, Tom},
  editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {430--443},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11744023_34},
  abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7\% of the available processing time. By comparison neither the Harris detector (120\%) nor the detection stage of SIFT (300\%) can operate at full frame rate.},
  isbn = {978-3-540-33833-8},
  langid = {english},
  keywords = {Corner Detection,Feature Detector,Interest Point,Interest Point Detector,Segment Test}
}

@inproceedings{rozenMoserFlowDivergencebased2021,
  title = {Moser {{Flow}}: {{Divergence-based Generative Modeling}} on {{Manifolds}}},
  shorttitle = {Moser {{Flow}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  date = {2021},
  volume = {34},
  pages = {17669--17680},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/93a27b0bd99bac3e68a440b48aa421ab-Abstract.html},
  urldate = {2022-05-30},
  abstract = {We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences.},
  keywords = {⛔ No DOI found}
}

@inproceedings{rusinkiewiczNewChangeVariables1998,
  title = {A {{New Change}} of {{Variables}} for {{Efficient BRDF Representation}}},
  booktitle = {Rendering {{Techniques}} ’98},
  author = {Rusinkiewicz, Szymon M.},
  editor = {Drettakis, George and Max, Nelson},
  date = {1998},
  series = {Eurographics},
  pages = {11--22},
  publisher = {{Springer}},
  location = {{Vienna}},
  doi = {10.1007/978-3-7091-6453-2_2},
  abstract = {We describe an idea for making decomposition of Bidirectional Reflectance Distribution Functions into basis functions more efficient, by performing a change-of-variables transformation on the BRDFs. In particular, we propose a reparameterization of the BRDF as a function of the halfangle (i.e. the angle halfway between the directions of incidence and reflection) and a difference angle instead of the usual parameterization in terms of angles of incidence and reflection. Because features in common BRDFs, including specular and retroreflective peaks, are aligned with the transformed coordinate axes, the change of basis reduces storage requirements for a large class of BRDFs. We present results derived from analytic BRDFs and measured data.},
  isbn = {978-3-7091-6453-2},
  langid = {english},
  keywords = {Basis Function,Bidirectional Reflectance Distribution Function,Computer Graphic,Nonzero Coefficient,Spherical Harmonic}
}

@article{sanchez-vivesVirtualHandIllusion2010,
  title = {Virtual {{Hand Illusion Induced}} by {{Visuomotor Correlations}}},
  author = {Sanchez-Vives, Maria V. and Spanlang, Bernhard and Frisoli, Antonio and Bergamasco, Massimo and Slater, Mel},
  date = {2010-04-29},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {5},
  number = {4},
  pages = {e10381},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010381},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010381},
  urldate = {2022-04-12},
  abstract = {Background Our body schema gives the subjective impression of being highly stable. However, a number of easily-evoked illusions illustrate its remarkable malleability. In the rubber-hand illusion, illusory ownership of a rubber-hand is evoked by synchronous visual and tactile stimulation on a visible rubber arm and on the hidden real arm. Ownership is concurrent with a proprioceptive illusion of displacement of the arm position towards the fake arm. We have previously shown that this illusion of ownership plus the proprioceptive displacement also occurs towards a virtual 3D projection of an arm when the appropriate synchronous visuotactile stimulation is provided. Our objective here was to explore whether these illusions (ownership and proprioceptive displacement) can be induced by only synchronous visuomotor stimulation, in the absence of tactile stimulation. Methodology/Principal Findings To achieve this we used a data-glove that uses sensors transmitting the positions of fingers to a virtually projected hand in the synchronous but not in the asynchronous condition. The illusion of ownership was measured by means of questionnaires. Questions related to ownership gave significantly larger values for the synchronous than for the asynchronous condition. Proprioceptive displacement provided an objective measure of the illusion and had a median value of 3.5 cm difference between the synchronous and asynchronous conditions. In addition, the correlation between the feeling of ownership of the virtual arm and the size of the drift was significant. Conclusions/Significance We conclude that synchrony between visual and proprioceptive information along with motor activity is able to induce an illusion of ownership over a virtual arm. This has implications regarding the brain mechanisms underlying body ownership as well as the use of virtual bodies in therapies and rehabilitation.},
  langid = {english},
  keywords = {Arms,Fingers,Forearms,Hands,Questionnaires,Rubber,Virtual reality,Vision}
}

@unpublished{schaulPrioritizedExperienceReplay2016,
  title = {Prioritized {{Experience Replay}}},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  date = {2016-02-25},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1511.05952},
  url = {http://arxiv.org/abs/1511.05952},
  urldate = {2022-05-05},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{scholkopfNewSupportVector2000,
  title = {New {{Support Vector Algorithms}}},
  author = {Schölkopf, Bernhard and Smola, Alex J. and Williamson, Robert C. and Bartlett, Peter L.},
  date = {2000-05-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {12},
  number = {5},
  pages = {1207--1245},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976600300015565},
  url = {https://direct.mit.edu/neco/article/12/5/1207-1245/6368},
  urldate = {2022-04-05},
  abstract = {We propose a new class of support vector algorithms for regression and classification. In these algorithms, a parameter ν lets one effectively control the number of support vectors. While this can be useful in its own right, the parameterization has the additional benefit of enabling us to eliminate one of the other free parameters of the algorithm: the accuracy parameter ε in the regression case, and the regularization constant C in the classification case. We describe the algorithms, give some theoretical results concerning the meaning and the choice of ν, and report experimental results.},
  langid = {english}
}

@article{schwartzDesignImplementationPractical2014,
  title = {Design and {{Implementation}} of {{Practical Bidirectional Texture Function Measurement Devices Focusing}} on the {{Developments}} at the {{University}} of {{Bonn}}},
  author = {Schwartz, Christopher and Sarlette, Ralf and Weinmann, Michael and Rump, Martin and Klein, Reinhard},
  date = {2014-04-28},
  journaltitle = {Sensors},
  shortjournal = {Sensors},
  volume = {14},
  number = {5},
  pages = {7753--7819},
  issn = {1424-8220},
  doi = {10.3390/s140507753},
  url = {http://www.mdpi.com/1424-8220/14/5/7753},
  urldate = {2022-09-01},
  langid = {english}
}

@article{seiJacobianInequalityGradient2009,
  title = {A {{Jacobian}} Inequality for Gradient Maps on the Sphere and Its Application to Directional Statistics},
  author = {Sei, Tomonari},
  date = {2009},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.0906.0874},
  url = {https://arxiv.org/abs/0906.0874},
  urldate = {2022-05-30},
  abstract = {In the field of optimal transport theory, an optimal map is known to be a gradient map of a potential function satisfying cost-convexity. In this paper, the Jacobian determinant of a gradient map is shown to be log-concave with respect to a convex combination of the potential functions when the underlying manifold is the sphere and the cost function is the distance squared. The proof uses the non-negative cross-curvature property of the sphere recently established by Kim and McCann, and Figalli and Rifford. As an application to statistics, a new family of probability densities on the sphere is defined in terms of cost-convex functions. The log-concave property of the likelihood function follows from the inequality.},
  version = {3},
  keywords = {49N60; 62E15; 90B06,Differential Geometry (math.DG),FOS: Mathematics,Statistics Theory (math.ST)}
}

@misc{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  author = {Sener, Ozan and Savarese, Silvio},
  date = {2018-06-01},
  number = {arXiv:1708.00489},
  eprint = {1708.00489},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1708.00489},
  url = {http://arxiv.org/abs/1708.00489},
  urldate = {2022-06-29},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{senguptaNeuralInverseRendering2019,
  title = {Neural {{Inverse Rendering}} of an {{Indoor Scene From}} a {{Single Image}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Sengupta, Soumyadip and Gu, Jinwei and Kim, Kihwan and Liu, Guilin and Jacobs, David and Kautz, Jan},
  date = {2019-10},
  pages = {8597--8606},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00869},
  abstract = {Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for single objects or with methods that solve for only one of the scene attributes. We propose the first learning based approach that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects (e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise. This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing the input image from the estimated components. We finetune with real data after pretraining with synthetic data. To this end, we use physically-based rendering to create a large-scale synthetic dataset, named SUNCG-PBR, which is a significant improvement over prior datasets. Experimental results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Face,Geometry,Image reconstruction,Lighting,Rendering (computer graphics),Three-dimensional displays,Training}
}

@article{settlesActiveLearningLiterature2009,
  title = {Active Learning Literature Survey},
  author = {Settles, Burr},
  date = {2009},
  publisher = {{University of Wisconsin-Madison Department of Computer Sciences}},
  keywords = {⛔ No DOI found}
}

@inproceedings{sharpAccurateRobustFlexible2015,
  title = {Accurate, {{Robust}}, and {{Flexible Real-time Hand Tracking}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sharp, Toby and Keskin, Cem and Robertson, Duncan and Taylor, Jonathan and Shotton, Jamie and Kim, David and Rhemann, Christoph and Leichter, Ido and Vinnikov, Alon and Wei, Yichen and Freedman, Daniel and Kohli, Pushmeet and Krupka, Eyal and Fitzgibbon, Andrew and Izadi, Shahram},
  date = {2015-04-18},
  pages = {3633--3642},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/2702123.2702179},
  url = {https://dl.acm.org/doi/10.1145/2702123.2702179},
  urldate = {2022-04-09},
  eventtitle = {{{CHI}} '15: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-3145-6},
  langid = {english}
}

@inproceedings{sheLinearConvergenceSupport2017,
  title = {Linear Convergence and Support Vector Identification of Sequential Minimal Optimization},
  booktitle = {10th {{NIPS Workshop}} on {{Optimization}} for {{Machine Learning}}},
  author = {She, Jennifer and Schmidt, Mark},
  date = {2017},
  volume = {5},
  pages = {50},
  keywords = {⛔ No DOI found}
}

@inproceedings{simonHandKeypointDetection2017,
  title = {Hand {{Keypoint Detection}} in {{Single Images Using Multiview Bootstrapping}}},
  author = {Simon, Tomas and Joo, Hanbyul and Matthews, Iain and Sheikh, Yaser},
  date = {2017},
  pages = {1145--1153},
  doi = {10.1109/CVPR.2017.494},
  url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.html},
  urldate = {2022-04-14},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{sitzmannImplicitNeuralRepresentations2020,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  date = {2020},
  volume = {33},
  pages = {7462--7473},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html},
  urldate = {2022-06-29},
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions.},
  keywords = {⛔ No DOI found}
}

@misc{sitzmannLightFieldNetworks2022,
  title = {Light {{Field Networks}}: {{Neural Scene Representations}} with {{Single-Evaluation Rendering}}},
  shorttitle = {Light {{Field Networks}}},
  author = {Sitzmann, Vincent and Rezchikov, Semon and Freeman, William T. and Tenenbaum, Joshua B. and Durand, Fredo},
  date = {2022-01-18},
  number = {arXiv:2106.02634},
  eprint = {2106.02634},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.02634},
  url = {http://arxiv.org/abs/2106.02634},
  urldate = {2022-10-06},
  abstract = {Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a single network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Multimedia}
}

@unpublished{sitzmannMetaSDFMetalearningSigned2020,
  title = {{{MetaSDF}}: {{Meta-learning Signed Distance Functions}}},
  shorttitle = {{{MetaSDF}}},
  author = {Sitzmann, Vincent and Chan, Eric R. and Tucker, Richard and Snavely, Noah and Wetzstein, Gordon},
  date = {2020-06-17},
  eprint = {2006.09662},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.09662},
  url = {http://arxiv.org/abs/2006.09662},
  urldate = {2022-01-19},
  abstract = {Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@misc{sitzmannSceneRepresentationNetworks2020,
  title = {Scene {{Representation Networks}}: {{Continuous 3D-Structure-Aware Neural Scene Representations}}},
  shorttitle = {Scene {{Representation Networks}}},
  author = {Sitzmann, Vincent and Zollhöfer, Michael and Wetzstein, Gordon},
  date = {2020-01-28},
  number = {arXiv:1906.01618},
  eprint = {1906.01618},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.01618},
  url = {http://arxiv.org/abs/1906.01618},
  urldate = {2022-09-04},
  abstract = {Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,I.2.10,I.4.10,I.4.5,I.4.8}
}

@article{slaterDigitalBodyVirtual2008,
  title = {Towards a Digital Body: The Virtual Arm Illusion},
  shorttitle = {Towards a Digital Body},
  author = {Slater, Mel and Pérez Marcos, Daniel and Ehrsson, Henrik and Sanchez-Vives, Maria},
  date = {2008},
  journaltitle = {Frontiers in Human Neuroscience},
  volume = {2},
  issn = {1662-5161},
  doi = {10.3389/neuro.09.006.2008},
  url = {https://www.frontiersin.org/article/10.3389/neuro.09.006.2008},
  urldate = {2022-04-12},
  abstract = {The integration of the human brain with computers is an interesting new area of applied neuroscience, where one application is replacement of a person's real body by a virtual representation. Here we demonstrate that a virtual limb can be made to feel part of your body if appropriate multisensory correlations are provided. We report an illusion that is invoked through tactile stimulation on a person's hidden real right hand with synchronous virtual visual stimulation on an aligned 3D stereo virtual arm projecting horizontally out of their shoulder. An experiment with 21 male participants showed displacement of ownership towards the virtual hand, as illustrated by questionnaire responses and proprioceptive drift. A control experiment with asynchronous tapping was carried out with a different set of 20 male participants who did not experience the illusion. After 5\,min of stimulation the virtual arm rotated. Evidence suggests that the extent of the illusion was also correlated with the degree of muscle activity onset in the right arm as measured by EMG during this period that the arm was rotating, for the synchronous but not the asynchronous condition. A completely virtual object can therefore be experienced as part of one's self, which opens up the possibility that an entire virtual body could be felt as one's own in future virtual reality applications or online games, and be an invaluable tool for the understanding of the brain mechanisms underlying body ownership.}
}

@article{smithConstrainingDenseHand2020,
  title = {Constraining Dense Hand Surface Tracking with Elasticity},
  author = {Smith, Breannan and Wu, Chenglei and Wen, He and Peluse, Patrick and Sheikh, Yaser and Hodgins, Jessica K. and Shiratori, Takaaki},
  date = {2020-12-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {6},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3414685.3417768},
  url = {https://dl.acm.org/doi/10.1145/3414685.3417768},
  urldate = {2022-02-20},
  langid = {english}
}

@article{smithStableNeoHookeanFlesh2018,
  title = {Stable {{Neo-Hookean Flesh Simulation}}},
  author = {Smith, Breannan and Goes, Fernando De and Kim, Theodore},
  date = {2018-07-03},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {2},
  pages = {1--15},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3180491},
  url = {https://dl.acm.org/doi/10.1145/3180491},
  urldate = {2022-02-25},
  abstract = {Nonlinear hyperelastic energies play a key role in capturing the fleshy appearance of virtual characters. Real-world, volume-preserving biological tissues have Poisson’s ratios near 1/2, but numerical simulation within this regime is notoriously challenging. In order to robustly capture these visual characteristics, we present a novel version of Neo-Hookean elasticity. Our model maintains the fleshy appearance of the Neo-Hookean model, exhibits superior volume preservation, and is robust to extreme kinematic rotations and inversions. We obtain closed-form expressions for the eigenvalues and eigenvectors of all of the system’s components, which allows us to directly project the Hessian to semipositive definiteness, and also leads to insights into the numerical behavior of the material. These findings also inform the design of more sophisticated hyperelastic models, which we explore by applying our analysis to Fung and Arruda-Boyce elasticity. We provide extensive comparisons against existing material models.},
  langid = {english}
}

@unpublished{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  date = {2017-06-19},
  eprint = {1703.05175},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1703.05175},
  url = {http://arxiv.org/abs/1703.05175},
  urldate = {2022-05-11},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{sobolDistributionPointsCube1967,
  title = {On the Distribution of Points in a Cube and the Approximate Evaluation of Integrals},
  author = {Sobol', I. M},
  date = {1967-01-01},
  journaltitle = {USSR Computational Mathematics and Mathematical Physics},
  shortjournal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {7},
  number = {4},
  pages = {86--112},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(67)90144-9},
  url = {https://www.sciencedirect.com/science/article/pii/0041555367901449},
  urldate = {2022-09-07},
  abstract = {In this study, the piston bowl geometry and injector design of a light-duty GCI engine were co-optimized using computational fluid dynamics (CFD) and advanced machine learning (ML) techniques to maximize the capability of a GCI technology. This study was performed at low- (6 bar), mid- (11 bar) and high-load (22 bar) indicated mean effective pressure (IMEP) conditions. The 3-D CFD setup was first validated against experimental data. Then, the injector and the piston bowl were simultaneously co-optimized. In total, 13 (ten piston- and three injector-related) design parameters were considered. At each load condition, 128 DoE cases were generated, and the features and performance of the top three designs were analyzed. The best DoE solution was selected by defining weighted merit values to provide one best design across all load conditions. The simulated dataset was used for further optimization using advanced ML techniques. It was found that a flatter design with low center height, wide and shallow bowl, and the low lip was preferable in the low- and high-loads. At the low-load partially premixed compression ignition (PPCI) mode, spray targeting the upper lip and divided into the bowl and squish zones for enhanced mixing is preferred. At the high-load mixing-controlled diffusion combustion mode, where injection occurs near the top dead center (TDC) and diffusion burn is the dominant combustion mode, targeting the lower lip is favorable. At the mid-load with a high premixed ratio, the combustion was close to homogeneous charge compression ignition (HCCI), and the piston bowl design had limited effect. Regarding the optimum injector parameters, larger number nozzles with smaller diameters were favorable at low-load to control partially premixed charge. At high-load, larger and fewer number nozzles are recommended. Lastly, the optimum ML design provided similar performance at mid-load but a 3.8–4.5 \% reduction in fuel consumption compared to the baseline cases at low- and high-load conditions. Multi-objective Bayesian optimization (MOBO) is an efficient and robust optimization framework for expensive functions. In this work, we use MOBO to optimize the free parameters of a high-order nonlinear weighted essentially non-oscillatory (WENO) reconstruction scheme to devise a model for implicit large eddy simulations. We concurrently optimize for a low dispersion error and sufficient shock-capturing ability for compressible flows as well as for physically consistent transition occurring in under-resolved flow regions. With our approach, we follow the genealogy of designing implicit sub-grid models. Yet, in contrast to previous works that were limited to incompressible flows, our model is also applicable to compressible flows. Validated results show that the model is able to decrease excessive dissipation in continuous flow regimes, to capture shocks with little dispersive and dissipative errors while achieving a well shaped vortical structures. The proposed framework is general and can be used to design a physically consistent numerical scheme for under-resolved compressible-flow simulations. The estimation of American option sensitivities is a challenging problem in financial engineering due to the stopping time involved. Progress has been made in using Monte Carlo (MC) simulation to obtain the estimation. However, the applicability of quasi-Monte Carlo (QMC) methods on estimating American option sensitivities is open. In this paper, we address this problem and propose efficient QMC methods for estimating American option sensitivities. Dimension reduction techniques, such as the Brownian bridge and principal component analysis, are used for further efficiency improvements. Numerical experiments in the cases of single underlying asset and multiple underlying assets under the Black–Scholes model and the variance gamma model demonstrate that QMC with dimension reduction techniques can significantly reduce the variance of the estimators of American option sensitivities. This paper introduces a decoupled method of calculating the PV hosting capacity of low voltage distribution system (LVDS) feeders for new solar photovoltaics (PV) installations. The hosting capacity calculation of LVDS is a multidimensional stochastic problem. A general polynomial chaos-based probabilistic power flow is used to solve this problem, as it allows for fast computation times without any compromise in accuracy. Two types of uncertainties exist in the hosting capacity calculation problem: planning level uncertainties such as size, location, type, and number of PV installations and operational uncertainties such as consumer load and PV generation. These two types of uncertainties are usually sampled together in probabilistic hosting capacity approaches. In this paper, a decoupled approach is presented where the impact of planning level scenarios on the probability of violation of operational limits is studied. The highest total PV among the planning scenarios inside the probabilistic bound of the operational limit is termed as the hosting capacity of the feeder. The results show that hosting capacity depends upon the planning uncertainties and operational variables, and decoupling them gives a more concise picture of the impact of different uncertainties. Uncertainty propagation and sensitivity analysis have been applied to SMA-TB interatomic potentials that are often used in atomistic simulations to study metals and alloys. Usually, the parameters of potentials are estimated by a minimization method to best fit a set of experimental or ab initio computational data, such as bulk properties (e.g., cohesive energy and elastic constants) without considering the uncertainty in these data. Our goal is to perform uncertainty propagation from the bulk properties to extract uncertainties and correlations of SMA-TB ξ,A,q,p{$<$}math{$><$}mfenced close=")" open="(" is="true"{$><$}mrow is="true"{$><$}mi is="true"{$>$}ξ{$<$}/mi{$><$}mo is="true"{$>$},{$<$}/mo{$><$}mi is="true"{$>$}A{$<$}/mi{$><$}mo is="true"{$>$},{$<$}/mo{$><$}mi is="true"{$>$}q{$<$}/mi{$><$}mo is="true"{$>$},{$<$}/mo{$><$}mi is="true"{$>$}p{$<$}/mi{$><$}/mrow{$><$}/mfenced{$><$}/math{$>$} potential parameters to generate valid parameter sets. Using a statistical framework, we estimate initial probability distributions that allow us to determine the uncertainties for each potential parameter of the SMA-TB. We show that many sets of potential parameters lead to bulk properties included in the initial probability distributions. Local and global Sobol’ sensitivity analysis methods, which entails no additional computational cost, show how physical properties can be adjusted using interatomic potentials. In this paper, scrambled Halton sequences are shown to have a form of negative dependence that is desirable for the purpose of improving upon the Monte Carlo method for multivariate integration. The scrambling methods with these properties are based on either the nested uniform permutations of Owen or the random linear scrambling of Matoušek. The framework of negative dependence is also used to develop new criteria for assessing the quality of generalized Halton sequences, in such a way that they can be analyzed for finite (potentially small) point set sizes and be compared to digital net constructions. Using this type of criteria, parameters for a new generalized Halton sequence are derived. Numerical results are presented to compare different generalized Halton sequences and their randomizations.},
  langid = {english}
}

@inproceedings{sridharInteractiveMarkerlessArticulated2013,
  title = {Interactive {{Markerless Articulated Hand Motion Tracking Using RGB}} and {{Depth Data}}},
  author = {Sridhar, Srinath and Oulasvirta, Antti and Theobalt, Christian},
  date = {2013},
  pages = {2456--2463},
  url = {https://www.cv-foundation.org/openaccess/content_iccv_2013/html/Sridhar_Interactive_Markerless_Articulated_2013_ICCV_paper.html},
  urldate = {2022-04-18},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}}
}

@inproceedings{stichSafeAdaptiveImportance2017,
  title = {Safe {{Adaptive Importance Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Stich, Sebastian U and Raj, Anant and Jaggi, Martin},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/1177967c7957072da3dc1db4ceb30e7a-Abstract.html},
  urldate = {2022-06-28},
  abstract = {Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants -- using importance values defined by the complete gradient information which changes during optimization -- enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is  (i) provably the \textbackslash emph\{best sampling\} with respect to the given bounds,  (ii) always better than uniform sampling and fixed importance sampling and  (iii) can efficiently be computed -- in many applications  at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.},
  keywords = {⛔ No DOI found}
}

@article{sunConnectingMeasuredBRDFs2018,
  title = {Connecting Measured {{BRDFs}} to Analytic {{BRDFs}} by Data-Driven Diffuse-Specular Separation},
  author = {Sun, Tiancheng and Jensen, Henrik Wann and Ramamoorthi, Ravi},
  date = {2018-12-04},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {6},
  pages = {273:1--273:15},
  issn = {0730-0301},
  doi = {10.1145/3272127.3275026},
  url = {https://doi.org/10.1145/3272127.3275026},
  urldate = {2022-09-03},
  abstract = {The bidirectional reflectance distribution function (BRDF) is crucial for modeling the appearance of real-world materials. In production rendering, analytic BRDF models are often used to approximate the surface appearance since they are compact and flexible. Measured BRDFs usually have a more realistic appearance, but consume much more storage and are hard to modify. In this paper, we propose a novel framework for connecting measured and analytic BRDFs. First, we develop a robust method for separating a measured BRDF into diffuse and specular components. This is commonly done in analytic models, but has been difficult previously to do explicitly for measured BRDFs. This diffuse-specular separation allows novel measured BRDF editing on the diffuse and specular parts separately. In addition, we conduct analysis on each part of the measured BRDF, and demonstrate a more intuitive and lower-dimensional PCA model than Nielsen et al. [2015]. In fact, our measured BRDF model has the same number of parameters (8 parameters) as the commonly used analytic models, such as the GGX model. Finally, we visualize the analytic and measured BRDFs in the same space, and directly demonstrate their similarities and differences. We also design an analytic fitting algorithm for two-lobe materials, which is more robust, efficient and simple, compared to previous non-convex optimization-based analytic fitting methods.},
  keywords = {.skimmed,👀,analytic BRDF fitting,analytic BRDF models,measured BRDF editing,measured BRDF models}
}

@article{sunNeural3DReconstruction2022,
  title = {Neural {{3D Reconstruction}} in the {{Wild}}},
  author = {Sun, Jiaming and Chen, Xi and Wang, Qianqian and Li, Zhengqi and Averbuch-Elor, Hadar and Zhou, Xiaowei and Snavely, Noah},
  date = {2022-05-25},
  doi = {10.1145/3528233.3530718},
  url = {https://arxiv.org/abs/2205.12955v1},
  urldate = {2022-06-13},
  abstract = {We are witnessing an explosion of neural implicit representations in computer vision and graphics. Their applicability has recently expanded beyond tasks such as shape generation and image-based rendering to the fundamental problem of image-based 3D reconstruction. However, existing methods typically assume constrained 3D environments with constant illumination captured by a small set of roughly uniformly distributed cameras. We introduce a new method that enables efficient and accurate surface reconstruction from Internet photo collections in the presence of varying illumination. To achieve this, we propose a hybrid voxel- and surface-guided sampling technique that allows for more efficient ray sampling around surfaces and leads to significant improvements in reconstruction quality. Further, we present a new benchmark and protocol for evaluating reconstruction performance on such in-the-wild scenes. We perform extensive experiments, demonstrating that our approach surpasses both classical and neural reconstruction methods on a wide variety of metrics.},
  langid = {english}
}

@article{sztrajmanNeuralBRDFRepresentation2021,
  title = {Neural {{BRDF Representation}} and {{Importance Sampling}}},
  author = {Sztrajman, Alejandro and Rainer, Gilles and Ritschel, Tobias and Weyrich, Tim},
  date = {2021-09},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {40},
  number = {6},
  pages = {332--346},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14335},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14335},
  urldate = {2021-12-18},
  langid = {english}
}

@article{takahashiRigorousProofTermination2005,
  title = {Rigorous {{Proof}} of {{Termination}} of {{SMO Algorithm}} for {{Support Vector Machines}}},
  author = {Takahashi, N. and Nishi, T.},
  date = {2005-05},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {16},
  number = {3},
  pages = {774--776},
  issn = {1045-9227},
  doi = {10.1109/tnn.2005.844857},
  url = {http://ieeexplore.ieee.org/document/1427778/},
  urldate = {2022-04-01},
  langid = {english}
}

@inproceedings{takikawaNeuralGeometricLevel2021,
  title = {Neural {{Geometric Level}} of {{Detail}}: {{Real-Time Rendering With Implicit 3D Shapes}}},
  shorttitle = {Neural {{Geometric Level}} of {{Detail}}},
  author = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
  date = {2021},
  pages = {11358--11367},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Takikawa_Neural_Geometric_Level_of_Detail_Real-Time_Rendering_With_Implicit_3D_CVPR_2021_paper.html},
  urldate = {2022-01-24},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english}
}

@unpublished{tancikLearnedInitializationsOptimizing2021,
  title = {Learned {{Initializations}} for {{Optimizing Coordinate-Based Neural Representations}}},
  author = {Tancik, Matthew and Mildenhall, Ben and Wang, Terrance and Schmidt, Divi and Srinivasan, Pratul P. and Barron, Jonathan T. and Ng, Ren},
  date = {2021-03-23},
  eprint = {2012.02189},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.02189},
  url = {http://arxiv.org/abs/2012.02189},
  urldate = {2022-01-19},
  abstract = {Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{tancikLearnedInitializationsOptimizing2021a,
  title = {Learned {{Initializations}} for {{Optimizing Coordinate-Based Neural Representations}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tancik, Matthew and Mildenhall, Ben and Wang, Terrance and Schmidt, Divi and Srinivasan, Pratul P. and Barron, Jonathan T. and Ng, Ren},
  date = {2021-06},
  pages = {2845--2854},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.00287},
  abstract = {Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Limiting,Pattern recognition,Shape,Solid modeling,Task analysis,Three-dimensional displays,Trajectory}
}

@inproceedings{tanFitsGloveRapid2016,
  title = {Fits {{Like}} a {{Glove}}: {{Rapid}} and {{Reliable Hand Shape Personalization}}},
  shorttitle = {Fits {{Like}} a {{Glove}}},
  author = {Tan, David Joseph and Cashman, Thomas and Taylor, Jonathan and Fitzgibbon, Andrew and Tarlow, Daniel and Khamis, Sameh and Izadi, Shahram and Shotton, Jamie},
  date = {2016},
  pages = {5610--5619},
  doi = {10.1109/CVPR.2016.605},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Tan_Fits_Like_a_CVPR_2016_paper.html},
  urldate = {2022-04-11},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{tanReducingDataDimensionality1995,
  title = {Reducing Data Dimensionality through Optimizing Neural Network Inputs},
  author = {Tan, Shufeng and Mayrovouniotis, Michael L.},
  date = {1995-06},
  journaltitle = {AIChE Journal},
  shortjournal = {AIChE J.},
  volume = {41},
  number = {6},
  pages = {1471--1480},
  issn = {0001-1541, 1547-5905},
  doi = {10.1002/aic.690410612},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/aic.690410612},
  urldate = {2022-03-08},
  langid = {english}
}

@article{taylorEfficientPreciseInteractive2016,
  ids = {taylorEfficientPreciseInteractive2016a},
  title = {Efficient and Precise Interactive Hand Tracking through Joint, Continuous Optimization of Pose and Correspondences},
  author = {Taylor, Jonathan and Bordeaux, Lucas and Cashman, Thomas and Corish, Bob and Keskin, Cem and Sharp, Toby and Soto, Eduardo and Sweeney, David and Valentin, Julien and Luff, Benjamin and Topalian, Arran and Wood, Erroll and Khamis, Sameh and Kohli, Pushmeet and Izadi, Shahram and Banks, Richard and Fitzgibbon, Andrew and Shotton, Jamie},
  date = {2016-07-11},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {35},
  number = {4},
  pages = {143:1--143:12},
  issn = {0730-0301},
  doi = {10.1145/2897824.2925965},
  url = {https://doi.org/10.1145/2897824.2925965},
  urldate = {2022-04-11},
  abstract = {Fully articulated hand tracking promises to enable fundamentally new interactions with virtual and augmented worlds, but the limited accuracy and efficiency of current systems has prevented widespread adoption. Today's dominant paradigm uses machine learning for initialization and recovery followed by iterative model-fitting optimization to achieve a detailed pose fit. We follow this paradigm, but make several changes to the model-fitting, namely using: (1) a more discriminative objective function; (2) a smooth-surface model that provides gradients for non-linear optimization; and (3) joint optimization over both the model pose and the correspondences between observed data points and the model surface. While each of these changes may actually increase the cost per fitting iteration, we find a compensating decrease in the number of iterations. Further, the wide basin of convergence means that fewer starting points are needed for successful model fitting. Our system runs in real-time on CPU only, which frees up the commonly over-burdened GPU for experience designers. The hand tracker is efficient enough to run on low-power devices such as tablets. We can track up to several meters from the camera to provide a large working volume for interaction, even using the noisy data from current-generation depth cameras. Quantitative assessments on standard datasets show that the new approach exceeds the state of the art in accuracy. Qualitative results take the form of live recordings of a range of interactive experiences enabled by this new approach.},
  keywords = {articulated tracking,subdivision surfaces,virtual reality}
}

@article{tewariStateArtNeural2020,
  title = {State of the {{Art}} on {{Neural Rendering}}},
  author = {Tewari, A. and Fried, O. and Thies, J. and Sitzmann, V. and Lombardi, S. and Sunkavalli, K. and Martin‐Brualla, R. and Simon, T. and Saragih, J. and Nießner, M. and Pandey, R. and Fanello, S. and Wetzstein, G. and Zhu, J.‐Y. and Theobalt, C. and Agrawala, M. and Shechtman, E. and Goldman, D. B and Zollhöfer, M.},
  date = {2020-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {39},
  number = {2},
  pages = {701--727},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.14022},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14022},
  urldate = {2022-02-02},
  langid = {english}
}

@article{tippingSparseBayesianLearning2001,
  title = {Sparse {{Bayesian}} Learning and the Relevance Vector Machine},
  author = {Tipping, Michael E.},
  date = {2001},
  journaltitle = {Journal of machine learning research},
  volume = {1},
  pages = {211--244},
  issue = {Jun},
  keywords = {⛔ No DOI found}
}

@article{tkachSpheremeshesRealtimeHand2016,
  title = {Sphere-Meshes for Real-Time Hand Modeling and Tracking},
  author = {Tkach, Anastasia and Pauly, Mark and Tagliasacchi, Andrea},
  date = {2016-11-11},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {35},
  number = {6},
  pages = {222:1--222:11},
  issn = {0730-0301},
  doi = {10.1145/2980179.2980226},
  url = {https://doi.org/10.1145/2980179.2980226},
  urldate = {2022-04-18},
  abstract = {Modern systems for real-time hand tracking rely on a combination of discriminative and generative approaches to robustly recover hand poses. Generative approaches require the specification of a geometric model. In this paper, we propose a the use of sphere-meshes as a novel geometric representation for real-time generative hand tracking. How tightly this model fits a specific user heavily affects tracking precision. We derive an optimization to non-rigidly deform a template model to fit the user data in a number of poses. This optimization jointly captures the user's static and dynamic hand geometry, thus facilitating high-precision registration. At the same time, the limited number of primitives in the tracking template allows us to retain excellent computational performance. We confirm this by embedding our models in an open source real-time registration algorithm to obtain a tracker steadily running at 60Hz. We demonstrate the effectiveness of our solution by qualitatively and quantitatively evaluating tracking precision on a variety of complex motions. We show that the improved tracking accuracy at high frame-rate enables stable tracking of extended and complex motion sequences without the need for per-frame re-initialization. To enable further research in the area of high-precision hand tracking, we publicly release source code and evaluation datasets.},
  keywords = {hand tracking,non-rigid registration,sphere-meshes}
}

@article{tompsonRealTimeContinuousPose2014,
  title = {Real-{{Time Continuous Pose Recovery}} of {{Human Hands Using Convolutional Networks}}},
  author = {Tompson, Jonathan and Stein, Murphy and Lecun, Yann and Perlin, Ken},
  date = {2014-09-23},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {33},
  number = {5},
  pages = {169:1--169:10},
  issn = {0730-0301},
  doi = {10.1145/2629500},
  url = {https://doi.org/10.1145/2629500},
  urldate = {2022-04-18},
  abstract = {We present a novel method for real-time continuous pose recovery of markerless complex articulable objects from a single depth image. Our method consists of the following stages: a randomized decision forest classifier for image segmentation, a robust method for labeled dataset generation, a convolutional network for dense feature extraction, and finally an inverse kinematics stage for stable real-time pose recovery. As one possible application of this pipeline, we show state-of-the-art results for real-time puppeteering of a skinned hand-model.},
  keywords = {analysis-by-synthesis,Hand tracking,markerless motion capture,neural networks}
}

@article{tongbuasirilaiCompactIntuitiveDatadriven2020,
  title = {Compact and Intuitive Data-Driven {{BRDF}} Models},
  author = {Tongbuasirilai, Tanaboon and Unger, Jonas and Kronander, Joel and Kurt, Murat},
  date = {2020-04-01},
  journaltitle = {The Visual Computer},
  shortjournal = {Vis Comput},
  volume = {36},
  number = {4},
  pages = {855--872},
  issn = {1432-2315},
  doi = {10.1007/s00371-019-01664-z},
  url = {https://doi.org/10.1007/s00371-019-01664-z},
  urldate = {2022-09-03},
  abstract = {Measured materials are rapidly becoming a core component in the photo-realistic image synthesis pipeline. The reason is that data-driven models can easily capture the underlying, fine details that represent the visual appearance of materials, which can be difficult or even impossible to model by hand. There are, however, a number of key challenges that need to be solved in order to enable efficient capture, representation and interaction with real materials. This paper presents two new data-driven BRDF models specifically designed for 1D separability. The proposed 3D and 2D BRDF representations can be factored into three or two 1D factors, respectively, while accurately representing the underlying BRDF data with only small approximation error. We evaluate the models using different parameterizations with different characteristics and show that both the BRDF data itself and the resulting renderings yield more accurate results in terms of both numerical errors and visual results compared to previous approaches. To demonstrate the benefit of the proposed factored models, we present a new Monte Carlo importance sampling scheme and give examples of how they can be used for efficient BRDF capture and intuitive editing of measured materials.},
  langid = {english},
  keywords = {Computer graphics,Reflectance modeling,Rendering}
}

@article{tzionasCapturingHandsAction2016,
  title = {Capturing {{Hands}} in {{Action Using Discriminative Salient Points}} and {{Physics Simulation}}},
  author = {Tzionas, Dimitrios and Ballan, Luca and Srikantha, Abhilash and Aponte, Pablo and Pollefeys, Marc and Gall, Juergen},
  date = {2016-06-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {118},
  number = {2},
  pages = {172--193},
  issn = {1573-1405},
  doi = {10.1007/s11263-016-0895-4},
  url = {https://doi.org/10.1007/s11263-016-0895-4},
  urldate = {2022-04-18},
  abstract = {Hand motion capture is a popular research field, recently gaining more attention due to the ubiquity of RGB-D sensors. However, even most recent approaches focus on the case of a single isolated hand. In this work, we focus on hands that interact with other hands or objects and present a framework that successfully captures motion in such interaction scenarios for both rigid and articulated objects. Our framework combines a generative model with discriminatively trained salient points to achieve a low tracking error and with collision detection and physics simulation to achieve physically plausible estimates even in case of occlusions and missing visual data. Since all components are unified in a single objective function which is almost everywhere differentiable, it can be optimized with standard optimization techniques. Our approach works for monocular RGB-D sequences as well as setups with multiple synchronized RGB cameras. For a qualitative and quantitative evaluation, we captured 29 sequences with a large variety of interactions and up to 150 degrees of freedom.},
  langid = {english},
  keywords = {Fingertip detection,Hand motion capture,Hand–object interaction,Physics simulation}
}

@unpublished{uriaDeepTractableDensity2014,
  title = {A {{Deep}} and {{Tractable Density Estimator}}},
  author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  date = {2014-01-11},
  eprint = {1310.1757},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1310.1757},
  url = {http://arxiv.org/abs/1310.1757},
  urldate = {2022-05-08},
  abstract = {The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{uriaRNADERealvaluedNeural2014,
  title = {{{RNADE}}: {{The}} Real-Valued Neural Autoregressive Density-Estimator},
  shorttitle = {{{RNADE}}},
  author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  date = {2014-01-09},
  eprint = {1306.0186},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1306.0186},
  url = {http://arxiv.org/abs/1306.0186},
  urldate = {2022-05-08},
  abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{vapnikEstimationDependencesBased2006,
  title = {Estimation of Dependences Based on Empirical Data},
  author = {Vapnik, Vladimir},
  date = {2006},
  publisher = {{Springer Science \& Business Media}}
}

@inproceedings{veachMetropolisLightTransport1997,
  title = {Metropolis Light Transport},
  booktitle = {Proceedings of the 24th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Veach, Eric and Guibas, Leonidas J.},
  date = {1997-08-03},
  series = {{{SIGGRAPH}} '97},
  pages = {65--76},
  publisher = {{ACM Press/Addison-Wesley Publishing Co.}},
  location = {{USA}},
  doi = {10.1145/258734.258775},
  url = {https://doi.org/10.1145/258734.258775},
  urldate = {2022-06-27},
  isbn = {978-0-89791-896-1},
  keywords = {global illumination,lighting simulation,Markov Chain Monte Carlo methods,Metropolis-Hastings algorithm,Monte Carlo integration,physically-based rendering,radiative heat transfer,variance reduction}
}

@thesis{veachRobustMonteCarlo1998,
  type = {phdthesis},
  title = {Robust Monte Carlo Methods for Light Transport Simulation},
  author = {Veach, Eric},
  date = {1998},
  institution = {{Stanford University}},
  location = {{Stanford, CA, USA}},
  abstract = {Light transport algorithms generate realistic images by simulating the emission and scattering of light in an artificial environment. Applications include lighting design, architecture, and computer animation, while related engineering disciplines include neutron transport and radiative heat transfer. The main challenge with these algorithms is the high complexity of the geometric, scattering, and illumination models used. In this dissertation, we develop new Monte Carlo techniques that greatly extend the range of input models for which light transport simulations are practical. We start by developing a rigorous theoretical basis for bidirectional light transport algorithms (those that combine direct and adjoint techniques). We propose a new formulation based on linear operators, such that for any physically valid input scene, the transport operators are symmetric. We also show how light transport can be formulated as an integral over a space of paths. This framework allows new sampling and integration techniques to be applied, such as the Metropolis sampling method. Our statistical contributions include multiple importance sampling, a new variance reduction technique that can greatly increase the robustness of Monte Carlo integration. It uses more than one sampling technique to evaluate an integral, and combines these samples in a way that is provably close to optimal. Finally, we propose new Monte Carlo light transport algorithms. Bidirectional path tracing uses a family of different path sampling techniques that generate some path vertices starting from a light source, and some starting from a sensor. We show that when these techniques are combined using multiple importance sampling, a large range of difficult lighting effects can be handled efficiently. The second algorithm we describe is Metropolis light transport, inspired by the Metropolis sampling method from computational physics. Paths are generated by following a random walk through path space, such that the probability density of visiting each path is proportional the contribution it makes to the ideal image. The resulting algorithm is unbiased, handles arbitrary geometry and materials, and can be orders of magnitude more efficient than previous unbiased approaches for difficult lighting problems. To our knowledge, this is the first application of the Metropolis method to transport problems of any kind.},
  pagetotal = {406},
  keywords = {⭐⭐⭐},
  annotation = {AAI9837162 ISBN-10: 0591907801}
}

@article{vevodapetrBayesianOnlineRegression2018,
  title = {Bayesian Online Regression for Adaptive Direct Illumination Sampling},
  author = {VévodaPetr and KondapaneniIvo and KřivánekJaroslav},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics (TOG)},
  publisher = {{ACM}},
  doi = {10.1145/3197517.3201340},
  url = {https://dl.acm.org/doi/abs/10.1145/3197517.3201340},
  urldate = {2022-06-27},
  abstract = {Direct illumination calculation is an important component of any physically-based Tenderer with a substantial impact on the overall performance. We present a novel adaptive solution for unbiased Monte Carlo direct illumination sampling, based on online ...},
  langid = {english},
  annotation = {PUB27 		New York, NY, USA}
}

@article{viciniPathReplayBackpropagation2021,
  title = {Path Replay Backpropagation: Differentiating Light Paths Using Constant Memory and Linear Time},
  shorttitle = {Path Replay Backpropagation},
  author = {Vicini, Delio and Speierer, Sébastien and Jakob, Wenzel},
  date = {2021-07-19},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {40},
  number = {4},
  pages = {108:1--108:14},
  issn = {0730-0301},
  doi = {10.1145/3450626.3459804},
  url = {https://doi.org/10.1145/3450626.3459804},
  urldate = {2022-10-06},
  abstract = {Differentiable physically-based rendering has become an indispensable tool for solving inverse problems involving light. Most applications in this area jointly optimize a large set of scene parameters to minimize an objective function, in which case reverse-mode differentiation is the method of choice for obtaining parameter gradients. However, existing techniques that perform the necessary differentiation step suffer from either statistical bias or a prohibitive cost in terms of memory and computation time. For example, standard techniques for automatic differentiation based on program transformation or Wengert tapes lead to impracticably large memory usage when applied to physically-based rendering algorithms. A recently proposed adjoint method by Nimier-David et al. [2020] reduces this to a constant memory footprint, but the computation time for unbiased gradient estimates then becomes quadratic in the number of scattering events along a light path. This is problematic when the scene contains highly scattering materials like participating media. In this paper, we propose a new unbiased backpropagation algorithm for rendering that only requires constant memory, and whose computation time is linear in the number of scattering events (i.e., just like path tracing). Our approach builds on the invertibility of the local Jacobian at scattering interactions to recover the various quantities needed for reverse-mode differentiation. Our method also extends to specular materials such as smooth dielectrics and conductors that cannot be handled by prior work.},
  keywords = {differentiable rendering,gradient-based optimization,inverse rendering,radiative backpropagation}
}

@article{vorbaOnlineLearningParametric2014a,
  title = {On-Line Learning of Parametric Mixture Models for Light Transport Simulation},
  author = {Vorba, Jiří and Karlík, Ondřej and Šik, Martin and Ritschel, Tobias and Křivánek, Jaroslav},
  date = {2014-07-27},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {33},
  number = {4},
  pages = {101:1--101:11},
  issn = {0730-0301},
  doi = {10.1145/2601097.2601203},
  url = {https://doi.org/10.1145/2601097.2601203},
  urldate = {2022-10-06},
  abstract = {Monte Carlo techniques for light transport simulation rely on importance sampling when constructing light transport paths. Previous work has shown that suitable sampling distributions can be recovered from particles distributed in the scene prior to rendering. We propose to represent the distributions by a parametric mixture model trained in an on-line (i.e. progressive) manner from a potentially infinite stream of particles. This enables recovering good sampling distributions in scenes with complex lighting, where the necessary number of particles may exceed available memory. Using these distributions for sampling scattering directions and light emission significantly improves the performance of state-of-the-art light transport simulation algorithms when dealing with complex lighting.},
  keywords = {importance sampling,light transport simulation,on-line expectation maximization,parametric density estimation}
}

@inproceedings{vorbaPathGuidingProduction2019,
  title = {Path Guiding in Production},
  booktitle = {{{ACM SIGGRAPH}} 2019 {{Courses}}},
  author = {Vorba, Jiří and Hanika, Johannes and Herholz, Sebastian and Müller, Thomas and Křivánek, Jaroslav and Keller, Alexander},
  date = {2019-07-28},
  series = {{{SIGGRAPH}} '19},
  pages = {1--77},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3305366.3328091},
  url = {https://doi.org/10.1145/3305366.3328091},
  urldate = {2022-08-14},
  abstract = {Path guiding is a family of adaptive variance reduction techniques in physically-based rendering, which includes methods for sampling both direct and indirect illumination, surfaces and volumes but also for sampling optimal path lengths and making splitting decisions. Since adoption of path tracing as a de facto standard in the VFX industry several years ago, there has been an increased interest in producing high-quality images with low amount of Monte Carlo samples per pixel. Path guiding, which has received attention in the research community in the past few years, has proven to be useful for this task and therefore has been adopted by Weta Digital. Recently, it has also been implemented in the Walt Disney Animation Studios' Hyperion and Pixar's Renderman. The goal of this course is to share our practical experience with path guiding in production and to provide self-contained overview of recently published techniques and to discuss their pros and cons. We also take audience through theoretical background of various path guiding methods which are mostly based on machine learning - used to adapt sampling distributons based on observed samples - and zero-variance random walk theory - used as a framework for combining different sampling decisions in an optimal way. At the end of our course we discuss open problems and invite researchers to further develop path guiding in their future work.},
  isbn = {978-1-4503-6307-5},
  keywords = {.unread}
}

@inproceedings{wang6DHandsMarkerless2011,
  title = {{{6D}} Hands: Markerless Hand-Tracking for Computer Aided Design},
  shorttitle = {{{6D}} Hands},
  booktitle = {Proceedings of the 24th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Wang, Robert and Paris, Sylvain and Popović, Jovan},
  date = {2011-10-16},
  series = {{{UIST}} '11},
  pages = {549--558},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2047196.2047269},
  url = {https://doi.org/10.1145/2047196.2047269},
  urldate = {2022-04-18},
  abstract = {Computer Aided Design (CAD) typically involves tasks such as adjusting the camera perspective and assembling pieces in free space that require specifying 6 degrees of freedom (DOF). The standard approach is to factor these DOFs into 2D subspaces that are mapped to the x and y axes of a mouse. This metaphor is inherently modal because one needs to switch between subspaces, and disconnects the input space from the modeling space. In this paper, we propose a bimanual hand tracking system that provides physically-motivated 6-DOF control for 3D assembly. First, we discuss a set of principles that guide the design of our precise, easy-to-use, and comfortable-to-use system. Based on these guidelines, we describe a 3D input metaphor that supports constraint specification classically used in CAD software, is based on only a few simple gestures, lets users rest their elbows on their desk, and works alongside the keyboard and mouse. Our approach uses two consumer-grade webcams to observe the user's hands. We solve the pose estimation problem with efficient queries of a precomputed database that relates hand silhouettes to their 3D configuration. We demonstrate efficient 3D mechanical assembly of several CAD models using our hand-tracking system.},
  isbn = {978-1-4503-0716-1},
  keywords = {3D object manipulation,computer aided design,hand tracking}
}

@article{wangGeneralizingFewExamples2021,
  ids = {wangyaqingGeneralizingFewExamples2020},
  title = {Generalizing from a {{Few Examples}}: {{A Survey}} on {{Few-shot Learning}}},
  shorttitle = {Generalizing from a {{Few Examples}}},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  date = {2021-05-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  number = {3},
  pages = {1--34},
  publisher = {{ACM}},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3386252},
  url = {https://dl.acm.org/doi/10.1145/3386252},
  urldate = {2022-06-28},
  abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.               1},
  langid = {english}
}

@inproceedings{wangMetaAvatarLearningAnimatable2021,
  title = {{{MetaAvatar}}: {{Learning Animatable Clothed Human Models}} from {{Few Depth Images}}},
  shorttitle = {{{MetaAvatar}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Shaofei and Mihajlovic, Marko and Ma, Qianli and Geiger, Andreas and Tang, Siyu},
  date = {2021},
  volume = {34},
  pages = {2810--2822},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/1680829293f2a8541efa2647a0290f88-Abstract.html},
  urldate = {2022-09-04},
  abstract = {In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.},
  keywords = {⛔ No DOI found}
}

@article{wangNeuSLearningNeural2021,
  title = {{{NeuS}}: {{Learning Neural Implicit Surfaces}} by {{Volume Rendering}} for {{Multi-view Reconstruction}}},
  shorttitle = {{{NeuS}}},
  author = {Wang, Peng and Liu, Lingjie and Liu, Yuan and Theobalt, Christian and Komura, Taku and Wang, Wenping},
  date = {2021-06-20},
  doi = {10.48550/arXiv.2106.10689},
  url = {https://arxiv.org/abs/2106.10689v2},
  urldate = {2022-06-13},
  abstract = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
  langid = {english}
}

@article{wangRealtimeHandtrackingColor2009,
  title = {Real-Time Hand-Tracking with a Color Glove},
  author = {Wang, Robert Y. and Popović, Jovan},
  date = {2009-07-27},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {28},
  number = {3},
  pages = {63:1--63:8},
  issn = {0730-0301},
  doi = {10.1145/1531326.1531369},
  url = {https://doi.org/10.1145/1531326.1531369},
  urldate = {2022-04-10},
  abstract = {Articulated hand-tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity. In this paper, we propose an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands. Our approach uses a single camera to track a hand wearing an ordinary cloth glove that is imprinted with a custom pattern. The pattern is designed to simplify the pose estimation problem, allowing us to employ a nearest-neighbor approach to track hands at interactive rates. We describe several proof-of-concept applications enabled by our system that we hope will provide a foundation for new interactions in modeling, animation control and augmented reality.},
  keywords = {augmented reality,hand tracking,motion capture,user interface}
}

@misc{wangRiemannianNormalizingFlow2019,
  title = {Riemannian {{Normalizing Flow}} on {{Variational Wasserstein Autoencoder}} for {{Text Modeling}}},
  author = {Wang, Prince Zizhuang and Wang, William Yang},
  date = {2019-04-22},
  number = {arXiv:1904.02399},
  eprint = {1904.02399},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1904.02399},
  urldate = {2022-05-29},
  abstract = {Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem, also known as the Kullback-Leibler (KL) term vanishing issue, where the posterior easily collapses to the prior, and the model will ignore latent codes in generative tasks. To address this problem, we introduce an improved Wasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling. The RNF transforms a latent variable into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the non-informative prior. The Wasserstein objective minimizes the distance between the marginal distribution and the prior directly and therefore does not force the posterior to match the prior. Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performances in tasks such as language modeling, likelihood approximation, and text generation. Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{wardMeasuringModelingAnisotropic1992,
  title = {Measuring and Modeling Anisotropic Reflection},
  author = {Ward, Gregory J.},
  date = {1992-07},
  journaltitle = {ACM SIGGRAPH Computer Graphics},
  shortjournal = {SIGGRAPH Comput. Graph.},
  volume = {26},
  number = {2},
  pages = {265--272},
  issn = {0097-8930},
  doi = {10.1145/142920.134078},
  url = {https://dl.acm.org/doi/10.1145/142920.134078},
  urldate = {2022-09-01},
  langid = {english}
}

@inproceedings{weinmannMaterialClassificationBased2014,
  title = {Material {{Classification Based}} on {{Training Data Synthesized Using}} a {{BTF Database}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Weinmann, Michael and Gall, Juergen and Klein, Reinhard},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {156--171},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-10578-9_11},
  abstract = {To cope with the richness in appearance variation found in real-world data under natural illumination, we propose to synthesize training data capturing these variations for material classification. Using synthetic training data created from separately acquired material and illumination characteristics allows to overcome the problems of existing material databases which only include a tiny fraction of the possible real-world conditions under controlled laboratory environments. However, it is essential to utilize a representation for material appearance which preserves fine details in the reflectance behavior of the digitized materials. As BRDFs are not sufficient for many materials due to the lack of modeling mesoscopic effects, we present a high-quality BTF database with 22,801 densely measured view-light configurations including surface geometry measurements for each of the 84 measured material samples. This representation is used to generate a database of synthesized images depicting the materials under different view-light conditions with their characteristic surface geometry using image-based lighting to simulate the complexity of real-world scenarios. We demonstrate that our synthesized data allows classifying materials under complex real-world scenarios.},
  isbn = {978-3-319-10578-9},
  langid = {english},
  keywords = {Material classification,material database,reflectance,texture synthesis}
}

@inproceedings{wellingBayesianLearningStochastic2011,
  title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Welling, Max and Teh, Yee Whye},
  date = {2011-06-28},
  series = {{{ICML}}'11},
  pages = {681--688},
  publisher = {{Omnipress}},
  location = {{Madison, WI, USA}},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  isbn = {978-1-4503-0619-5}
}

@article{whiteReflectometerMeasuringBidirectional1998,
  title = {Reflectometer for Measuring the Bidirectional Reflectance of Rough Surfaces},
  author = {White, D. R. and Saunders, P. and Bonsey, S. J. and van de Ven, J. and Edgar, H.},
  options = {useprefix=true},
  date = {1998-06-01},
  journaltitle = {Applied Optics},
  shortjournal = {Appl Opt},
  volume = {37},
  number = {16},
  eprint = {18273308},
  eprinttype = {pmid},
  pages = {3450--3454},
  issn = {1559-128X},
  doi = {10.1364/ao.37.003450},
  abstract = {A fully automatic, four-axis gonioreflectometer is described. It has an angular accuracy of 0.3 degrees and a range of 90 degrees in both the theta(i) and the theta(r) zenith angles. The gonioreflectometer is simpler than previous designs because of its use of rotating arms rather than moving carriages to mount the optical components. Where possible, commercial components have been used to reduce the cost. A novel off-axis angular encoding scheme is also described.},
  langid = {english}
}

@article{wonHomuncularFlexibilityVirtual2015,
  title = {Homuncular {{Flexibility}} in {{Virtual Reality}}},
  author = {Won, Andrea Stevenson and Bailenson, Jeremy and Lee, Jimmy and Lanier, Jaron},
  date = {2015-05-01},
  journaltitle = {Journal of Computer-Mediated Communication},
  shortjournal = {Journal of Computer-Mediated Communication},
  volume = {20},
  number = {3},
  pages = {241--259},
  issn = {1083-6101},
  doi = {10.1111/jcc4.12107},
  url = {https://doi.org/10.1111/jcc4.12107},
  urldate = {2022-04-12},
  abstract = {Immersive virtual reality allows people to inhabit avatar bodies that differ from their own, and this can produce significant psychological and physiological effects. The concept of homuncular flexibility (Lanier, 2006) proposes that users can learn to control bodies different from their own by changing the relationship between tracked and rendered motion. We examine the effects of remapping movements in the real world onto an avatar that moves in novel ways. In Experiment 1, participants moved their legs more than their arms in conditions where leg movements were more effective for the task. In Experiment 2, participants controlling 3-armed avatars learned to hit more targets than participants in 2-armed avatars. We discuss the implications of embodiment in novel bodies.}
}

@article{wrightRobustFaceRecognition2009,
  title = {Robust {{Face Recognition}} via {{Sparse Representation}}},
  author = {Wright, J. and Yang, A.Y. and Ganesh, A. and Sastry, S.S. and {Yi Ma}},
  date = {2009-02},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {31},
  number = {2},
  pages = {210--227},
  issn = {0162-8828},
  doi = {10.1109/tpami.2008.79},
  url = {http://ieeexplore.ieee.org/document/4483511/},
  urldate = {2022-03-06}
}

@article{wuDeepIncrementalLearning2018,
  title = {Deep Incremental Learning for Efficient High-Fidelity Face Tracking},
  author = {Wu, Chenglei and Shiratori, Takaaki and Sheikh, Yaser},
  date = {2018-12-31},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {6},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3272127.3275101},
  url = {https://dl.acm.org/doi/10.1145/3272127.3275101},
  urldate = {2022-02-25},
  abstract = {In this paper, we present an incremental learning framework for efficient and accurate facial performance tracking. Our approach is to alternate the modeling step, which takes tracked meshes and texture maps to train our deep learning-based statistical model, and the tracking step, which takes predictions of geometry and texture our model infers from measured images and optimize the predicted geometry by minimizing image, geometry and facial landmark errors. Our               Geo-Tex VAE               model extends the convolutional variational autoencoder for face tracking, and jointly learns and represents deformations and variations in geometry and texture from tracked meshes and texture maps. To accurately model variations in facial geometry and texture, we introduce the               decomposition               layer in the Geo-Tex VAE architecture which decomposes the facial deformation into global and local components. We train the global deformation with a fully-connected network and the local deformations with convolutional layers. Despite running this model on each frame independently - thereby enabling a high amount of parallelization - we validate that our framework achieves sub-millimeter accuracy on synthetic data and outperforms existing methods. We also qualitatively demonstrate high-fidelity, long-duration facial performance tracking on several actors.},
  langid = {english}
}

@article{xuDeepImagebasedRelighting2018,
  title = {Deep Image-Based Relighting from Optimal Sparse Samples},
  author = {Xu, Zexiang and Sunkavalli, Kalyan and Hadap, Sunil and Ramamoorthi, Ravi},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {4},
  pages = {126:1--126:13},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201313},
  url = {https://doi.org/10.1145/3197517.3201313},
  urldate = {2022-09-04},
  abstract = {We present an image-based relighting method that can synthesize scene appearance under novel, distant illumination from the visible hemisphere, from only five images captured under pre-defined directional lights. Our method uses a deep convolutional neural network to regress the relit image from these five images; this relighting network is trained on a large synthetic dataset comprised of procedurally generated shapes with real-world reflectances. We show that by combining a custom-designed sampling network with the relighting network, we can jointly learn both the optimal input light directions and the relighting function. We present an extensive evaluation of our network, including an empirical analysis of reconstruction quality, optimal lighting configurations for different scenarios, and alternative network architectures. We demonstrate, on both synthetic and real scenes, that our method is able to reproduce complex, high-frequency lighting effects like specularities and cast shadows, and outperforms other image-based relighting methods that require an order of magnitude more images.},
  keywords = {appearance capture,convolutional neural network,illumination,image-based relighting,sparse sampling}
}

@inproceedings{xuEfficientHandPose2013,
  title = {Efficient {{Hand Pose Estimation}} from a {{Single Depth Image}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Xu, Chi and Cheng, Li},
  date = {2013-12},
  pages = {3456--3462},
  publisher = {{IEEE}},
  location = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.429},
  url = {http://ieeexplore.ieee.org/document/6751541/},
  urldate = {2022-04-19},
  eventtitle = {2013 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4799-2840-8}
}

@article{yanEfficientPracticalFar2017,
  title = {An Efficient and Practical near and Far Field Fur Reflectance Model},
  author = {Yan, Ling-Qi and Jensen, Henrik Wann and Ramamoorthi, Ravi},
  date = {2017-07-20},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {36},
  number = {4},
  pages = {67:1--67:13},
  issn = {0730-0301},
  doi = {10.1145/3072959.3073600},
  url = {https://doi.org/10.1145/3072959.3073600},
  urldate = {2022-08-19},
  abstract = {Physically-based fur rendering is difficult. Recently, structural differences between hair and fur fibers have been revealed by Yan et al. (2015), who showed that fur fibers have an inner scattering medulla, and developed a double cylinder model. However, fur rendering is still complicated due to the complex scattering paths through the medulla. We develop a number of optimizations that improve efficiency and generality without compromising accuracy, leading to a practical fur reflectance model. We also propose a key contribution to support both near and far-field rendering, and allow smooth transitions between them. Specifically, we derive a compact BCSDF model for fur reflectance with only 5 lobes. Our model unifies hair and fur rendering, making it easy to implement within standard hair rendering software, since we keep the traditional R, TT, and TRT lobes in hair, and only add two extensions to scattered lobes, TTs and TRTs. Moreover, we introduce a compression scheme using tensor decomposition to dramatically reduce the precomputed data storage for scattered lobes to only 150 KB, with minimal loss of accuracy. By exploiting piecewise analytic integration, our method further enables a multi-scale rendering scheme that transitions between near and far field rendering smoothly and efficiently for the first time, leading to 6 -- 8× speed up over previous work.},
  keywords = {fur,multi-scale,reflectance}
}

@misc{yangDiffusionModelsComprehensive2022,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  date = {2022-10-03},
  number = {arXiv:2209.00796},
  eprint = {2209.00796},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.00796},
  url = {http://arxiv.org/abs/2209.00796},
  urldate = {2022-10-06},
  abstract = {Diffusion models are a class of deep generative models that have shown impressive results on various tasks with a solid theoretical foundation. Despite demonstrated success than state-of-the-art approaches, diffusion models often entail costly sampling procedures and sub-optimal likelihood estimation. Significant efforts have been made to improve the performance of diffusion models in various aspects. In this article, we present a comprehensive review of existing variants of diffusion models. Specifically, we provide the taxonomy of research in diffusion models and categorize them into three types: sampling-efficiency enhancement, likelihood-maximization enhancement, and data-generalization enhancement. We also introduce the other generative models (i.e., variational autoencoders, generative adversarial networks, normalizing flow, autoregressive models, and energy-based models) and discuss the connections between diffusion models and these generative models. Then we review the applications of diffusion models, including computer vision, natural language processing, temporal data modeling, multi-modal learning, robust learning, molecular graph modeling, material design, and inverse problem solving. Furthermore, we propose new perspectives pertaining to the development of generative models. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@thesis{yanPhysicallybasedModelingRendering2018,
  title = {Physically-Based {{Modeling}} and {{Rendering}} of {{Complex Visual Appearance}}},
  author = {Yan, Lingqi},
  date = {2018},
  institution = {{UC Berkeley}},
  url = {https://escholarship.org/uc/item/6bg5143b},
  urldate = {2022-08-19},
  abstract = {In this dissertation, we focus on physically-based rendering that synthesizes realistic images from 3D models and scenes. State of the art rendering still struggles with two fundamental challenges --- realism and speed. The rendered results look artificial and overly perfect, and the rendering process is slow for both offline and interactive applications. Moreover, better realism and faster speed are inherently contradictory, because the computational complexity increases substantially when trying to render higher fidelity detailed results. We put emphasis on both ends of the realism-speed spectrum in rendering by introducing the concept of detailed rendering and appearance modeling to accurately represent and reproduce the rich visual world from micron level to overall appearance, and combining sparse ray sampling with fast high dimensional filtering to achieve real-time performance.To make rendering more realistic, our first claim is that, we need details. However, rendering a complex surface with lots of details is far from easy. Traditionally, the surface microstructure is approximated using a smooth normal distribution, but this ignores details such as glinty effects, easily observable in the real world. While modeling the actual surface microstructure is possible, the resulting rendering problem is prohibitively expensive using Monte Carlo point sampling: the energy is concentrated in tiny highlights that take up a minuscule fraction of the pixel. We instead compute the accurate solution that Monte Carlo would eventually converge to, using a completely different deterministic approach (Chapter 3). Our method considers the highly complicated distribution of normals on a surface patch seen through a single pixel. We show different methods to evaluate this efficiently with closed-form solutions, assuming a surface patch is made up of either 2D planar triangles or 4D Gaussian elements, respectively. We also show how to extend our method to accurately handle wave optics. Our results show complicated, temporally varying glints from materials such as bumpy plastics, brushed and scratched metals, metallic paint and ocean waves.In the above, although rendering details imposes many challenges, we assumed we know how the surface reflects light. However, there are a lot of natural materials in the real world where we are not sure exactly how they interact with the light. To render these materials realistically, we need accurate appearance/reflectance models derived from microstructures to define their optical behavior. We demonstrate this by introducing a reflectance model for animal fur in Chapter 4. Rendering photo-realistic animal fur is a long-standing problem in computer graphics. Considerable effort has been made on modeling the geometric complexity of human hair, but the appearance/reflectance of fur fibers is not well understood. Based on anatomical literature and measurements, we develop a double cylinder model for the reflectance of a single fur fiber, where an outer cylinder represents the biological observation of a cortex covered by multiple cuticle layers, and an inner cylinder represents the scattering interior structure known as the medulla, often absent from human hair fibers. We validate our physical model with measurements on real fur fibers, and introduce the first database in computer graphics of reflectance profiles for nine fur samples. For efficient rendering, we develop a method to precompute 2D medulla scattering profiles and analytically approximate our reflectance model with factored lobes. We then develop a number of optimizations that improve efficiency and generality without compromising accuracy. And we present the first global illumination model, based on dipole diffusion for subsurface scattering, to approximate light bouncing between individual fur fibers by modeling complex light and fur interactions as subsurface scattering, and using a simple neural network to convert from fur fibers' properties to scattering parameters.However, even without these details to improve rendered realism, current rendering still suffers from low performance with state of the art Monte Carlo ray tracing. Physically correct, noise-free images can require hundreds or thousands of ray samples per pixel, and take a long time to compute. Recent approaches have exploited sparse sampling and filtering; the filtering is either fast (axis-aligned), but requires more input samples, or needs fewer input samples but is very slow (sheared). We present a new approach for fast sheared filtering on the GPU in Chapter 5. Our algorithm factors the 4D sheared filter into four 1D filters. We derive complexity bounds for our method, showing that the per-pixel complexity is reduced from O(n\^2 l\^2)\$ to O(nl), where n is the linear filter width (filter size is O(n\^2)) and l is the (usually very small) number of samples for each dimension of the light or lens per pixel (spp is l\^2). We thus reduce sheared filtering overhead dramatically. We demonstrate rendering of depth of field, soft shadows and diffuse global illumination at interactive speeds.},
  langid = {english},
  keywords = {.skimmed}
}

@article{yanRenderingGlintsHighresolution2014,
  title = {Rendering Glints on High-Resolution Normal-Mapped Specular Surfaces},
  author = {Yan, Ling-Qi and Hašan, Miloš and Jakob, Wenzel and Lawrence, Jason and Marschner, Steve and Ramamoorthi, Ravi},
  date = {2014-07-27},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {33},
  number = {4},
  pages = {116:1--116:9},
  issn = {0730-0301},
  doi = {10.1145/2601097.2601155},
  url = {https://doi.org/10.1145/2601097.2601155},
  urldate = {2022-10-31},
  abstract = {Complex specular surfaces under sharp point lighting show a fascinating glinty appearance, but rendering it is an unsolved problem. Using Monte Carlo pixel sampling for this purpose is impractical: the energy is concentrated in tiny highlights that take up a minuscule fraction of the pixel. We instead compute an accurate solution using a completely different deterministic approach. Our method considers the true distribution of normals on a surface patch seen through a single pixel, which can be highly complex. We show how to evaluate this distribution efficiently, assuming a Gaussian pixel footprint and Gaussian intrinsic roughness. We also take advantage of hierarchical pruning of position-normal space to rapidly find texels that might contribute to a given normal distribution evaluation. Our results show complex, temporally varying glints from materials such as bumpy plastics, brushed and scratched metals, metallic paint and ocean waves.},
  keywords = {.skimmed,glints,high-resolution normal maps,normal distribution functions,specular highlights}
}

@article{yanSurveyBlueNoiseSampling2015,
  title = {A {{Survey}} of {{Blue-Noise Sampling}} and {{Its Applications}}},
  author = {Yan, Dong-Ming and Guo, Jian-Wei and Wang, Bin and Zhang, Xiao-Peng and Wonka, Peter},
  date = {2015-05-01},
  journaltitle = {Journal of Computer Science and Technology},
  shortjournal = {J. Comput. Sci. Technol.},
  volume = {30},
  number = {3},
  pages = {439--452},
  issn = {1860-4749},
  doi = {10.1007/s11390-015-1535-0},
  url = {https://doi.org/10.1007/s11390-015-1535-0},
  urldate = {2022-08-14},
  abstract = {In this paper, we survey recent approaches to blue-noise sampling and discuss their beneficial applications. We discuss the sampling algorithms that use points as sampling primitives and classify the sampling algorithms based on various aspects, e.g., the sampling domain and the type of algorithm. We demonstrate several well-known applications that can be improved by recent blue-noise sampling techniques, as well as some new applications such as dynamic sampling and blue-noise remeshing.},
  langid = {english},
  keywords = {blue-noise sampling,Lloyd relaxation,Poisson-disk sampling,remeshing,rendering}
}

@inproceedings{yuanDepthBased3DHand2018,
  title = {Depth-{{Based 3D Hand Pose Estimation}}: {{From Current Achievements}} to {{Future Goals}}},
  shorttitle = {Depth-{{Based 3D Hand Pose Estimation}}},
  author = {Yuan, Shanxin and Garcia-Hernando, Guillermo and Stenger, Björn and Moon, Gyeongsik and Chang, Ju Yong and Lee, Kyoung Mu and Molchanov, Pavlo and Kautz, Jan and Honari, Sina and Ge, Liuhao and Yuan, Junsong and Chen, Xinghao and Wang, Guijin and Yang, Fan and Akiyama, Kai and Wu, Yang and Wan, Qingfu and Madadi, Meysam and Escalera, Sergio and Li, Shile and Lee, Dongheui and Oikonomidis, Iason and Argyros, Antonis and Kim, Tae-Kyun},
  date = {2018},
  pages = {2636--2645},
  doi = {10.1109/cvpr.2018.00279},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.html},
  urldate = {2022-04-12},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{yuanRubberHandIllusion2010,
  title = {Is the Rubber Hand Illusion Induced by Immersive Virtual Reality?},
  booktitle = {2010 {{IEEE Virtual Reality Conference}} ({{VR}})},
  author = {Yuan, Ye and Steed, Anthony},
  date = {2010-03},
  pages = {95--102},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/vr.2010.5444807},
  url = {http://ieeexplore.ieee.org/document/5444807/},
  urldate = {2022-04-12},
  eventtitle = {2010 {{IEEE Virtual Reality Conference}} ({{VR}})},
  isbn = {978-1-4244-6237-7}
}

@article{yuSparseSamplingImageBased2016,
  title = {Sparse {{Sampling}} for {{Image-Based SVBRDF Acquisition}}},
  author = {Yu, Jiyang and Xu, Zexiang and Mannino, Matteo and Jensen, Henrik Wann and Ramamoorthi, Ravi},
  date = {2016},
  journaltitle = {Workshop on Material Appearance Modeling},
  pages = {4 pages},
  publisher = {{The Eurographics Association}},
  issn = {2309-5059},
  doi = {10.2312/MAM.20161251},
  url = {https://diglib.eg.org/handle/10.2312/mam20161251},
  urldate = {2022-09-04},
  abstract = {We acquire the data-driven spatially-varying (SV)BRDF of a flat sample from only a small number of images (typically 20). We generalize the homogenous BRDF acquisition work of Nielsen et al., who derived an optimal minmal set of lighting/view directions, treating a 4 degree-of-freedom spherical gantry as a gonioreflectometer. In contrast, we benefit from using the full 2D camera image from the gantry to enable SVBRDF acquisition. Like Nielsen et al, our method is data-driven, based on the MERL database of isotropic BRDFs, and finds the optimal directions by minimizing the condition number of the acquisition matrix. We extend their approach to SVBRDFs by modifying the optimal incident/outgoing directions to avoid grazing angles that reduce resolution and make alignment of different views difficult. Another key practical issue is aligning multiple viewpoints, and correcting for near-field effects. We demonstrate our method on SVBRDF measurements of new flat materials, showing that full data-driven SVBRDF acquisition is now possible from a sparse set of only about 20 light-view pairs.},
  isbn = {9783038680079}
}

@article{zhangDifferentialTheoryRadiative2019,
  title = {A Differential Theory of Radiative Transfer},
  author = {Zhang, Cheng and Wu, Lifan and Zheng, Changxi and Gkioulekas, Ioannis and Ramamoorthi, Ravi and Zhao, Shuang},
  date = {2019-11-08},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {38},
  number = {6},
  pages = {227:1--227:16},
  issn = {0730-0301},
  doi = {10.1145/3355089.3356522},
  url = {https://doi.org/10.1145/3355089.3356522},
  urldate = {2022-10-25},
  abstract = {Physics-based differentiable rendering is the task of estimating the derivatives of radiometric measures with respect to scene parameters. The ability to compute these derivatives is necessary for enabling gradient-based optimization in a diverse array of applications: from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, physics-based differentiable rendering remains challenging, due to the complex and typically nonlinear relation between pixel intensities and scene parameters. We introduce a differential theory of radiative transfer, which shows how individual components of the radiative transfer equation (RTE) can be differentiated with respect to arbitrary differentiable changes of a scene. Our theory encompasses the same generality as the standard RTE, allowing differentiation while accurately handling a large range of light transport phenomena such as volumetric absorption and scattering, anisotropic phase functions, and heterogeneity. To numerically estimate the derivatives given by our theory, we introduce an unbiased Monte Carlo estimator supporting arbitrary surface and volumetric configurations. Our technique differentiates path contributions symbolically and uses additional boundary integrals to capture geometric discontinuities such as visibility changes. We validate our method by comparing our derivative estimations to those generated using the finite-difference method. Furthermore, we use a few synthetic examples inspired by real-world applications in inverse rendering, non-line-of-sight (NLOS) and biomedical imaging, and design, to demonstrate the practical usefulness of our technique.},
  keywords = {differentiable rendering,Monte Carlo path tracing,radiative transfer}
}

@article{zhangPathspaceDifferentiableRendering2020,
  title = {Path-Space Differentiable Rendering},
  author = {Zhang, Cheng and Miller, Bailey and Yan, Kai and Gkioulekas, Ioannis and Zhao, Shuang},
  date = {2020-08-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {4},
  pages = {143:143:1--143:143:19},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392383},
  url = {https://doi.org/10.1145/3386569.3392383},
  urldate = {2022-10-25},
  abstract = {Physics-based differentiable rendering, the estimation of derivatives of radiometric measures with respect to arbitrary scene parameters, has a diverse array of applications from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, general-purpose differentiable rendering remains challenging due to the lack of efficient estimators as well as the need to identify and handle complex discontinuities such as visibility boundaries. In this paper, we show how path integrals can be differentiated with respect to arbitrary differentiable changes of a scene. We provide a detailed theoretical analysis of this process and establish new differentiable rendering formulations based on the resulting differential path integrals. Our path-space differentiable rendering formulation allows the design of new Monte Carlo estimators that offer significantly better efficiency than state-of-the-art methods in handling complex geometric discontinuities and light transport phenomena such as caustics. We validate our method by comparing our derivative estimates to those generated using the finite-difference method. To demonstrate the effectiveness of our technique, we compare inverse-rendering performance with a few state-of-the-art differentiable rendering methods.},
  keywords = {differentiable rendering,Monte Carlo rendering,path integral}
}

@thesis{zhangShapeReflectanceIllumination2021,
  type = {Thesis},
  title = {Shape, {{Reflectance}}, and {{Illumination From Appearance}}},
  author = {Zhang, Xiuming},
  date = {2021-09},
  institution = {{Massachusetts Institute of Technology}},
  url = {https://dspace.mit.edu/handle/1721.1/140015},
  urldate = {2022-10-13},
  abstract = {The image formation process describes how light interacts with the objects in a scene and eventually reaches the camera, forming an image that we observe. Inverting this process is a long-standing, ill-posed problem in computer vision, which involves estimating shape, material properties, and/or illumination passively from the object’s appearance. Such “inverse rendering” capabilities enable 3D understanding of our world (as desired in autonomous driving, robotics, etc.) and computer graphics applications such as relighting, view synthesis, and object capture (as desired in extended reality [XR], etc.).    In this dissertation, we study inverse rendering by recovering three-dimensional (3D) shape, reflectance, illumination, or everything jointly under different setups. The input in these different setups varies from single images to multi-view images lit by multiple known lighting conditions, then to multi-view images under one unknown illumination. Across the setups, we explore optimization-based recovery that exploits multiple observations of the same object, learning-based reconstruction that heavily relies on data-driven priors, and a mixture of both. Depending on the target application, we perform inverse rendering at three different levels of decomposition: I) At a low level of abstraction, we develop physically-based models that explicitly solve for every term in the rendering equation, II) at a middle level, we utilize the light transport function to abstract away intermediate light bounces and model only the final “net effect”, and III) at a high level, we treat rendering as a black box and directly invert it with learned data-driven priors. We also demonstrate how higherlevel abstraction leads to models that are simple and applicable to single images but also possess fewer capabilities.    This dissertation discusses four instances of inverse rendering, gradually ascending in the level of abstraction. In the first instance, we focus on the low-level abstraction where we decompose appearance explicitly into shape, reflectance, and illumination. To this end, we present a physically-based model capable of such full factorization under one unknown illumination and another that handles one-bounce indirect illumination. In the second instance, we ascend to the middle level of abstraction, at which we model appearance with the light transport function, demonstrating how this level of modeling easily supports relighting with global illumination, view synthesis, and both tasks simultaneously. Finally, at the high level of abstraction, we employ deep learning to directly invert the rendering black box in a data-driven fashion. Specifically, in the third instance, we recover 3D shapes from single images by learning data-driven shape priors and further make our reconstruction generalizable to novel shape classes unseen during training. Also relying on data-driven priors, the fourth instance concerns how to recover lighting from the appearance of the illuminated object, without explicitly modeling the image formation process.},
  langid = {english},
  annotation = {Accepted: 2022-02-07T15:19:00Z}
}

@inproceedings{zhangUnreasonableEffectivenessDeep2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  date = {2018},
  pages = {586--595},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html},
  urldate = {2022-04-14},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@inproceedings{zhaoJointSVBRDFRecovery2020,
  title = {Joint {{SVBRDF Recovery}} and {{Synthesis From}} a {{Single Image}} Using an {{Unsupervised Generative Adversarial Network}}},
  author = {Zhao, Yezi and Wang, Beibei and Xu, Yanning and Zeng, Zheng and Wang, Lu and Holzschuch, Nicolas},
  date = {2020},
  publisher = {{The Eurographics Association}},
  issn = {1727-3463},
  doi = {10.2312/sr.20201136},
  url = {https://diglib.eg.org:443/xmlui/handle/10.2312/sr20201136},
  urldate = {2022-10-06},
  abstract = {We want to recreate spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image. Pro- ducing these SVBRDFs from single images will allow designers to incorporate many new materials in their virtual scenes, increasing their realism. A single image contains incomplete information about the SVBRDF, making reconstruction difficult. Existing algorithms can produce high-quality SVBRDFs with single or few input photographs using supervised deep learning. The learning step relies on a huge dataset with both input photographs and the ground truth SVBRDF maps. This is a weakness as ground truth maps are not easy to acquire. For practical use, it is also important to produce large SVBRDF maps. Existing algorithms rely on a separate texture synthesis step to generate these large maps, which leads to the loss of consistency be- tween generated SVBRDF maps. In this paper, we address both issues simultaneously. We present an unsupervised generative adversarial neural network that addresses both SVBRDF capture from a single image and synthesis at the same time. From a low-resolution input image, we generate a large resolution SVBRDF, much larger than the input images. We train a generative adversarial network (GAN) to get SVBRDF maps, which have both a large spatial extent and detailed texels. We employ a two-stream generator that divides the training of maps into two groups (normal and roughness as one, diffuse and specular as the other) to better optimize those four maps. In the end, our method is able to generate high-quality large scale SVBRDF maps from a single input photograph with repetitive structures and provides higher quality rendering results with more details compared to the previous works. Each input for our method requires individual training, which costs about 3 hours.},
  isbn = {978-3-03868-117-5},
  langid = {english},
  annotation = {Accepted: 2020-06-28T15:27:49Z}
}

@inproceedings{zhaoStochasticOptimizationImportance2015,
  title = {Stochastic {{Optimization}} with {{Importance Sampling}} for {{Regularized Loss Minimization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Peilin and Zhang, Tong},
  date = {2015-06-01},
  pages = {1--9},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/zhaoa15.html},
  urldate = {2022-04-10},
  abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study stochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{zhengLearningImportanceSample2019,
  title = {Learning to {{Importance Sample}} in {{Primary Sample Space}}},
  author = {Zheng, Quan and Zwicker, Matthias},
  date = {2019-05},
  journaltitle = {Computer Graphics Forum},
  shortjournal = {Computer Graphics Forum},
  volume = {38},
  number = {2},
  pages = {169--179},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13628},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13628},
  urldate = {2022-05-28},
  langid = {english}
}

@inproceedings{zhouMonocularRealTimeHand2020,
  title = {Monocular {{Real-Time Hand Shape}} and {{Motion Capture Using Multi-Modal Data}}},
  author = {Zhou, Yuxiao and Habermann, Marc and Xu, Weipeng and Habibie, Ikhsanul and Theobalt, Christian and Xu, Feng},
  date = {2020},
  pages = {5346--5355},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_Monocular_Real-Time_Hand_Shape_and_Motion_Capture_Using_Multi-Modal_Data_CVPR_2020_paper.html},
  urldate = {2022-04-19},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
}

@article{zhouSparseaspossibleSVBRDFAcquisition2016,
  title = {Sparse-as-Possible {{SVBRDF}} Acquisition},
  author = {Zhou, Zhiming and Chen, Guojun and Dong, Yue and Wipf, David and Yu, Yong and Snyder, John and Tong, Xin},
  date = {2016-11-11},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {35},
  number = {6},
  pages = {189:1--189:12},
  issn = {0730-0301},
  doi = {10.1145/2980179.2980247},
  url = {https://doi.org/10.1145/2980179.2980247},
  urldate = {2022-09-04},
  abstract = {We present a novel method for capturing real-world, spatially-varying surface reflectance from a small number of object views (k). Our key observation is that a specific target's reflectance can be represented by a small number of custom basis materials (N) convexly blended by an even smaller number of non-zero weights at each point (n). Based on this sparse basis/sparser blend model, we develop an SVBRDF reconstruction algorithm that jointly solves for n, N, the basis BRDFs, and their spatial blend weights with an alternating iterative optimization, each step of which solves a linearly-constrained quadratic programming problem. We develop a numerical tool that lets us estimate the number of views required and analyze the effect of lighting and geometry on reconstruction quality. We validate our method with images rendered from synthetic BRDFs, and demonstrate convincing results on real objects of pre-scanned shape and lit by uncontrolled natural illumination, from very few or even a single input image.},
  keywords = {sprase reconstruction,SVBRDF accqusition}
}

@article{zhuBlendedCuredQuasinewton2018,
  title = {Blended Cured Quasi-Newton for Distortion Optimization},
  author = {Zhu, Yufeng and Bridson, Robert and Kaufman, Danny M.},
  date = {2018-07-30},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {37},
  number = {4},
  pages = {40:1--40:14},
  issn = {0730-0301},
  doi = {10.1145/3197517.3201359},
  url = {https://doi.org/10.1145/3197517.3201359},
  urldate = {2022-04-19},
  abstract = {Optimizing distortion energies over a mesh, in two or three dimensions, is a common and critical problem in physical simulation and geometry processing. We present three new improvements to the state of the art: a barrier-aware line-search filter that cures blocked descent steps due to element barrier terms and so enables rapid progress; an energy proxy model that adaptively blends the Sobolev (inverse-Laplacian-processed) gradient and L-BFGS descent to gain the advantages of both, while avoiding L-BFGS's current limitations in distortion optimization tasks; and a characteristic gradient norm providing a robust and largely mesh- and energy-independent convergence criterion that avoids wrongful termination when algorithms temporarily slow their progress. Together these improvements form the basis for Blended Cured Quasi-Newton (BCQN), a new distortion optimization algorithm. Over a wide range of problems over all scales we show that BCQN is generally the fastest and most robust method available, making some previously intractable problems practical while offering up to an order of magnitude improvement in others.},
  keywords = {deformation,distortion,elasticity,fast solvers,geometry optimization,numerical optimization,preconditioning,simulation}
}

@misc{zhuGenerativeAdversarialActive2017,
  title = {Generative {{Adversarial Active Learning}}},
  author = {Zhu, Jia-Jie and Bento, José},
  date = {2017-11-15},
  number = {arXiv:1702.07956},
  eprint = {1702.07956},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.07956},
  url = {http://arxiv.org/abs/1702.07956},
  urldate = {2022-06-28},
  abstract = {We propose a new active learning by query synthesis approach using Generative Adversarial Networks (GAN). Different from regular active learning, the resulting algorithm adaptively synthesizes training instances for querying to increase learning speed. We generate queries according to the uncertainty principle, but our idea can work with other active learning principles. We report results from various numerical experiments to demonstrate the effectiveness the proposed approach. In some settings, the proposed algorithm outperforms traditional pool-based approaches. To the best our knowledge, this is the first active learning work using GAN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{zhuUnpairedImagetoImageTranslation2020,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2020-08-24},
  number = {arXiv:1703.10593},
  eprint = {1703.10593},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1703.10593},
  urldate = {2022-05-27},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

